[
  {
    "name": "Andrew Macdonald",
    "date": "2026_01_21 15_28 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Andrew Macdonald) \u2013 2026_01_21 15_28 SAST \u2013 Notes by Gemini.docx",
    "text_length": 2439,
    "analysis": "# OMT Discovery Interview Analysis: Andrew Macdonald\n\n## \u26a0\ufe0f CRITICAL NOTE\n**This transcript does NOT contain an interview with Andrew Macdonald.** Andrew was expected but did not attend (likely due to calendar sync issues). The transcript documents a conversation between Barbara Dale-Jones and Justin Germishuys about AI functionality and organizational tool licensing, but contains **no information about the OMT scholarship application review process.**\n\n---\n\n## 1. Interviewee Role & Background\n\n**Not applicable** \u2013 Andrew Macdonald did not participate in this interview.\n\nThe attendees who did speak were:\n- **Barbara Dale-Jones** \u2013 Role in OMT unclear from transcript; appears to have budgetary/procurement authority\n- **Justin Germishuys** \u2013 AI/technical expert; advises on AI systems and licensing; manages team subscriptions\n- Both referenced other team members: Alli, Johannes, Steven, and Kia\n\n---\n\n## 2. Current Review Process Description\n\n**Not addressed in this transcript.** No information provided about:\n- How applications are currently reviewed\n- Review steps or workflows\n- Current tools/systems used for application assessment\n- Time spent on individual reviews\n\n---\n\n## 3. Pain Points & Challenges\n\n**Related to AI tools/licensing (not scholarship review):**\n- **Expensive proprietary tools**: Alli using \"Lovable\" at ~5,000-6,000 rand/month (~$500/month); team was previously paying ~8,000 rand/month across multiple users\n- **AI personality misalignment**: Barbara's frustration with Claude's overly cautious \"hysterical\" behavior when asked about medical advice \u2013 wanted information, not maternal worry\n- **Fragmented licensing**: Multiple team members on different platforms (some on Claude, some on expensive alternatives) with inconsistent organizational support\n- **Lack of control over AI behavior**: Users initially unaware they could modify system prompts to change AI personality\n\n---\n\n## 4. What They Value in Applications\n\n**Not applicable** \u2013 Scholarship applications were not discussed.\n\n---\n\n## 5. Views on AI/Technology\n\n### Attitude Toward AI\n- **Pragmatic and customization-focused**: Justin prefers AI \"that can be what I need it to be\" depending on context\n- **Skeptical of fixed personality**: Both speakers dislike AI with predetermined personality traits\n- **Technically sophisticated**: Understanding that AI behavior is shaped by explicit instructions rather than fundamental model training\n\n### Specific Concerns & Hopes\n- **Liability concerns**: Providers intentionally design cautious AI behavior (esp. medical contexts) to manage liability\n- **Transparency preference**: Support for organizations like Anthropic that publish their system prompts (~10,000 words in Anthropic's case)\n- **Customization capability**: Preference for ability to modify AI behavior through system prompts rather than accepting factory defaults\n\n### What They Would/Wouldn't Want Automated\n- **Would want**: Flexible AI personality that adapts to user needs (tough love vs. gentle compassion)\n- **Wouldn't want**: Fixed, overly cautious, or infantilizing AI responses\n- **Appreciated learning**: That AI behavior can be modified through system message adjustments rather than being inherent to the model\n\n### Trust & Transparency Requirements\n- Value organizations that are transparent about how they've configured AI (system prompt publication)\n- Understand liability management creates intentional constraints but want option to opt-out\n- Accept \"factory settings\" model where organizations disclaim liability if users modify defaults\n\n---\n\n## 6. Suggestions & Ideas\n\n### Improvements Suggested\n1. **Centralized Claude team license** \u2013 Move away from expensive fragmented tools to coordinated team subscription (~$150 for 5 people vs. ~8,000 rand/month current spend)\n2. **Investigate Claude team features** \u2013 Determine whether $150 team subscription provides \"Claude hard options\" (unclear technical requirement)\n3. **Control over system prompts** \u2013 Enable ability to customize AI personality through system message modifications rather than accepting defaults\n4. **Educational approach** \u2013 Show team members where/how to modify system prompts to get desired AI behavior\n\n### Features Wanted in New System\n- Cost efficiency (20x more value than current Lovable service)\n- Flexibility in AI personality configuration\n- Team-level licensing with centralized management\n- API access tier for higher-level system direction control\n\n### Priorities for Change\n1. **Cost reduction** (primary): Consolidate from ~8,000 rand/month to ~150 USD/month\n2. **Tool standardization**: Move everyone to Claude ecosystem\n3. **User empowerment**: Enable customization of AI behavior for appropriate responses\n\n---\n\n## 7. Key Direct Quotes\n\n| Quote | Context | Time |\n|-------|---------|------|\n| \"I think you've told me before that you used AI with like you know medical stuff giving. So last week I had this situation where I got bitten by I think I got bitten by a spider and I had this terrible kind of swelling on my arm... it got hysterical like you have to go to a doctor\" | Barbara describing frustration with AI's overly cautious medical advice response | 00:02:16 |\n| \"I think it has to do more with the system message than the training... foundation model providers trying to figure out what is the right personality and what does it mean to respond ethically. So there's a lot of liability if it gives medical advice\" | Justin explaining that cautious behavior is intentional design choice, not fundamental model training | 00:02:16 |\n| \"It's been trained to respect them in order of importance. If they've given it a developer prompt or instructions, the next level down can't override it... And that way you can build applications that respect the developer's intention.\" | Justin explaining hierarchical system message structure and why personality can be modified | 00:03:31 |\n| \"It's a very explicit instruction on how it should tackle it... it's not fundamental to the trained model. It's an instruction.\" | Justin clarifying that AI behavior is configured instruction, not innate | 00:04:50 |\n| \"I don't like AI with personality. I like personality that is appropriate to the circumstance. I like that it isn't human and that it doesn't that it's not fixed and it can be what I need it to be. Sometimes I just need tough love. Sometimes I need gentle compassion.\" | Justin's core philosophy on desirable AI behavior \u2013 context-dependent and customizable | 00:07:03 |\n| \"All I was looking for was some information... not like a kind of hysterical mother going you know um see a doctor at once otherwise you know you might die overnight\" | Barbara succinctly stating mismatch between her needs and AI's default response | 00:07:03 |\n| \"I can have it insult me every time I ask it for medical advice and it can like just call me a nambi pami and stop being such a mama's boy\" | Justin demonstrating extent of customization possible \u2013 humorous but illustrates deep malleability of AI personality | 00:07:03 |\n| \"she could get 20 times as much value for three grand\" [vs. 5,000-6,000 rand/month for Lovable] | Justin quantifying cost-benefit of moving to Claude subscription | 00:09:12 |\n| \"between her and Kasha, we were paying like eight grand a month or something like that, you know\" | Barbara illustrating current expensive tool ecosystem burden | 00:09:12 |\n| \"I'll just ask Claude to tell me if it's a thing that we should or shouldn't do with like the pros and cons\" | Justin's plan to use Claude itself to evaluate Claude licensing decision | 00:10:00 |\n\n---\n\n## 8. Unique Insights\n\n### Distinctive Contributions\n1. **System Prompt Transparency as Trust Mechanism**: Justin's emphasis on Anthropic publishing their ~10,000-word system prompt as a differentiator is noteworthy. This suggests organizations should value AI providers who are transparent about how they've configured behavior, not just capability claims.\n\n2. **Liability-Driven Design Philosophy**: The insight that overly cautious AI behavior is *intentional liability management* rather than model limitation is important. This suggests organizations may need to actively negotiate behavior customization rather than accepting defaults.\n\n3. **Hierarchical Prompt Architecture**: The distinction between developer prompts \u2192 system prompts \u2192 user inputs (in order of override importance) is technically sophisticated and suggests different control levels available depending on API vs. interface access.\n\n4. **\"Factory Settings\" Liability Model**: The concept that providers give \"factory settings\" and disclaim liability if users modify them is a novel organizational/legal strategy worth considering for OMT's future AI implementation.\n\n5. **Context-Dependent Personality as Core Design Principle**: Justin's preference for AI that adapts personality to context (tough love vs. compassion) rather than having a fixed personality suggests a sophisticated understanding that *one-size-fits-all AI behavior may not serve all review contexts*.\n\n### What's Missing (Critical Gap)\n**This interview failed to capture the intended content.** There is no discussion of:\n- Scholarship application review criteria\n- Current bottlenecks in the OMT review process\n- How AI might assist with actual application assessment\n- Concerns specific to scholarship evaluation\n- How reviewers evaluate merit, potential, institutional need, etc.\n\n**This appears to be an accidental off-topic conversation.** A follow-up interview with Andrew Macdonald (or rescheduled session) is essential to meet the original research objectives.\n\n---\n\n## Recommendations for Next Steps\n\n1. **Reschedule Andrew Macdonald interview** \u2013 Core discovery interview did not occur\n2. **Use this conversation as reference only** \u2013 for understanding organizational AI literacy and preferences, but not for scholarship review process improvement\n3. **Consider whether system prompt customization insights apply to review context** \u2013 Could context-dependent AI personality be valuable in evaluating different scholarship categories?\n4. **Document the technical learnings** \u2013 The system prompt modification capability could be relevant for any future AI implementation in the review process",
    "tokens_in": 4164,
    "tokens_out": 2287,
    "skipped": false
  },
  {
    "name": "Andrew Macdonald",
    "date": "2026_01_23 14_59 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Andrew Macdonald) \u2013 2026_01_23 14_59 SAST \u2013 Notes by Gemini.docx",
    "text_length": 2074,
    "analysis": "# OMT Discovery Interview Analysis\n\n## \u26a0\ufe0f CRITICAL NOTE\n\nThis transcript does not contain an interview with an OMT application reviewer. Instead, it is a **post-interview debrief conversation between two OMT staff members (Justin Germishuys and Barbara Dale-Jones)** discussing their interview program and logistics. The transcript provides no substantive information about the OMT application review process, review criteria, or views on AI in review.\n\n**This analysis cannot be completed as requested** because the transcript lacks the necessary content.\n\n---\n\n## What This Transcript Actually Contains\n\nThe transcript captures:\n- **Meta-discussion** about an interview program (possibly related to StrideShift, a program or initiative)\n- **Administrative decisions** (discontinuing Andrew McDonald interviews, confirming participant diversity)\n- **Personal anecdotes** (sleep deprivation, child bedtime stories, jump scares)\n- **Tangential psychology discussion** (nervous system desensitization, phobia treatment methods)\n\n---\n\n## 1. Interviewee Role & Background\n**NOT AVAILABLE** - This is not an interview with an application reviewer.\n\n---\n\n## 2. Current Review Process Description\n**NOT AVAILABLE** - The transcript does not describe any application review process.\n\n---\n\n## 3. Pain Points & Challenges\n**NOT AVAILABLE** - No review process challenges are discussed.\n\nThe only \"pain point\" mentioned is personal: Justin Germishuys's exhaustion from sleep deprivation and daily school runs affecting his cognitive capacity (00:03:23).\n\n---\n\n## 4. What They Value in Applications\n**NOT AVAILABLE** - No criteria for evaluating applications are discussed.\n\nThe only evaluative comments relate to their interview program:\n- They value **transformation of participants** (\"20 or 30 who were transformed\") over broad reach (00:01:58)\n- They appreciate **authentic participant feedback and quotes** (00:01:58-00:02:58)\n- They value **interdisciplinary diversity** in interview participants: \"fine arts through to science\" (00:07:56)\n\n---\n\n## 5. Views on AI/Technology\n**NOT AVAILABLE** - AI and technology for review processes are not discussed.\n\nThe only technology mention is **Gemini note-taking**, referenced in the meeting notes section (feedback requested on using Gemini for notes).\n\n---\n\n## 6. Suggestions & Ideas\nThe only suggestions relate to their program delivery, not the review process:\n\n- **Content repurposing**: \"I just love turning it into um like small 50-second clips. They're just so easy to digest.\" (00:02:00) - suggests digestible formats are valuable\n- **Selective follow-up**: They plan to follow up with strong participants rather than chase non-responders (00:07:56)\n\n---\n\n## 7. Key Direct Quotes\n\n| Quote | Context | Relevance |\n|-------|---------|-----------|\n| \"I mean it it looks like it as long as one focuses on the 20 or 30 who were transformed instead of saying okay well let's be let's focus on the the balance\" | Program impact philosophy | Suggests preference for depth over breadth in evaluation |\n| \"I just love turning it into um like small 50-second clips. They're just so easy to digest.\" | Content strategy | Indicates appreciation for concise, accessible formats |\n| \"I don't think we should chase um chase somebody. Um and also we may discover that with the remaining ones we get everything we need.\" | Participant engagement strategy | Pragmatic approach to incomplete data |\n| \"we also very nicely have a range of different disciplines everything from sort of fine arts through to science you know etc\" | Interview diversity assessment | Values interdisciplinary perspectives |\n| \"The most ponderous of the interviews I found was the man I think his name was Muhammad...I found him a bit ponderous, but other than that, I found them all helpful.\" | Interview quality evaluation | Prefers concise, efficient communication over lengthy responses |\n\n---\n\n## 8. Unique Insights\n\n**This transcript contains no insights relevant to OMT's application review process or AI implementation goals.**\n\nThe only distinctive element is an **incidental discussion of behavioral psychology and nervous system desensitization** (00:05:38-00:06:46), which, while interesting, is entirely tangential to the research objective.\n\n---\n\n## Recommendation\n\n**Request a corrected transcript** or clarification on:\n1. Was this the intended interview to analyze?\n2. Are there other discovery interviews with actual application reviewers?\n3. Does the \"StrideShift\" reference indicate this is a different program entirely?\n\nThe current transcript cannot inform improvements to OMT's application review process or AI integration strategy.",
    "tokens_in": 3742,
    "tokens_out": 1074,
    "skipped": false
  },
  {
    "name": "Cephas Chikanda",
    "date": "2026_01_21 10_57 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Cephas Chikanda) \u2013 2026_01_21 10_57 SAST \u2013 Notes by Gemini.docx",
    "text_length": 11193,
    "analysis": "# Analysis of OMT Discovery Interview Transcript\n\n## CRITICAL NOTE\nThis transcript does not appear to be an interview with an OMT scholarship applicant or reviewer. Instead, it is a conversation between **Barbara Dale-Jones** and **Justin Germishuys** (both OMT staff/leadership) discussing side projects, educational initiatives, and business ideas unrelated to the scholarship review process. The interviewee listed as \"Cephas Chikanda\" never appears in the transcript.\n\n**This analysis cannot be completed as requested** because the transcript does not contain interview content about the OMT application review process. However, I will note what IS present:\n\n---\n\n## What This Transcript Actually Contains\n\n**Participants:** Barbara Dale-Jones and Justin Germishuys (internal OMT discussion)\n\n**Topics Discussed:**\n- \"Vibe engineering\" projects with Nathan (a child)\n- AI-generated code about \"nothing\" \n- Murder mystery generation using AI\n- Philosophical discussions about AI safety (Jeffrey Hinton)\n- Educational initiatives (\"Vibe coding\")\n- Business ideas around 3D printing and design\n- Plans for DAB (presumed internal program)\n- Speculation about \"voxing\" technology\n- Cash flow and invoicing issues\n\n---\n\n## Why This Analysis Cannot Proceed\n\nThe requested analysis framework requires:\n- \u2717 Interviewee's role in **OMT review process** \u2014 not discussed\n- \u2717 **Current review process description** \u2014 not discussed\n- \u2717 **Pain points with applications** \u2014 not discussed\n- \u2717 **Application evaluation criteria** \u2014 not discussed\n- \u2717 Views on **AI in review process** \u2014 not discussed in operational context\n\n---\n\n## Recommendation\n\n**Please verify:**\n1. Is this the correct transcript? (Listed interviewee \"Cephas Chikanda\" doesn't appear)\n2. Should this analysis focus on the educational/AI philosophy discussions present instead?\n3. Are there other interview transcripts for OMT scholarship reviewers that should be analyzed?\n\nIf you'd like analysis of the content that *is* present (AI philosophy, educational methodology, creative use of AI), I can provide that instead.",
    "tokens_in": 17231,
    "tokens_out": 497,
    "skipped": false
  },
  {
    "name": "Dina  Ligaga",
    "date": "2026_01_12 13_24 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Dina  Ligaga) \u2013 2026_01_12 13_24 SAST \u2013 Notes by Gemini.docx",
    "text_length": 7307,
    "analysis": "# OMT APPLICATION REVIEW PROCESS ANALYSIS\n## Interview with Dina Ligaga | January 12, 2026\n\n---\n\n## 1. INTERVIEWEE ROLE & BACKGROUND\n\n**Role in OMT Review Process:**\nDina Ligaga serves as an adjudicator/reviewer for OMT scholarship applications.\n\n**Duration of Involvement:**\nApproximately 4 years (approached in 2022, as of January 2026)\n\n**Discipline & Expertise:**\n- Faculty: Humanities\n- Field: Media and Cultural Studies (trained in literature)\n- Specialist areas: Cultural studies, social sciences, work around gender\n- Institutional affiliation: VIT (Vaal University of Technology, based on context)\n- Brings interdisciplinary perspective spanning literature, cultural studies, and social sciences\n\n---\n\n## 2. CURRENT REVIEW PROCESS DESCRIPTION\n\n**Overall Review Workflow:**\n\nDina begins by examining applications that have already been pre-screened (incomplete applications removed):\n\n1. **Initial Reading Phase** - Opens application and reviews candidate's background, motivation, proposed study, CV, transcripts\n2. **Motivation Assessment** - Reads motivation statement first to gauge applicant's thinking and sincerity\n3. **Proposal Evaluation** - Reviews proposed study for originality and feasibility\n4. **Academic History Review** - Examines transcripts and past work\n5. **Extended Note-Taking** - Takes extensive notes in a notebook during review\n6. **Rubric Completion** - Applies tick-box rubric scoring (4-point scale)\n7. **Narrative Writing** - Writes narrative commentary explaining reasoning (though reports have become shorter)\n8. **Reconciliation** - Sometimes adjusts rubric scores to align with overall intuitive assessment\n9. **Final Recommendation** - Makes recommend/don't recommend decision with written justification\n\n**Steps in Detail:**\n\n- First looks at **motivation** to understand \"how this person's thinking feels like\" (00:08:17)\n- Reviews **proposed study** for originality and excitement, especially at PhD level\n- Examines **continuity** between applicant's history and proposed work\n- Uses **notes** extensively but struggles with converting them to final written narratives\n- Applies **rubric scoring** (1-4 scale) with tick-box sections\n- Makes **final judgment call** on recommendation status\n- Sometimes **backtracks** to adjust rubric scores to match intuitive sense\n\n**Tools & Systems Currently Used:**\n\n- **OMT submission platform** with multi-window interface (current system since approximately 2022)\n- **Previous system**: Excel sheets with comparative review process (no longer in use)\n- **Notebook** for extensive handwritten notes during review\n- **Rubric** with 4-point scale (1=fail, 2=poor, 3=good, 4=excellent) for tick-box scoring\n- **Narrative text fields** for final commentary\n- **Document downloads** (multiple downloads required, described as \"tedious\")\n\n**Time Spent on Reviews:**\n\n- Approximately **30 minutes per application** for full reading and assessment (00:21:00)\n- Time-intensive at **yearly start-up** due to platform navigation learning curve\n- Becomes easier once familiar with interface rhythm\n- Note-taking and final narrative writing adds significant time\n- Occurs during busy period when also marking exams (00:16:06)\n\n---\n\n## 3. PAIN POINTS & CHALLENGES\n\n**Critical Frustrations:**\n\n**A. Platform/Interface Issues:**\n- **Multi-window interface** is highly frustrating (00:06:03) - requires opening different windows to access information\n- **Steep yearly learning curve** - significant time wasted at start of review cycle figuring out navigation\n- **Tedious document downloads** required (00:33:28)\n- Platform feels like an improvement over Excel sheets but still creates friction\n\n**B. Rubric Limitations:**\n- **\"Fuzziness\" between scores 3 and 4** (00:11:56) - the distinction is unclear and subjective\n- **Insufficient gradation** - only 4-point scale doesn't capture nuance of excellence (00:21:00)\n- **Lack of range for high-performing candidates** - cannot distinguish between 75% excellent and 90% excellent (00:23:04)\n- **Bluntness of scoring system** - feels overly rigid and doesn't capture complexity of judgment\n- **Scoring creates negotiation and uncertainty** (00:21:00) - reviewer must perform mental gymnastics to align scores with actual conviction\n\n**C. Judgment-Rubric Disconnect:**\n- Reviewer frequently needs to **mark up or down** scores to match their overall intuitive assessment (00:13:16, 00:14:41)\n- Rubric process **\"boxes them in\"** and **\"slows them down\"** (00:19:59, 00:21:00)\n- Describes phase as losing confidence - worries about whether scoring is accurate\n- Feels rubric doesn't capture \"all of the effort that I've put into reading through the whole thing\" (00:21:00)\n\n**D. Narrative Feedback Challenges:**\n- **Written reports have grown progressively shorter** - often only 1-2 lines after rubric completion (00:04:41)\n- Despite extensive note-taking, final narratives are minimal\n- **No feedback mechanism** on quality or length of narrative assessments (00:04:41, 00:16:06)\n- Uncertainty about expectations: \"Am I supposed to make it longer? Am I supposed to make it shorter?\" (00:16:06)\n- Struggles with **converting notes to final written form**, especially when workload is heavy\n\n**E. Volume & Time Pressure:**\n- **Masters applications are particularly challenging** due to high volume (00:24:17, 00:25:22)\n- PhD applications easier because applicants often near completion (00:24:17)\n- Review occurs during **peak exam marking periods**, creating time crunch (00:16:06)\n- Takes considerable time to \"sink yourself into\" applications and understand what people want (00:35:58)\n\n**F. Process Clarity Issues:**\n- **Uncertain about purpose of race/class disclosure** in applications (00:32:06)\n- Ambiguity about **function of demographic information** collection\n- Limited **transparency on decision synthesis** - unsure how multiple reviewers' assessments are combined\n\n**Specific Bottlenecks:**\n\n1. **Start-of-cycle learning curve** with platform navigation\n2. **Rubric-to-judgment translation** step that causes hesitation\n3. **Narrative writing phase** when workload is heaviest\n4. **Masters volume** creating decision fatigue\n5. **Edge cases** between 3 and 4 scores requiring difficult judgment calls\n\n---\n\n## 4. WHAT THEY VALUE IN APPLICATIONS\n\n**Primary Evaluation Criteria (in priority order):**\n\n**1. Applicant Motivation (Most Important)**\n- Seeks **genuine, unique, honest motivation** (00:08:17)\n- Values **sincere personal statement** about influences and experiences (00:09:30)\n- **Red flag**: Generic motivation immediately signals applicant isn't serious (00:08:17)\n- Looks for applicant's **personal story** and thinking process (00:08:17)\n- Evaluates **believability and authenticity** - distinguishes between performed vs. genuine motivation\n\n**2. Proposed Study (Highly Influential)**\n- Prioritizes **originality and excitement** in proposed research, especially at PhD level (00:10:46)\n- Examines **feasibility and thoughtfulness** of proposal (00:35:58)\n- **Red flag**: Generic, unoriginal proposals with nothing exciting about them (00:10:46)\n- At Masters level, more variable quality; at PhD level, generally stronger (00:10:46)\n- Wants to see **clear thinking about research direction**, not vague statements\n\n**3. Continuity Between History and Proposed Work**\n- **\"String through\"** from applicant's background to proposed study (00:09:30)\n- Evidence of **logical progression** and connection between past work and future plans\n- Shows **sustained commitment** to research area (00:09:30)\n- Influences overall judgment significantly\n\n**4. Academic History & Transcripts**\n- Reviews **past study performance** as context\n- Less influential than motivation and proposal\n- Can create tension when transcripts don't match application quality (00:25:22)\n- Doesn't determine decisions but provides supporting evidence\n\n**5. Budget (Least Important)**\n- Reviewed but **minimally influential** on decision (00:19:59)\n- Often dismissed casually: \"Oo, that's a lot\" or \"Oh, that's little\"\n- Doesn't substantively impact recommendations\n\n**Red Flags Watched For:**\n\n1. **Generic motivation** - immediately raises concerns about seriousness\n2. **AI-generated content** - describes concern about \"fuzzier\" genuine motivation in some applications where AI may have been used (00:09:30)\n3. **Lack of originality** in proposed study\n4. **Disconnect between transcripts and application quality** - applicant appears stronger on paper than grades suggest (00:25:22)\n5. **No clear narrative arc** between past work and future plans\n6. **Vague research direction** without specificity\n\n**What Makes Applications Stand Out:**\n\n- **Compelling personal narrative** with specific influences and experiences\n- **Novel, clearly-thought-out research proposal** with genuine excitement about topic\n- **Alignment between applicant's demonstrated interests** (in CV, previous work) and proposed study\n- **Authenticity and honesty** in describing motivation\n- **Evidence of sustained intellectual engagement** with research area\n- **Clear understanding** of research problem and approach\n\n**How Different Factors Are Weighted:**\n\n- **Motivation + Proposed Study + Continuity** = Approximately 70-80% of decision-making weight\n- **Academic history/transcripts** = Supporting evidence, approximately 15-20%\n- **Budget** = Minimal weight (acknowledged as necessary but not determinative)\n- **References/recommendations** = Provides \"sense of who this person is\" (00:34:38), valuable but secondary\n\n**Tacit/Unwritten Criteria:**\n\nDina acknowledges using judgment factors **not formally captured in rubric**:\n\n- **Sincerity and authenticity** in motivation (00:09:30)\n- **Perceived difficulty of applicant's upbringing** - race and class angles influence reading (00:32:06)\n- **Personal conviction about what person deserves** - synthesizes facts not fully in rubric (00:14:41)\n- **Gut sense of commitment** and continuity (00:09:30)\n- **\"Feel\"** for how applicant's thinking comes across\n\n---\n\n## 5. VIEWS ON AI/TECHNOLOGY\n\n**Overall Attitude Toward AI in Review Process:**\n\nCautiously open but with significant concerns. Dina distinguishes between:\n- AI she would welcome (preliminary vetting, pre-populated rubrics)\n- AI she would not want (replacement of core judgment)\n\n**Specific Concerns About AI:**\n\n1. **Authenticity Detection** - Observes that AI-generated content creates \"fuzzier\" motivation statements that lack genuine sincerity (00:09:30). Concerned AI makes it harder to detect authentic personal motivation.\n\n2. **Loss of Core Judgment** - Emphasizes that \"the intellectual leg work is the fun part\" (00:35:58) - doesn't want AI replacing the actual evaluation of ideas and candidates.\n\n3. **Incomplete Capture of Decision Factors** - Recognizes that much of her judgment is intuitive and tacit (sincerity, continuity, personal conviction), which may be difficult for AI to systematize (00:09:30, 00:14:41).\n\n4. **Dimension Reduction Risk** - Concerned that rubric lacks dimensionality; AI based on rubric would likely be even more reductive.\n\n**What They WOULD Want Automated:**\n\n1. **Preliminary Vetting** (00:35:58)\n   - AI identifies which applications \"need attention and which do not\"\n   - Focuses on originality and feasibility assessment\n   - Reduces volume to manageable number\n   - Described as helpful because sinking into applications is time-intensive\n\n2. **Pre-Populated Rubric** (00:37:18)\n   - Trusted partner/AI completes rubric scoring\n   - Reviewer receives filled-in rubric as starting point\n   - Allows focus on narrative feedback rather than tick-box completion\n   - Provides \"leeway to change scores if they disagreed\" (00:37:18)\n\n3. **Administrative/Technical Support**\n   - Document aggregation to reduce tedious downloads\n   - Platform interface improvements\n   - Note organization assistance\n\n**What They Would NOT Want Automated:**\n\n1. **Final judgment call** on recommendation\n2. **Narrative assessment and explanation** of reasoning\n3. **Evaluation of sincerity and authenticity** in motivation\n4. **Originality assessment** of proposed study (wants to read it themselves)\n5. **Decisions involving tacit factors** (continuity, personal conviction)\n\n**Trust & Transparency Requirements:**\n\n1. **\"Trusted source\" or \"trusted partner\"** - repeatedly emphasized (00:35:58, 00:37:18)\n   - Would need confidence in AI's training and logic\n   - Would need to understand how decisions were made\n\n2. **Flexibility to override** - essential requirement (00:38:21)\n   - Pre-populated rubric must be changeable\n   - Reviewer must retain authority to adjust scores\n   - \"I do need to have the kind of leeway or at least know that I could change something if I didn't agree\" (00:38:21)\n\n3. **Transparency about AI use** - notes applicants should state if they've used AI in applications (00:09:30)\n   - Implies expectation same transparency should apply to review process\n\n4. **Synthesis and feedback** - values knowing other reviewers' thoughts provided \"somebody else is doing the work\" of synthesis (00:29:08)\n   - Wants assurance independent judgment isn't \"too far off\" (00:30:28)\n   - Would accept AI synthesis if trusted\n\n5. **Explainability** - wants to understand how AI rubric scores were determined so she can contextualize them\n\n**Comparative Comfort with Technology:**\n\n- Current system feels \"faster\" than old Excel comparative process but creates other friction\n- Multi-window interface is frustrating but not a deal-breaker\n- Comfortable with platform-based assessment rather than direct comparison with other reviewers\n- Doesn't miss comparative review aspect (\"mudies the water\") but values knowing overall consensus exists\n\n---\n\n## 6. SUGGESTIONS & IDEAS\n\n**Suggested Improvements to Rubric:**\n\n1. **Expand Scoring Range** (primary suggestion) (00:24:17, 00:23:04)\n   - Current 4-point scale insufficient, especially for distinguishing excellence\n   - Should allow differentiation between \"75% excellent\" and \"90% excellent\"\n   - Particularly critical for **Masters applications** due to high volume (00:25:22)\n   - Note: Different rubrics exist for PhDs, Masters, and Postdocs; Postdoc rubric particularly constrained (00:24:17)\n\n2. **Add Dimensionality to Scoring** \n   - Current rubric is too \"blunt\" (00:18:40)\n   - Needs more gradations to capture the full range of high-performing candidates\n   - Mentioned specifically wanting to distinguish excellence levels that reviewers intuitively recognize\n\n**Suggested Improvements to Process:**\n\n1. **Pre-Populated Rubric System** (strong recommendation) (00:37:18)\n   - AI or trusted source completes rubric scoring first\n   - Reviewer receives pre-filled rubric with ability to modify\n   - Allows focus on **narrative feedback** rather than mechanical tick-box completion\n   - Reduces back-and-forth adjustment currently required\n   - \"I'd love that. Absolutely.\" (00:38:21)\n\n2. **Preliminary Vetting/Filtering** (strong recommendation) (00:35:58)\n   - AI or research assistant reviews all applications\n   - Identifies applications worthy of deep attention vs. those that don't require full reading\n   - Outputs something like: \"you need to pay attention to them and this person you don't need to pay attention to them\"\n   - Would dramatically reduce time spent on clearly-not-competitive applications\n   - Caveat: Difficult to operationalize without reading full applications, but would still save time on partial vetting\n\n3. **Platform Interface Improvements**\n   - Reduce multi-window complexity (00:06:03)\n   - **Consolidate information** so doesn't require opening multiple windows\n   - **Reduce steep learning curve** at start of yearly cycle\n   - Minimize **tedious document downloads** (00:33:28)\n   - Create more intuitive navigation\n\n4. **Feedback Mechanism on Narrative Assessments** (implied suggestion)\n   - Provide feedback on length and quality of narrative reports\n   - Clarify expectations for final commentary\n   - Would help reviewers understand if brief (1-2 line) reports are acceptable or should be longer\n\n5. **Support for Note Organization**\n   - Assist with converting extensive notes into coherent narrative\n   - Could be simple system for organizing notes by criteria\n   - Would address bottleneck where notes exist but aren't translated to final written assessment\n\n**Priority Features for New System (Ranked by Emphasis):**\n\n1. **Wider scoring range** (repeatedly emphasized)",
    "tokens_in": 11220,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Edzai Conilias Zvobwo",
    "date": "2026_01_13 09_26 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Edzai Conilias Zvobwo) \u2013 2026_01_13 09_26 SAST \u2013 Notes by Gemini.docx",
    "text_length": 6737,
    "analysis": "# OMT Application Review Process Analysis\n## Interview with Edzai Conilias Zvobwo\n\n---\n\n## 1. Interviewee Role & Background\n\n**Position & Expertise:**\n- Science reviewer for OMT scholarship applications\n- Background in science and mathematics\n- Reviews applications across the full science spectrum (natural science and others)\n- Runs own company specializing in analytics and AI solutions\n\n**Duration:**\n- Reviewing applications since approximately 2019 (pre-COVID)\n- \"It's been a while\" - exact timeline unclear, but multiple years of experience\n- Also involved in the OMT Two Fellows process (mentioned as comparable to scholarship review)\n\n**Key Context:**\n- Familiar with previous manual processes (Excel spreadsheets) and recent software adoption\n- Currently consulting on AI solutions for clients, giving him informed perspective on AI capabilities and limitations\n\n---\n\n## 2. Current Review Process Description\n\n**Process Flow:**\n1. Receives applications through software system (first year using this after manual Excel-based process)\n2. Reviews candidate certificates and transcripts (high school, undergraduate performance)\n3. Assesses applications against detailed rubric with multiple dimensions\n4. Assigns numerical scores (0-4 scale)\n5. Conducts \"final assessment\" where written summation of all evaluation is required\n6. System allows reviewers to revisit and revise scores as pattern emerges\n\n**Tools & Systems:**\n- Recently adopted software (described as \"quite convenient\" compared to manual Excel spreadsheets)\n- Uses a detailed, algorithmic rubric provided by OMT\n- Scoring system: 0-4 scale\n- System allows flexibility to change/revise scores after initial assignment\n\n**Time & Workflow:**\n- Process described as \"mind-numbing\" after extended periods\n- Struggles particularly with:\n  - First application review (no reference point)\n  - Final written assessment/summation phase\n- Can revisit and adjust scores as evidence accumulates across application batch\n- No explicit mention of time per application, but implied to be significant given \"mind-numbing\" nature\n\n**Standing Questions:**\n- Unclear how many reviewers assess the same candidate for bias mitigation\n- Unsure about inter-year institutional memory practices\n\n---\n\n## 3. Pain Points & Challenges\n\n**The \"Cold Start Problem\":**\n- First application is \"always the most difficult one because there's no frame of reference\"\n- \"You are flying in blind\" with initial assessments\n- \"Could actually prejudice the person who's judged first\"\n- Only after 1-2 applications does a pattern emerge\n- Requires going back and revising early scores when quality becomes clearer\n- System flexibility to revise is necessary intervention for this problem\n\n**Scoring Fatigue & Fuzzy Numbers:**\n- Scoring becomes \"mind-numbing\" over time\n- Numbers become \"fuzzy\" - reviewers lose clarity on what distinguishes a 3 from a 4\n- Lack of clear boundaries in scoring scale\n- Consistency deteriorates as fatigue sets in\n\n**Final Assessment Summation:**\n- Described as \"quite a task\" and \"quite difficult\"\n- Struggle to write comprehensive summary despite having just reviewed all materials\n- Attributes this to \"human memory issue\" - can't synthesize what was fresh during detailed review\n- Process is \"quite easy\" while reviewing step-by-step, but summation phase creates sudden difficulty\n- Uncertain if this is widespread issue or individual problem\n\n**Bias Issues:**\n- Personal bias toward impact-driven research helping \"bottom of the pyramid\"\n- Bias from reviewing candidate transcripts with poor early performance\n- Bias toward fundamental problems over niche/first-world oriented research\n- Trying to \"self-regulate\" against these biases with mixed success\n- Difficulty comparing disparate subjects (e.g., nanotechnology vs. animal husbandry)\n\n**Subject Matter Expertise Gaps:**\n- Covers wide range of science disciplines but isn't expert in all\n- May inadvertently dismiss \"next Nobel Prize winner\" due to lack of subject expertise\n- \"I'm not a water purification expert\"\n- Risk of bias based on subject appeal rather than actual merit\n\n**Saturation & Topic Clustering:**\n- Hot topics (e.g., AI applications) see multiple similar submissions\n- Need to identify \"most impactful\" within saturated topic areas\n- Intra-year saturation requires contextualizing impact within South African context\n- Makes comparison and differentiation more difficult\n\n**Lack of Institutional Memory:**\n- No inter-year contiguity - doesn't remember what was adjudicated previous years\n- Feels like starting \"on a little island\" each year\n- Cannot leverage patterns or benchmarks from previous cohorts\n- Uncertainty whether this is intentional design or system limitation\n\n**Application Quality Variance:**\n- Significant disparity in application quality between schools\n- Some schools teach effective application-writing; others produce poor quality\n- Applicants struggle to articulate impact (focus on \"I I I\" rather than world-facing impact)\n- Difficulty distinguishing between candidates who can't articulate vs. those without clear ideas\n\n**Scoring Scale Resolution:**\n- 0-4 scale has bins that are \"too big\"\n- \"High quality three\" and \"low quality three\" bunched together indistinguishably\n- Cannot adequately differentiate within quartiles\n- Scale feels too coarse-grained for nuanced assessment\n\n---\n\n## 4. What They Value in Applications\n\n**The Personal Story (Highest Weight):**\n- Personal narrative has \"the biggest weighting\" in final decisions\n- Looks for evidence of resilience and overcoming obstacles\n- Values applicants showing they \"came from a broken home and against all odds...overcome those obstacles\"\n- This is the critical element that \"has to resonate with you as the adjudicator\"\n- Described as \"very human\" and \"can't be automated\"\n- Will sometimes rate higher (4) even if articulation is weak if story implicitly resonates: \"I get what you're saying and I'll give you four because I get what you're saying, but you're not saying it, but I get it\"\n\n**Meeting Minimum Requirements:**\n- Must meet all baseline qualification requirements\n- \"They have to be competitive in terms of grades\"\n- Rubric minimum thresholds are disqualifying if not met\n- \"If they don't qualify then I discard literally\"\n- Academic credentials checked: high school performance, undergraduate GPA, transcript quality\n\n**Research Impact & Scope:**\n- Research must \"help as many people as possible\"\n- Values fundamental problem-solving over niche applications\n- Prefers immediate, visible impact: \"someone who's creating a new way to clean jobic water\" over \"a way to multiply matrices better\"\n- Explicitly stated: \"I would go for the job water because I can see the immediate\"\n- Bias toward research benefiting broader population vs. first-world oriented work\n- Values South African contextual impact specifically\n\n**Compelling Narrative Arc:**\n- Application must demonstrate clear research direction and purpose\n- Applicants must articulate (or at least implicitly convey) what they're trying to accomplish\n- Connection between personal story and research focus matters\n- Story that is \"neutral\" (nothing amazing or bad) gets lower weighting without strong narrative\n\n**Quality of Presentation:**\n- Ability to articulate ideas clearly matters\n- Some applicants from certain schools demonstrate this; others struggle\n- Recognition that some strong researchers lack application-writing training\n- Appreciates when ideas are explained from external impact perspective vs. self-centered framing\n\n**Red Flags (Implicit):**\n- Poor undergraduate performance without compelling explanation\n- Inability to articulate research direction or impact\n- Research focused solely on niche academic advancement without broader application\n- Applications suggesting candidates cannot present themselves effectively\n- Lack of evidence of resilience or character development when reviewing transcripts\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Position: \"Human in the Loop\" Model Essential**\n\nEdzai is cautiously optimistic about AI support but emphatic about human oversight requirements. He describes himself as conversant with AI capabilities through his consulting work and has strong technical opinions.\n\n**What AI Could/Should Do:**\n- Handle \"drudge work\" - the algorithmic, rubric-based scoring\n- Apply detailed rubrics consistently without emotional fatigue\n- Process documents and extract information objectively\n- Score applications using pre-programmed system prompts and South African context as guidelines\n- Provide consistent baseline scoring as starting point for human review\n- Manage the \"mind-numbing\" repetitive scoring work\n\n**What AI Should NOT Do:**\n- Make final decisions autonomously\n- Evaluate personal stories or resilience narratives\n- Assess \"human stuff\" like empathy or character\n- Handle contextual judgment about South African impact\n- Make truly subjective determinations about who \"deserves\" funding\n- Replace human judgment on non-automatable elements\n\n**Specific AI Implementation Concerns:**\n\n*Reasoning Limitations:*\n- \"AI as it is can't reason... It's purely statistical. It's probabilistic.\"\n- \"It doesn't even understand what it's doing itself\"\n- \"Autonomous end to end doesn't work\"\n- AI can only handle \"things that require statistical pattern matching\"\n\n*Decision-Making Limitations:*\n- AI cannot handle \"intangibles like empathy\"\n- Cannot make \"actual decision making\" - this must remain in human domain\n- \"AI is not there yet maybe one day but it's not there\"\n\n*Transparency & Verification:*\n- Implicit requirement that AI scores be explainable and reviewable\n- Human must be able to understand and potentially override AI scoring\n- \"Leaving room for the human to actually review and almost give a second opinion basically\"\n\n**Trust & Confidence Requirements:**\n- Willing to trust AI for algorithmic, rule-based tasks\n- Insists on domain expertise for subject-matter assessment\n- Would want subject matter experts to \"shadow\" and provide second opinions on specialized topics\n- Trust based on clear system prompts and predetermined rubric criteria\n\n**System Requirements:**\n- Must have \"system prompt literally where those instructions...are pre-programmed\"\n- Should be \"pre-programmed...give it quant South African context such that it can go in and actually use that as a rubric\"\n- South African context is essential - not generic AI scoring\n- Needs to maintain flexibility for human correction and revision\n\n**Key Insight from Experience:**\n- Has written a book on AI limitations\n- Builds AI solutions for clients - understands both capabilities and constraints intimately\n- This expertise makes him more realistic about what AI can achieve\n- Clear distinction between marketing claims and actual AI capability\n\n---\n\n## 6. Suggestions & Ideas\n\n**For Addressing the Cold Start Problem:**\n\n1. **System flexibility is partial solution:**\n   - Current ability to revise scores is \"an intervention in itself\"\n   - Described as Bayesian approach: \"change your beliefs as you gather more evidence\"\n   - Sufficient as interim solution but not ideal\n\n2. **Institutional memory system:**\n   - Would benefit from being able to review previous years' adjudications\n   - Could establish benchmarks across years\n   - \"If there's that institutional memory literally for every adjudicator that would help actually\"\n   - Would reduce the \"little island\" problem of starting fresh annually\n\n**For Reducing Bias & Improving Comparability:**\n\n1. **Thematic grouping by subject area:**\n   - \"If there's a theme where it's almost closely neat around a subject...that would help in terms of objectivity\"\n   - Group similar research areas together for comparison\n   - Instead of comparing \"nanotechnology to someone who's doing animal husbandry\"\n   - Example: \"if you had 10 submissions dealing with water and finding the best solution around water then it's easier to judge to compare\"\n   - Mitigates bias toward personally resonant topics\n   - Allows for more meaningful relative comparison\n\n2. **Subject matter expert consultation:**\n   - Have domain specialists \"shadowing\" for specific research areas\n   - Not broad field expertise but narrow specialization\n   - \"A subject matter expert in the particular research not in a broad area\"\n   - Would prevent dismissing breakthrough research due to personal unfamiliarity\n   - \"I might be throwing away the next Nobel Prize winner because it doesn't look appealing\"\n\n**For Addressing Scoring Scale Issues:**\n\n1. **Increase bin granularity:**\n   - Current 0-4 scale is \"too blunt\" with \"bin size is too big\"\n   - \"High quality three and a low quality three, they're all bunched up\"\n   - Suggested movement toward finer-grained scale\n   - Would allow better differentiation within quality bands\n   - No specific number suggested, but clearly wants more distinction points\n\n**For Improving Final Assessment Process:**\n\n1. **Support for summation task:**\n   - Acknowledged as difficult but didn't propose specific solution\n   - Possibly could benefit from:\n     - Structured template for synthesis\n     - Review guidance or checklist\n     - System that surfaces key notes/highlights from review process\n   - Noted uncertainty if this is widespread need or individual issue\n\n**For Application Preparation & Support:**\n\n1. **Training for applicants and schools:**\n   - Some schools teach effective application articulation; others don't\n   - \"Maybe some of those people...need guidance or support or training\"\n   - Suggested intervention: support for universities in teaching application-writing skills\n   - Particularly for institutions producing lower-quality applications\n   - Goal: level playing field across geographic/institutional backgrounds\n\n**What NOT to Change:**\n\n- Rubric dimensionality: \"I think the rubric is quite comprehensive...mutually exclusive cumulatively exhaustive...don't think there any dimensions that are missing currently\"\n- Overall process: \"The process is amazing. I wouldn't change anything\"\n- Basic structure: The approach is sound, just needs refinement in execution\n\n**Priorities for AI Implementation:**\n\n1. **First priority:** Rubric-based scoring automation (explicitly mentioned as \"mind-numbing drudge work\")\n2. **Second priority:** Freeing human cognitive capacity for non-automatable elements (story, resilience, empathy)\n3. **Foundational requirement:** Maintain human oversight and revision capability\n\n---\n\n## 7. Key Direct Quotes\n\n**On the Core Challenge of Scoring:**\n> \"The most difficult one is always the first one because there's no frame of reference. There's no standard set. So literally you are flying in blind. So that could actually prejudice the person who's judged first because there's no frame of reference.\" (00:04:10)\n\n*Context: Explains the cold start problem and its impact on fairness*\n\n---\n\n**On AI's Appropriate Role:**\n> \"I would suggest a situation where there is AI with a human in the loop where AI does the drudge work the mind numbing stuff but leaving room for the human to actually review and almost give a second opinion basically.\" (00:08:19)\n\n*Context: Proposes the ideal AI-human collaboration model*\n\n---\n\n**On What Humans Must Assess:**\n> \"On the personal story where applicants are trying to show their resilience their story where they came from a broken home and against all odds they tended to overcome those obstacles. So that is very human because it has to resonate with you as the adjudicator and I don't think that element that little element in the process can be automated. So that's the one that I would bring to the table as a human being.\" (00:13:46)\n\n*Context: Defines what is non-automatable and requires human judgment*\n\n---\n\n**On Justifying Higher Scores:**\n> \"I've had a situation where I literally held their hand to say, 'Okay, I get what you're saying and I'll give you four because I I I get what you're saying, but you're not saying it, but I get it.'\" (00:14:52)\n\n*Context: Illustrates the gap between explicit articulation and implicit understanding, showing human judgment in action*\n\n---\n\n**On What \"Deserving\" Means:**\n> \"Deserve is where they have a really compelling story. They have all the requirements and their research is going to help as many people as possible. So in a way you find that in terms of impact the natural bias where is almost towards how many people is it going to help you know and how fundamental is the problem.\" (00:15:56)\n\n*Context: Defines his weighting criteria and reveals impact-focused bias*\n\n---\n\n**On the Personal Story's Weight:**\n> \"I think the story the story matters. For me the story matters...If it's a case of there's nothing amazing or bad, it's just neutral kind of thing. Then the story comes into the frame to say, 'Okay, what is this person's story?' So that's the one with the biggest weighting here.\" (00:18:20)\n\n*Context: Confirms personal narrative as the heaviest factor in his decision-making*\n\n---\n\n**On the Summation Challenge:**\n> \"When you're doing the process it's quite easy but that final assessment where you have to write to say okay I don't know somehow it's so difficult...I struggle with that. Yeah the final assessment where you're writing and just finding the summation of everything...For a candidate. Yeah. I find it quite a task.\" (00:18:20, 00:19:38)\n\n*Context: Identifies a significant cognitive bottleneck in the review process*\n\n---\n\n**On Subject Matter Expert Needs:**\n> \"Because of the current configuration where you have many subjects, I'm not a water purification expert. So if I could have a water purification expert shadowing and giving their second opinion around that particular one then yes that would add value because I might be throwing away the next Nobel Prize winner because it doesn't look appealing because you know so that would actually help of having a subject matter expert in the particular research not in a broad area.\" (00:26:06)\n\n*Context: Identifies the risk of bias from lack of specialized knowledge*\n\n---\n\n**On AI's Fundamental Limitations:**\n> \"I'm quite aware of what AI can",
    "tokens_in": 11591,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Freedom Gumedze",
    "date": "2026_01_26 11_29 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Freedom Gumedze) \u2013 2026_01_26 11_29 SAST \u2013 Notes by Gemini.docx",
    "text_length": 4932,
    "analysis": "# OMT Discovery Interview Analysis: Freedom Gumedze\n\n## 1. Interviewee Role & Background\n\n**Role in OMT Review Process:**\n- Active reviewer for OMT postgraduate scholarship applications for \"a few years\"\n- Also an applicant to OMT programs (received sabbatical funding in 2020; shortlisted for New Frontiers Award two rounds prior)\n- Experienced across multiple review contexts\n\n**Duration of Involvement:**\n- Multiple years as reviewer (exact duration not specified, but indicates \"three or four years\" of accumulated review experience at minimum)\n\n**Discipline/Expertise Area:**\n- Professor of Statistical Sciences at University of Cape Town\n- Biostatistician specializing in applied health research\n- Research focus: innovative statistical methods in dermatology, cardiology, and infectious diseases\n- Advanced credentials: C1-rated researcher (NRF rating), Master's in Mathematical Statistics, PhD in Statistics from UCT\n- Editorial experience: Associate Editor for *Journal of Applied Statistics* and *Biometrics* (premier journal, newly appointed for 3-4 years)\n- Additional academic roles: Department head (5 years, continuing for 3 more), sits on NRF rating panels, University of Cape Town promotions committee, recruitment committees\n\n---\n\n## 2. Current Review Process Description\n\n**Overall Approach:**\nFreedom describes a **mixed process** combining initial holistic reading with structured rubric application:\n\n> \"I'll read a portfolio. Obviously other things like you're looking at references, you're looking at whether there's a budget or not and things like that, but one kind of starts off looking at the core sort of content as to what the proposal what the proposal is about.\"\n\n**Specific Steps:**\n\n1. **Initial familiarization with rubric** - reviews rubric before/during assessment (looked at it \"a week ago\")\n2. **First read-through** - reads the portfolio holistically \"as I would read in a portfolio,\" getting a sense of the proposal's substance\n3. **Core content assessment** - evaluates:\n   - What the proposal is about\n   - Whether applicant has sufficient/good background for the work\n   - Whether they're already accepted at the proposed institution (for most categories)\n   \n4. **Criterion-based assessment** - returns to rubric to check against:\n   - Social impact\n   - Academy contribution\n   - Applicant benefits\n   - Innovation level\n\n5. **Detailed evaluation elements** - reviews:\n   - References\n   - Budget\n   - Institutional fit (overseas school/university quality)\n   - Research quality (particularly for postdocs and sabbaticals)\n\n6. **Comparative benchmarking** - applies historical comparison:\n   - Compares against \"previous one[s]\" across multiple years\n   - Uses accumulated 4-5 year records from similar processes\n   - Acknowledges this introduces bias but considers it necessary for calibration\n\n7. **Star rating assignment** - determines whether application meets 1-star through 4-star criteria\n\n**Tools/Systems Currently Used:**\n- OMT rubric (standardized across categories with level-specific criteria modifications)\n- Personal/historical records (Freedom maintains summary reports from multi-year reviews)\n- External sources for publication assessment:\n  - Google Scholar\n  - Scopus\n  - Bibliometrics systems (used at UCT)\n  - NRF rating information\n- Communication via letter (receives thank-you letters for reviews, but limited feedback)\n\n**Time Spent:**\nNot explicitly stated, but context suggests:\n- Typically reviews up to 20 applications per round (mixed categories)\n- No indication of time per application or total time commitment provided\n- Indicates reviewers have \"limited time\" (mentioned as constraint)\n\n---\n\n## 3. Pain Points & Challenges\n\n**Challenge 1: Ambiguous Publication Assessment Criteria**\n> \"Publications by the way that's loose because you're not told how you should do this publication. Should you be looking at Google Scholar? should be scopas... it's going to pick up something from MDPI or something. It's still going to be in Google Scholar. Um, so we're not told how you should assess the publications.\"\n\nThis requires subjective judgment and is time-consuming for thorough review.\n\n**Challenge 2: NRF Rating as Outdated/Imperfect Metric**\n> \"if you can look at their work now maybe they are not they not at that at at at at that rating... the rating is dwindling in the sense that that's not the only metric that you you would measure someone's uh research impact.\"\n\n- Ratings can be 5+ years old\n- Doesn't reflect current research quality\n- Overreliance on ratings can be misleading\n- Universities like UCT moving away from ratings\n\n**Challenge 3: Missing Standardization for Key Submissions**\n- Applicants aren't instructed on publication sources to provide\n- Social impact/sustainability goal contributions \"not asked of the candidates\u2014they can just put it in any way that they want\"\n- Biblometrics not requested despite being valuable for assessment\n- No standardized guidance on what metrics should be included\n\n**Challenge 4: Disciplinary Mismatch in Rubric Design**\n> \"The rubric is built on a master's by dissertation... if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part...\"\n\n- Rubric assumes dissertation-heavy master's (traditional model)\n- Many disciplines (statistics, data science) are course-work heavy with small dissertation components\n- Cannot fairly evaluate coursework-based vs. dissertation-based programs using same criteria\n\n**Challenge 5: NRF Model Over-Reliance**\n> \"it's built on a NRF kind of the only thing that has been added is that you have got this social responsive uh social sort of like impact that you're looking for in fact everywhere you are looking for this social impact which I think it's a good thing but... it's kind of leans towards NRF sort of like way of assessing proposals.\"\n\n- Rubric structure mirrors NRF assessment heavily\n- May not be optimal for scholarship assessment\n- Social impact addition is positive but broadly applied\n\n**Challenge 6: Lack of Reviewer Calibration & Feedback Loop**\n> \"I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group because I'm assuming that I'm I look at the statistics uh sort of uh uh projects or rather proposals but maybe there's another statistician that looks at at that but I want to know at the end what they they desire so that maybe we can calibrate that from this side\"\n\n- No visibility into how other reviewers scored same applications\n- No indication of whether consensus or moderation process exists\n- Unclear if scores are averaged or reviewed by committee\n- Cannot calibrate scoring without feedback on outcomes\n\n**Challenge 7: Sabbatical Proposals Less Constrained**\n> \"What's more kind of like loose is when it comes to sabaticals people will find that I want to go on sabatical but it's not necessarily going to a particular sort of like institution overseas... they just putting together a a sort of a a research project\"\n\n- Less structural clarity than degree-seeking categories\n- Applicants have more flexibility but less guidance\n\n**Challenge 8: Potential for Subconscious Bias in Benchmarking**\n> \"that does happen. I don't think it's completely independent that you you one can't compare with past that they have seen.\"\n\n- Acknowledges that historical comparison introduces bias\n- Different from context-appropriate benchmarking (e.g., scoring against current cohort standards)\n- Not done systematically for OMT but questions whether it should be\n\n---\n\n## 4. What They Value in Applications\n\n**Core Research Quality Elements:**\n\n1. **Innovation & Novelty**\n> \"I can sort of have a feel as to whether something is innovative or not\"\n- Uses expertise from editing peer-reviewed journals to assess innovation\n- Looks for novel statistical/methodological approaches in their discipline\n\n2. **Applicant-Proposal Fit**\n> \"the person has chosen a school has got has got sufficient background has good background to actually pursue the work they saying they're going to be pursuing\"\n- Sufficient background for the proposed work\n- Good background (quality of preparation)\n- Alignment between applicant's training and proposed research\n\n3. **Institutional/School Fit**\n> \"looking at they want to go overseas. Is that a good school or not? Is it aligned with what they are currently doing?\"\n- Quality of the overseas institution\n- Strategic alignment with applicant's current work and career trajectory\n\n4. **Research Standing (for Postdocs/Sabbaticals)**\n- Depth of understanding of their own research quality\n- Current research impact (not just ratings)\n- Publication venues and quality (not just H-index)\n- Active research networks and collaborations\n\n5. **Social Impact Alignment**\n> \"Are these things there for this? Because I I remember in the last not last round this last round there were applicants who even in the scoring obviously there will be one star candidate but there's some information that's probably missing or not properly articulated in the portfolio\"\n- Social impact goals clearly articulated\n- Alignment between stated social activities and career direction\n- Sustainability goals clearly linked to proposal\n\n6. **Academic Background Quality**\n- Educational trajectory\n- Prior training adequacy\n- CV quality and completeness\n\n**Critical Assessment Criteria by Category:**\n\n**For Master's Students:**\n- Should NOT primarily focus on publications (though publications are a bonus)\n- Academy contribution potential\n- Applicant's personal benefit from degree\n- Social impact/benefit\n- Clarity of proposal articulation\n\n**For Postdocs/Sabbaticals:**\n- High impact of work\n- Publication venues/quality (not just quantity)\n- Current research standing and trajectory\n- NRF rating (with caveats about outdatedness)\n- Peer references and endorsements\n\n**Red Flags/Negative Indicators:**\n\n1. **Poorly Articulated Proposals**\n> \"there were applicants who even in the scoring obviously there will be one star candidate but there's some information that's probably missing or not properly articulated in the portfolio\"\n- Missing educational background information\n- Unclear social activity alignment\n- Incomplete or vague proposal descriptions\n\n2. **Misaligned Institution Selection**\n- Choosing overseas school not aligned with current work\n- Poor strategic fit for career development\n\n3. **Low Research Quality Masked by Metrics**\n> \"just it's just not up to standard... you can just look at the numbers and say h index of 30 this person is a very good when in fact what they are publishing is just it's just not up to standard\"\n- High H-index with low-quality publications\n- Numbers don't reflect actual research caliber\n\n4. **Outdated Credentials**\n- NRF ratings that are 5+ years old\n- No evidence of current research activity\n\n**How They Weight Different Factors:**\n\nThe hierarchy appears to be:\n1. **Core proposal quality/innovation** (primary driver)\n2. **Applicant-institution fit and quality** (secondary)\n3. **Research standing** (for senior categories)\n4. **Social impact alignment** (important but secondary to research quality)\n5. **Budget** (reviewed last, described as \"the last thing\")\n\n> \"Budget is the last thing actually that I that I that I look at even though it's part of the it's part of the as part of the portfolio.\"\n\n---\n\n## 5. Views on AI/Technology\n\n**Openness to AI Assistance:**\nFreedom does not explicitly reject AI but is cautious and sets clear boundaries.\n\n**What Would Be Delegated (AI/Assistant Tasks):**\n\n1. **Research Quality Digging** (HIGH PRIORITY)\n> \"I would like to say the research because there's information that needs to be dug out there that I have to dig. And if it's not in the if it's not it is going to be in the pack, they're going to list that they're going to list their papers, which is what people normally do\"\n\n- Extracting and compiling publication metrics (bibliometrics)\n- Calculating research impact indicators not requested from applicants\n- Verifying publication quality and venue impact\n- Assessing research output quality beyond what applicants provide\n\n2. **Checklist/Completeness Verification (for Masters & Lower Categories)**\n> \"I would like there is perhaps just a summary of their academic background... it's almost like checks academic background... it's really maybe just a checklist. Are these things there for this?\"\n\n- Confirming all required information is present\n- Summarizing academic background\n- Verifying institutional fit\n- Cross-checking proposal consistency\n- Flagging missing or poorly articulated elements\n\n**What Would NOT Be Delegated:**\n\n1. **Core Proposal Evaluation** (MUST REMAIN WITH HUMAN)\n> \"I still think as a reviewer you still want to read the portfolio\"\n\n- Assessing innovation and novelty\n- Judging research quality\n- Determining star ratings\n- Final synthesis and judgment\n\n2. **Disciplinary Expertise Application**\n- Evaluating appropriateness of methodology\n- Assessing research quality within field context\n- Making judgment calls on innovation\n\n**Concerns & Trust Requirements:**\n\nWhile not explicitly stated as concerns, Freedom's approach implies several requirements:\n\n1. **Need for Expert-Level Assessment**\n- Task delegation assumes sophisticated understanding of research quality\n- Cannot be just surface-level data extraction\n- Requires going \"deep\" as stated in the interview question: \"going deep and doing some preparation\"\n\n2. **Disciplinary Specificity**\n- Different fields assess publications differently\n- Need field-aware bibliometric interpretation\n- Statistical/data science disciplines have unique characteristics\n\n3. **Transparency & Feedback**\n> \"I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group\"\n- Needs visibility into how recommendations are used\n- Wants to know outcomes to calibrate future reviews\n- Seeks transparency in scoring aggregation process\n\n**Model Precedent References:**\n\nFreedom references other established processes as models for AI/tech integration:\n\n1. **NRF Rating Panels**\n> \"I sit on NRF rating panel we actually share scores and then there kind of like moderators where there's kind of a a decision right at the end where I don't want to say one gets forced to to to agree but some moderation where you kind of have to come to a consensus\"\n- Shows comfort with structured moderation processes\n- Not uncomfortable with formalized reviewer calibration\n\n2. **EU/MRC UK Scientific Committee Model**\n> \"There's something called a scientific committee that actually sits and deliberates on the scoring that actually... they'll come to a consensus that maybe they're funding two or three out of maybe 20 research proposals\"\n- Appreciates deliberative group processes\n- Willing to invest time in consensus-building for high-stakes decisions\n- Recognizes this adds burden but provides confidence\n\n**Implicit Trust Markers:**\n- Values process transparency\n- Wants accountability and feedback loops\n- Expects any tool/process to maintain rigor equivalent to peer-review standards\n- Comfortable with moderation but requires systematic approach\n\n---\n\n## 6. Suggestions & Ideas\n\n**Major Improvements Suggested:**\n\n**1. Separate Rubric Categories by Master's Type**\n> \"The master's programs are changing. The rubric is built on a master's by dissertation... So the masters were seen at least in my discipline whether locally or overseas... if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part... definitely you can't be evaluating the masters by dissertation the same way as you would evaluate the masters by coursework... So that so I would like that to be separated.\"\n\n**2. Implement Reviewer Score Sharing & Moderation Process**\n> \"I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group... maybe we can calibrate that from this side... reviewers definitely I sit on NRF rating panel we actually share scores and then there kind of like moderators where there's kind of a a decision right at the end where... you kind of have to come to a consensus of some sort\"\n\n- Share scores across reviewers of same applications\n- Implement formal moderation/consensus process (model on NRF approach)\n- Unknown if this exists downstream; should clarify\n- Acknowledge burden but emphasize value for high-stakes funding\n\n**3. Standardize Publication Assessment Guidance**\n- Specify whether evaluators should use Google Scholar, Scopus, or other sources\n- Provide guidance on assessing publication quality\n- Move beyond simple H-index counting\n\n**4. Formalize Data Submission Requirements**\n- Request bibliometrics in applicant submissions\n- Specify what social impact data should be provided\n- Request specific publication venue information\n- Standardize how applicants present research credentials\n\n**5. Create Feedback Loop for Reviewers**\n> \"So in terms of the process itself, there's nothing that I think one can sort of like maybe change but I would like to know... what they they desire so that maybe we can calibrate... maybe we can give feedback that there's a set of proposals that really didn't have to go through this stage\"\n\n- Share outcomes of funding decisions with reviewers\n- Provide aggregate scoring feedback\n- Indicate which applicants actually received funding\n- Allow reviewers to calibrate future assessments\n\n**6. Caution Against Over-Reliance on NRF Ratings**\n> \"I would imagine other sort of research organization in South Africa are still using rating but if they use the rating is dwindling... top university... we are not relying on rating. We're relying on peers who write about the individual\"\n\n- Don't weight NRF ratings too heavily\n- Emphasize referee reports\n- Account for rating currency/recency\n- Consider peer endorsements as primary metric\n\n**7. Improve Clarity on Sabbatical Expectations**\n- Provide more structure for what constit",
    "tokens_in": 7374,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Frelet De Villiers",
    "date": "2026_01_26 12_54 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Frelet De Villiers) \u2013 2026_01_26 12_54 SAST \u2013 Notes by Gemini.docx",
    "text_length": 5655,
    "analysis": "# OMT Discovery Interview Analysis: Frelet De Villiers\n\n## 1. Interviewee Role & Background\n\n**Role:** Head of Music Department at University of the Free State; OMT reviewer for postgraduate funding applications\n\n**Duration of Involvement:** Approximately 4-5 years with OMT\n\n**Additional Experience:** Also serves as a reviewer for the National Research Fund (NRF); has been in academia for approximately 12 years; has experience as a UNISA examiner; personally applied for bursaries and funding as a younger academic\n\n**Discipline/Expertise:** Music (Arts discipline); however, also reviews across disciplines (e.g., NRF reviews are not music-specific)\n\n---\n\n## 2. Current Review Process Description\n\n**Process Flow:**\n\n1. **Initial Contact & Commitment:** OMT contacts reviewer to ask if they're available; interviewee prioritizes OMT work due to belief in the organization's mission\n\n2. **Initial Scanning:** Reviews all applicants at once in an initial pass to \"get a sense of who the students are\" (note: minimum requirements screening is already completed by OMT staff)\n\n3. **Detailed Review Approach:** \n   - Works through applications one by one\n   - Randomly selects a candidate to start with\n   - Reads through \"the questions one by one\"\n   - Refers to reference letters from supervisors as key information source\n   - Compares candidates iteratively as progressing through batch\n\n4. **Scoring & Calibration:**\n   - Provides \"very honest feedback on what I see in the application regarding merit\"\n   - Scores immediately upon first reading (influenced by UNISA examiner background of giving first impressions)\n   - Does NOT submit immediately; saves and returns later\n   - Reviews and adjusts scores after reading all applications to ensure consistency across cohort\n   - Identifies top 3 candidates intuitively, then cross-references with numerical scores\n   - May revise scores during this calibration phase\n\n5. **Feedback Documentation:**\n   - Writes detailed comments explaining scores and recommendations\n   - \"Beautifies\" sentences and refines language after initial scoring\n   - Provides specific improvement suggestions for unsuccessful applicants\n\n**Batch Management:**\n- Does not review many applications at once\n- Explicitly avoids doing multiple applications in a single day session\n- Maintains focus and \"fresh thoughts\"\n\n**Tools/Systems Currently Used:**\n- OMT online platform with split-screen functionality (praised as \"very user friendly\")\n- Compares favorably to NRF system which requires managing separate PDF and rubric documents\n\n**Time Commitment:** Not explicitly stated, but implies significant investment in detailed feedback writing\n\n---\n\n## 3. Pain Points & Challenges\n\n**Challenge 1: Rubric Limitations**\n- Rubric can be \"very restrictive and not really compelling\"\n- Sometimes things reviewers judge \"does not really fit into the rubric\"\n- Has suggested rubric improvements (though cannot recall specifics during interview)\n\n**Challenge 2: Lack of Fine-Grain Calibration Guidance**\n- Unclear scoring thresholds (what constitutes a 3 vs. 4)\n- No explicit percentage equivalents provided\n- Reviewer develops own internal mental scale\n\n**Challenge 3: Reference Letter Authenticity**\n- Cannot determine if letters are genuinely written by referees or drafted by students\n- Cannot detect if letters have been processed through AI\n- Must take all documents \"at face value\"\n\n**Challenge 4: Comparing Value Across Disciplines**\n- Subjective nature of \"value\" differs by reviewer framework\n- Generic statements about \"adding knowledge to existing knowledge\" are unhelpful\n- Has to evaluate quality across fields where they lack deep expertise\n\n**Challenge 5: Inconsistent Financial Realism**\n- Some proposals have inflated budgets (e.g., claiming \u20ac35,000 for European travel when actual costs are much lower)\n- Difficult to assess feasibility of research plans (e.g., claiming 4 articles, 3 conferences, 2 books during postgraduate study)\n- Red flags about whether applicant genuinely needs to be overseas vs. seeking trip\n\n**Challenge 6: Benchmarking Across Years**\n- Cannot remember previous year's cohort quality/standards\n- Does multiple reviews across different organizations, making comparison difficult\n- Essentially benchmarks relative to current cohort only\n\n**Challenge 7: Process Consistency Concerns**\n- Worried about inconsistent rubric application across reviewers\n- Notes that what seems valuable to one reviewer may not be to another\n- No way to ensure all reviewers start from same calibration point\n\n---\n\n## 4. What They Value in Applications\n\n**HIGH VALUE CRITERIA:**\n\n**A. Motivation Letter / Personal Statement:**\n- \"The motivation letter tells me a lot about the person itself\"\n- Look for evidence of resilience and agency, NOT victim mentality\n- Differentiates between: (1) people who blame circumstances vs. (2) people who acknowledge circumstances but demonstrate what they've achieved despite them\n- Values honesty about background paired with demonstrated initiative (\"I come from this background but I did this with what I have already\")\n- Writing style reveals authenticity; inconsistency between motivation and proposal can suggest outsourced writing\n\n**B. Supervisor Reference Letters:**\n- Quality of reference letter is crucial indicator\n- Detailed, substantive letters (multiple paragraphs) indicate supervisor's genuine investment\n- Generic, short letters (1-2 paragraphs) raise concerns\n- Personally experienced writing reference letters, so understands effort required for good ones\n\n**C. Research Proposal/Project Viability:**\n- \"Very important\" - \"the openers must get value for their money\"\n- Internal coherence: Does title align with aims? Do aims align with methodology? Does methodology align with research question?\n- \"Golden thread\" within proposal itself is essential\n- Realistic scope and deliverables (red flag: claiming 4 articles + 3 conferences + 2 books)\n- Feasibility assessment: Can this actually be accomplished?\n\n**D. Demonstrated Value/Impact:**\n- Must articulate specific, meaningful value (not generic \"contributing to knowledge\")\n- Examples of strong value statements: \"community program that will uplift 20 community members\"\n- Value judgments are subjective but should be clear and defensible\n- OMT's significant investment requires confidence funds will be used effectively\n\n**E. Academic Record (LOW priority):**\n- \"Not that important to me\"\n- Acknowledges contextual factors: students may have \"horrible time\" due to circumstances beyond their control, unsupportive lecturers, or different institutional grading scales\n- Cautions against penalizing students from institutions with different marking standards\n- Some variation between institutions makes direct comparison unreliable\n\n**RED FLAGS:**\n\n1. **Unrealistic Budgets:** Inflated costs for travel/resources suggest either dishonesty or lack of due diligence\n2. **Unnecessary Travel:** Proposing to go abroad for things that could be done online (e.g., meetings)\n3. **Victim Mentality:** Complaining about circumstances rather than demonstrating agency\n4. **Unrealistic Deliverables:** Overpromising outputs that cannot possibly be achieved in timeframe\n5. **Disconnected Proposal:** Aims/methodology/research question not aligned; lacks internal \"golden thread\"\n6. **Generic Value Statements:** Vague claims that apply to any research project\n7. **Weak Reference Letters:** Short, generic, uninformed letters suggest lack of supervisor engagement\n\n**WEIGHTING/PRIORITY:**\n1. Proposal quality & feasibility (most important)\n2. Motivation/personal statement & resilience (very important)\n3. Quality of reference letters (important)\n4. Academic record (least important)\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Attitude:** Cautiously skeptical; concerned about AI's role in review but pragmatic about current limitations\n\n**Specific Concerns:**\n\n1. **Authenticity Problems:**\n   - \"With AI these days even the reviewer or the reference referee can also put that through AI and I mean you can't even see if it's a student writing or whatever\"\n   - Cannot distinguish AI-generated from human-written text in reference letters\n   - Impossible to verify authenticity of any document submitted\n\n2. **Value of Automated Assistance:**\n   - Does not believe an AI assistant could meaningfully screen for \"red flags\" in her work\n   - \"If it's a red flag for the assistant, it's not necessarily a red flag for me because we will never be on the same level of thinking of conceptualization of experience\"\n   - Concerned about false positives: AI might flag things as inconsistent that aren't, or miss context\n\n3. **What Automation COULD Help With:**\n   - Checking minimum requirements (already being done by OMT)\n   - Possibly doing basic fact-checking on budgets/costs\n   - Organizing/presenting information more clearly (though current OMT system already good)\n   - Expert consultation in unfamiliar disciplines (if expert is human, not AI)\n\n4. **What Should NOT Be Automated:**\n   - Judgment calls on value and merit\n   - Evaluation of proposal coherence\n   - Assessment of applicant motivation/character\n   - Final scoring/recommendations\n\n5. **Procedural Concerns:**\n   - Worried about time delays if relying on assistant: \"they don't always stick to your timeline. So now I have to wait for the assistant to come back to me and in the meantime I would have been finished already\"\n   - Values autonomy and control over process\n   - Self-describes as \"perhaps I'm a control freak\"\n\n6. **Philosophy on Trust:**\n   - \"If they lie to me it's on their black book\" - takes philosophical approach that she can only review what's in front of her; if information is false, responsibility is on applicant\n   - This extends to accepting AI-generated text: since it's impossible to detect, must treat all submissions as authentic\n\n7. **Preference for Human Expertise:**\n   - \"If I'm not sure about something, I will go and ask an expert in a certain field. I will not trust an assistant.\"\n   - Values personal, expert consultation over algorithmic support\n\n---\n\n## 6. Suggestions & Ideas\n\n**What Should Be Changed:**\n\n1. **Rubric Improvements:**\n   - Current rubric sometimes \"restrictive and not really compelling\"\n   - Reviewer considers values outside rubric that should potentially be incorporated\n   - Offered to provide written feedback on rubric if sent specific version\n   - Implied need for clearer scoring guidance (what distinguishes 3 from 4)\n\n2. **System Strengths to Maintain:**\n   - \"The Openheimer's website is really very user friendly because you can have the split screen and everything is really available\"\n   - Explicitly praised compared to NRF system\n   - No requests to change current OMT platform interface\n\n3. **Process-Level Suggestions:**\n   - Continue using multiple reviewers (different values/frameworks mean plurality is valuable)\n   - Consider providing pre-review calibration session for reviewers (though not explicitly suggested, implied by benchmarking challenge)\n   - Ensure feedback is detailed and specific to help applicants improve for future applications\n\n4. **Assistance with Unfamiliar Disciplines:**\n   - When reviewing outside her expertise, could benefit from disciplinary expert consultation\n   - But would prefer to initiate this herself rather than have system mediate it\n   - Does not need AI to flag when she's out of depth; recognizes this herself\n\n5. **Information Organization:**\n   - Current system already solves this well with split-screen\n   - Unlike NRF's fragmented document approach\n   - No improvements needed in this area\n\n**What Should NOT Change:**\n\n- Core review responsibility and judgment\n- Final recommendation/scoring authority\n- Detailed feedback-writing process\n- Applicant evaluation criteria\n\n---\n\n## 7. Key Direct Quotes\n\n**Quote 1: On OMT's Mission and Impact**\n> \"I really believe in the Open Trust. So I really want to make time for them because I I really think they're also gracious in their funding. And some of our students have also received funding and it really really makes a difference. One of our students have received it now as well and if it wasn't for them this person just couldn't do her M's degree you know so they are really setting students on their path for life.\"\n\n*Context: Explains her motivation for investing significant time in OMT reviews despite other demands*\n\n---\n\n**Quote 2: On Weighing Proposals vs. Academic Records**\n> \"The academic record is not that important to me because there are circumstances um I know from our own students they just have the most horrible here and just nothing works out for them or they have a lecturer that that doesn't fancy them and they get lower marks you know so academic record for me and then also I mean if you for example music um what we have in our module for say example music theory is not the same as UC has so now as oh all our students usually get 80 and now I get someone that gets 60 so this is a bad candidate so I I the academic record I must say that is the least important for me.\"\n\n*Context: Explains her holistic approach to candidate evaluation and refusal to over-weight GPA*\n\n---\n\n**Quote 3: On Identifying Resilience vs. Victimhood**\n> \"When you look at the motivation you can see when they are just um complaining if that makes sense. You you get some people and they will say I come from this very bad background and I don't have this and I don't have that. Um then you get other people who say I come from the background but I did this with what I have already and I want to um achieve this.\"\n\n*Context: Describes her ability to differentiate between applicants who use circumstances as excuses versus those who demonstrate agency*\n\n---\n\n**Quote 4: On OMT's Fiduciary Responsibility**\n> \"Is there really value in the study if they've done that so well? But you I'm not really looking between a connection between the motivation and the proposal. Um, the openers must get value for their money. So I cannot say oh this is a wonderful project and then nothing can come from it. So is it really viable? Is it really something that can work? Are sometimes these people they say they are going to have four articles and three conferences and two books. I mean it's just not possible. So when you you look at the proposal, you can also see this person is really so far removed from reality and then are they really going to to keep their promise and you know because it's it's a lot of money that they really give away a lot of money.\"\n\n*Context: Explains her assessment criteria and concern for due diligence with large funding amounts*\n\n---\n\n**Quote 5: On Red Flags in Budget Proposals**\n> \"I have this preposterous or or what is it like exuberant amounts and then I know a a ticket to um you know uh Europe is definitely not 35,000 rand and I know a hotel or a whatever um will not cost that. So if if they really inflate the prices like that that is a red flag for me. And then when I see that I will be more cautious when I look at the value and the proposal and things like that.\"\n\n*Context: Concrete example of how she identifies dishonesty or lack of due diligence in applications*\n\n---\n\n**Quote 6: On Reference Letters as Trust Signals**\n> \"I was also asked a few times to to write reference letters. So I know what it feels like. You really want this person to get something you will really put everything in that. So that's also for me important.\"\n\n*Context: Shows how her own experience informs her interpretation of reference letter quality as indicator of supervisor engagement*\n\n---\n\n**Quote 7: On AI and Authenticity Detection**\n> \"And especially with AI these days even the reviewer or or the you know the reference referee um can also put that through AI and I mean you can't even see if it's a student writing or whatever. So um yeah you you have to believe what you get and you also have to believe the facts that you get. I mean so I I always have I have the philosophy um that is on their black book. If if they lie to me it's on their black book.\"\n\n*Context: Pragmatic stance on inability to verify authenticity, combined with philosophical acceptance of risk*\n\n---\n\n**Quote 8: On Why Automation Cannot Replace Expert Judgment**\n> \"Well, once again, if it's a red flag for the assistant, it's not necessarily a red flag for me because we will never be on the same level of thinking of conceptualization of experience. Um the the assistance we need is the people that that go through applications and see that minimum requires requirements are are met.\"\n\n*Context: Clear articulation of why AI/automated assistance is limited; only appropriate for clerical tasks already being done*\n\n---\n\n**Quote 9: On Subjectivity in Value Assessment**\n> \"You see that is now why hopefully I I think they have more than one reviewer for the whole process. Now it's not only one. So once again what is valuable for me in a study is not valuable for you in a study. So it all depends on your own um reference your own framework.\"\n\n*Context: Acknowledges fundamental subjectivity while endorsing multiple reviewers as solution*\n\n---\n\n**Quote 10: On Feedback as Essential for Applicant Development**\n> \"I had um applied for many, you know, burseries and applications things when I when I was younger as well, and it was so sad never to get really feedback because what what must I do? Why didn't I get it? What must I do better next time? So, I'm very I'm really very um you know um de detailed in my comments and what what why is this not the best application or what must be improved?\"\n\n*Context: Personal motivation for writing detailed feedback; pays forward her own experience of rejection without explanation*\n\n---\n\n## 8. Unique Insights\n\n**Insight 1: The \"Black Book\" Philosophy**\nDe Villiers articulates a distinctive epistemological stance",
    "tokens_in": 8898,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Martin Clark",
    "date": "2026_01_15 13_55 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Martin Clark) \u2013 2026_01_15 13_55 SAST \u2013 Notes by Gemini.docx",
    "text_length": 6520,
    "analysis": "# OMT Discovery Interview Analysis: Martin Clark\n\n## 1. Interviewee Role & Background\n\n**Role in OMT Review Process:**\nMartin Clark is an adjudicator/reviewer for the Oppenheimer Memorial Trust, responsible for evaluating postgraduate scholarship applications across different academic levels (Master's, PhD, sabbatical/postdoctoral).\n\n**Duration of Involvement:**\nApproximately 3-4 years of experience as an adjudicator.\n\n**Discipline/Expertise Area:**\n- Geologist with an Earth Science BSc\n- Based at the University of the Free State in Bloemfontein, South Africa\n- Originally from Canada\n- Studied in Germany (reference to \"pedantic reviews\")\n- Runs a laboratory and manages students\n- Self-describes as having broad knowledge across earth sciences but acknowledges gaps in biology and other specialized fields\n\n---\n\n## 2. Current Review Process Description\n\n**Step-by-Step Process:**\n\n1. **Initial engagement:** Receives a link to all publications on the OMT website\n2. **Application parcellation:** Separates applications by level (Master's, PhD, postdoc/sabbatical) to calibrate expectations of excellence appropriately for each category\n3. **Time allocation:** Allocates time carefully to avoid adjudication fatigue, typically reading no more than 5-10 applications in one sitting\n4. **Fresh-minded review:** Reads each application with a fresh mind to properly value both the proposal and the proposing person\n5. **Rubric completion:** Completes a rubric using a 1-4 point scale across specific categories\n6. **Final assessment:** Provides a final 5-point scale assessment of the candidate\n7. **Contextual commenting:** Adds comments explaining nuanced decisions, particularly where perception of grade conflicts with rubric requirements\n\n**Tools/Systems:**\n- OMT website/portal for accessing applications\n- Standard rubric provided by OMT\n- AI tools (personal use): Uses AI to verify logic and validate decision-making approaches\n- Manual processes: Reading, note-taking, deep research for unfamiliar subjects\n\n**Time Spent:**\n- Allocates time strategically to read 5-10 applications maximum per sitting\n- Views this as a pro bono commitment undertaken in addition to primary work responsibilities\n- Notes that pressure to move faster through applications on some days can impact quality\n\n---\n\n## 3. Pain Points & Challenges\n\n**Rubric Rigidity:**\n- The rubric categories are \"rather rigid\" and don't accommodate disciplinary differences\n- Specific metrics (e.g., Golden Key Society membership) disadvantage otherwise excellent candidates who lack access to resources\n- Binary scoring system (1-4 point scale, then 5-point final assessment) lacks sufficient granularity to make meaningful distinctions\n\n**Adjudication Fatigue:**\n- Risk of fatigue affecting quality if reviewing too many applications consecutively\n- Trade-off between speed and thoroughness when there's time pressure\n\n**Subject Matter Expertise Gaps:**\n- Difficult to evaluate applications in disciplines outside geological expertise (e.g., butterfly subspecies in biology)\n- Requires significant \"deep diving\" to understand niche topics, consuming additional time\n- Cannot always appreciate specialized significance within unfamiliar fields\n\n**Edge Cases and Undefined Scenarios:**\n- Applications that don't fit neatly into defined boxes are challenging\n- Tension between local/community-focused projects (showing passion) and metrics requiring national/global significance\n- Difficulty weighing variables when they're not equally weighted or when circumstances are novel (e.g., candidates with 20 years of industry experience vs. typical age progression)\n\n**Benchmarking Across Institutions:**\n- Significant variation in grading standards between institutions (e.g., \"C\" at UCT vs. \"B\" at Nelson Mandela University)\n- Misalignment between academic grades and how candidates express themselves in applications\n- Limited ability to contextualize these differences within the current system\n\n**Spelling/Presentation Issues:**\n- Personal bias toward penalizing spelling mistakes (German education background) conflicts with fairness\u2014not all applicants have access to spell-check technology\n- Must constantly weigh presentation quality against actual capability and potential\n\n**Limited Scoring Granularity:**\n- 4-point rubric scale (potentially divided into 25% increments) insufficient to distinguish between candidates at similar levels (e.g., difference between 75% and 100%)\n- 5-point final scale similarly restrictive\n\n**Bias and Frame of Reference:**\n- Acknowledged that adjudicators naturally find it easier to evaluate applications following career paths similar to their own\n- Difficulty understanding life experiences different from own (e.g., township backgrounds) despite efforts to remain objective\n\n---\n\n## 4. What They Value in Applications\n\n**Key Criteria:**\n\n1. **Passion (prioritized over cleverness)**\n   - Values passion as difficult to teach and highly indicative of potential\n   - Looks for evidence of genuine enthusiasm for the work\n\n2. **Positive Outlook for Change**\n   - Seeks evidence of commitment to local, continental, or global change\n   - Indicates future leadership potential\n\n3. **Aspiration**\n   - Key metric for identifying next generation of world leaders in respective fields\n\n4. **Circumstance Consideration**\n   - Takes into account applicant's background, opportunities, and life circumstances\n   - Evaluates \"excellence relative to what is possible with the time the candidate has had\"\n   - Values tenacity and evidence of overcoming adversity\n\n5. **Character and Life Path**\n   - Attempts to understand the applicant as a person through available materials\n   - Values non-linear paths to excellence\n   - Recognizes that different life experiences can lead to \"true excellence\"\n\n6. **Project Feasibility**\n   - Always examines proposed budgets for alignment with project scope\n   - Flags excessively high budgets that don't match project needs\n   - Flags extremely modest budgets that may indicate unfeasibility\n   - Views feasibility as critical even for potentially transformative work (\"cure for cancer\")\n\n7. **Clarity and Articulation**\n   - Values clear expression of ideas and projects\n   - Looks for applicants who can articulate where their work fits within broader contexts (especially important at postdoc/sabbatical level)\n\n**Red Flags:**\n\n1. **Spelling and presentation errors** (though tempered by consideration of access to resources)\n2. **Grades-to-presentation misalignment** across institution types\n3. **Excessively high or extremely modest budgets** suggesting lack of realistic planning\n4. **Unclear project significance** or inability to articulate relevance\n5. **Lack of evidence of passion or aspiration**\n\n**What Makes Applications Stand Out:**\n\n- Clear evidence of passion combined with feasible, well-thought-out projects\n- Applicants who overcome disadvantage to pursue ambitious goals\n- Projects with explicit connection to local or global change\n- Clear articulation of how work fits within discipline (especially for advanced degrees)\n- Realistic budget planning showing understanding of project scope\n- Multiple \"dots\" in the application that create a coherent picture of the person and their potential\n\n**How They Weigh Different Factors:**\n\n- Uses a balancing approach across multiple non-equally-weighted variables\n- Prioritizes passion over academic credentials or presentation quality\n- Adjusts expectations based on level (Master's vs. PhD vs. Postdoc)\n- Considers \"excellence relative to circumstance\" rather than absolute metrics\n- Willing to provide commentary justifying ratings that don't align with rubric scores when compelling mitigating factors exist\n- Views final decision as judgment call requiring holistic consideration of multiple factors\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Attitude:**\nEnthusiastically positive. Martin Clark states \"I love AI\" and believes it \"can help in almost any element\" of the adjudication process. Views AI as a tool to improve accuracy, coverage, and fairness.\n\n**Current AI Use:**\n- Uses AI to verify and validate his own logic in real-time during adjudication\n- Employs AI to test how his evaluations might be perceived by different stakeholders\n- Uses AI for project management on a separate system\n- Familiar with the concept of training AI on personality profiles of people one admires to receive personalized feedback\n\n**Specific Uses Envisioned:**\n\n1. **Logic Validation**\n   - Testing whether his reasoning is sound and well-articulated\n   - Evaluating how to best present nuanced evaluations to decision committees\n   - Helping articulate appreciation of candidates when there's tension between rubric scores and perceived merit\n\n2. **Subject Matter Expertise Supplementation**\n   - Data scraping and research assistance for unfamiliar fields\n   - Acts as \"CliffsNotes\" equivalent: providing summaries and contextual information on specialized topics (e.g., caterpillar diseases, butterfly subspecies)\n   - Enables verification of sources and significance claims in unfamiliar disciplines\n   - Reduces need for time-consuming \"deep diving\" research\n\n3. **Data Management and Analysis**\n   - Could assist with data scraping from online sources\n   - Potential for verifying sources and information\n   - Could help organize and cross-reference application materials\n\n4. **Presentation and Communication**\n   - Helping present candidate value more effectively\n   - Assisting in articulating nuanced decisions in ways decision-makers will understand\n   - Supporting clearer communication of reasoning for grade decisions\n\n**Concerns and Caveats:**\n\n1. **Uneven Implementation**\n   - Risk that different adjudicators will use AI differently, creating inequitable evaluation processes\n   - One adjudicator's AI use may not equal another's, potentially affecting consistency\n   - Questions about training and standardization across adjudicators\n\n2. **Accuracy Issues**\n   - AI can \"give wrong answers\" depending on implementation level\n   - Reliability concerns with certain AI systems\n\n3. **Technical Infrastructure**\n   - Connection issues to AI servers\n   - Potential downtime or accessibility problems\n   - Dependence on system stability\n\n4. **Transparency and Trust**\n   - Mentions \"comfortability argument\" that would need to be made regarding AI use\n   - Implicit concern about decision-makers and funders accepting AI-assisted adjudication\n   - Questions about whether AI recommendations should be binding or advisory\n\n5. **Scope Limitations**\n   - Not clear whether AI could help with rubric interpretation or scoring decisions\n   - Expresses uncertainty about AI's role in the core judgment-making aspects\n\n**What He Would NOT Want Automated:**\n\n- The fundamental judgment call of adjudication\n- Core decision-making about candidate merit\n- Weighing passion against credentials\n- Contextual evaluation of excellence relative to circumstance\n- The human interpretation of life circumstances and potential\n\nImplicit in his comments: AI should support, verify, and enhance human judgment\u2014not replace it.\n\n---\n\n## 6. Suggestions & Ideas\n\n**Primary Recommendation: Redefine \"Excellence\" More Flexibly**\n\n> \"The definition of what they want to fund can be articulated in a manner in which I don't necessarily have explicit boxes.\"\n\n- Move away from rigid, explicitly boxed categories\n- Provide broader definitional framework allowing discipline-specific and context-specific interpretation\n- Allow adjudicators to apply professional judgment within a clear but flexible framework\n- Should define *what* the foundation is trying to fund rather than *how* to identify it\n\n**Specific Improvements:**\n\n1. **More Sensitive Definition of Excellence**\n   - Accommodate circumstances like tragedy, tenacity, and unconventional paths\n   - Less explicitly defined categories, particularly around academic performance\n   - Account for excellence that emerges from disadvantage or non-traditional backgrounds\n   - Recognize that life paths \"different from my own\" can lead to \"true excellence\"\n\n2. **Discipline-Specific and Level-Specific Rubrics**\n   - Rubric categories could \"change based off of discipline or category\"\n   - Adjust expectations according to field-specific norms\n   - Different emphasis for Master's, PhD, and postdoctoral applications\n\n3. **Address Equity Issues in Existing Metrics**\n   - Review metrics like Golden Key Society that require financial resources to access\n   - Ensure excellence indicators aren't biased toward privileged backgrounds\n   - Acknowledge that excellent candidates may not have access to certain traditional markers\n\n4. **Improved Scoring Granularity**\n   - Consider finer-grained scoring system (mentioned that 4-point scale with 25% increments is insufficient)\n   - Allow for better discrimination between candidates at similar levels\n\n5. **Institutional Benchmarking Guidance**\n   - Provide framework for understanding grade variations across institutions\n   - Help adjudicators contextualize grades from different universities\n   - Guidance on how to interpret whether grades reflect institutional standards or individual performance\n\n6. **Formalize Contextual Commentary**\n   - Explicitly encourage and value the contextual comments adjudicators provide\n   - Make clearer that these comments influence final funding decisions\n   - Provide templates or frameworks for important contextual factors (circumstance, tenacity, etc.)\n\n**Features Desired in New System:**\n\n1. AI-assisted subject matter expertise lookup for unfamiliar fields\n2. Logic validation tools to test reasoning and articulation\n3. Presentation/stakeholder analysis features (how will decision-makers perceive this evaluation?)\n4. Budget feasibility checking tools\n5. Source verification capabilities\n6. Data scraping and research assistance for niche topics\n\n**Priorities for Change (implicit):**\n\n1. **Highest Priority:** Flexibility in excellence definition to prevent exclusion of deserving candidates\n2. **High Priority:** AI tools for subject matter expertise gaps\n3. **Medium Priority:** Improved scoring granularity\n4. **Medium Priority:** Institutional benchmarking guidance\n5. **Ongoing:** Training and consistency in adjudicator approaches\n\n---\n\n## 7. Key Direct Quotes\n\n### Quote 1: On Adjudication Approach and Fresh Perspective\n**\"Every application you read, you need to read with a fresh mind. You need to find a way to value what is being proposed but also value the person who is proposing that.\"**\n- *Context: 00:03:16*\n- *Significance: Captures the holistic, person-centered approach Martin takes and the importance of cognitive freshness in fair evaluation*\n\n### Quote 2: On Valuing Passion Over Credentials\n**\"I always value passion higher than I value cleverness. You can always teach passion but you can't build passion always.\"**\n- *Context: 00:08:11*\n- *Significance: Reveals core values in assessment; prioritizes intrinsic motivation and drive over academic credentials, which can be acquired*\n\n### Quote 3: On the Golden Key Problem\u2014Equity in Metrics\n**\"One metric for excellence of a candidate is being part of the golden key society...you have to pay to be part of a golden key society. So I do my best to see applicants who argue, well, I've been invited, but I can't afford to pay as evidence of excellence.\"**\n- *Context: 00:04:25*\n- *Significance: Illustrates how explicit rubric items can inadvertently disadvantage excellent candidates from lower socioeconomic backgrounds; shows adjudicator attempts to work around this limitation*\n\n### Quote 4: On the Fundamental Challenge of Weighing Variables\n**\"Adjudication involves balancing a set of variables that are not always weighed equally, which can sometimes constrain how well I can adjudicate an application that is not clearly excellent across all metrics.\"**\n- *Context: 00:10:33*\n- *Significance: Articulates the core tension in the current system where excellence isn't one-dimensional but the rubric forces linear scoring*\n\n### Quote 5: On Excellence in Context\u2014Disadvantage and Local Impact\n**\"I might review an application of someone from a disadvantaged background and that disadvantaged background would articulate how they would want to make a change in their community...but the grading metrics asks me to evaluate if the project could have national or global significance.\"**\n- *Context: 00:10:33-00:11:57*\n- *Significance: Reveals a fundamental misalignment between funding criteria (global significance) and valuable excellence indicators (local impact from disadvantaged backgrounds); highlights edge cases the system doesn't accommodate*\n\n### Quote 6: On Life Experience and Multiple Pathways to Excellence\n**\"Excellence can be measured whether it is from a lay person's perspective or from an absolute a disciplined leader...there are backgrounds and life paths that are different than my own and it does not mean that a different life path or a life path that appears to be less developmental or less of a straight line towards excellence should be precluded from excellence.\"**\n- *Context: 00:18:45*\n- *Significance: Challenges implicit assumptions that linear career paths are markers of excellence; advocates for recognizing diverse routes to achievement*\n\n### Quote 7: On the Bluntness of Scoring\n**\"Five points isn't always enough to discriminate one against the other.\"**\n- *Context: 00:11:57*\n- *Significance: Directly critiques the scoring system's granularity; suggests current scale cannot capture important distinctions between candidates*\n\n### Quote 8: On How AI Could Help with Subject Gaps\n**\"When I adjudicate I have an earth science BSc which means I should be able to consider topics on a broad range of things. But I can tell you that with a first year, second-year biology I might not appreciate diseases that are specific to a caterpillar species...I could use an AI to better rate what would that mean within a certain degree of significance.\"**\n- *Context: 00:31:08-00:32:33*\n- *Significance: Identifies specific, practical AI use case\u2014providing CliffsNotes-style summaries to enable fair evaluation of unfamiliar topics without requiring deep expertise*\n\n### Quote 9: On Desired Change\u2014Flexibility Over Boxes\n**\"I think that the definition of what they want to fund can be articulated in a manner in which I don't necessarily have",
    "tokens_in": 9095,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Maureen De Jager",
    "date": "2026_01_22 13_25 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Maureen De Jager) \u2013 2026_01_22 13_25 SAST \u2013 Notes by Gemini.docx",
    "text_length": 7004,
    "analysis": "# OMT Discovery Interview Analysis: Maureen De Jager\n\n## 1. Interviewee Role & Background\n\n**Role in OMT Review Process:**\n- Assessor for creative arts submissions, primarily fine art\n- Reviews applications for postgraduate scholarships (Master's and PhD level)\n- Makes recommendations on funding decisions\n\n**Duration & Experience:**\n- Approximately 3 years assessing OMT applications\n- Also sits on a national DHET (Department of Higher Education and Training) sub-panel for review of creative outputs\n- Personal experience as OMT-funded PhD candidate (Kingston University, 2015-2019)\n\n**Discipline/Expertise:**\n- Fine art practitioner and educator at Rhodes University (since 2002)\n- Has also reviewed performance-based and graphic design submissions\n- Specializes in practice-based creative research degrees\n- Recently served as Deputy Dean in Humanities (involved in recruitment and selection processes)\n\n---\n\n## 2. Current Review Process Description\n\n**Overall Approach: Comparative & Iterative**\n\nMaureen uses a distinctive **comparative assessment method** rather than linear evaluation:\n\n1. **Initial Overview Phase:**\n   - Skims through all applications to get \"an overall lay of the land\" before detailed assessment\n   - Reviews portfolios and materials to form first impressions\n   - Establishes comparative context across the cohort\n\n2. **Detailed Evaluation Phase:**\n   - Applies rubric-based scoring (1-4 scale) with specific criteria\n   - Answers rubric questions systematically\n   - Tests impressions against explicit criteria\n\n3. **Retrofit/Adjustment Phase:**\n   - Frequently returns to adjust final scores if calculated rating doesn't match strength of submission\n   - Uses initial impression (often reinforced through detailed review) to override mechanical scoring\n   - Described as \"retrofit\" - tweaking scores to reflect actual quality assessment\n\n**Tools & Systems Used:**\n- Rubric with 1-4 scoring scale\n- Written application materials (motivation letters, CVs, academic transcripts)\n- Portfolio documents (images/work samples)\n- Reference letters\n- Institutional knowledge/external databases for verifying program quality\n\n**Time Investment:**\n- Not explicitly quantified, but indicated as significant\n- Plans to spend 3 days with DHET sub-panel to review creative outputs in detail\n- Concerned about avoiding \"sitting for like 4 hours on each application\"\n\n---\n\n## 3. Pain Points & Challenges\n\n**Key Frustrations:**\n\n1. **Interpreting Academic Achievement:**\n   - Difficulty when candidates' final grades don't directly relate to proposed study\n   - Must sift through partial results or in-progress studies\n   - Relies on tacit knowledge of institutional rigor: *\"I rely on my knowledge of this sector... a 75% from a particular institution is probably worth more than a 75% from another institution\"*\n   - Labor-intensive to verify actual program quality\n\n2. **Portfolio Contextualization Gap:**\n   - Portfolios often lack annotation or explanation\n   - *\"If it's just a bunch of images and there's very little contextualization of what was informing the making of the work... it's also a little bit difficult to kind of get a handle on it\"*\n   - Critical information about artistic practice, methodology, and development is missing\n\n3. **Problematic Rubric Questions:**\n\n   **a) \"Significance to South African Society\" Question:**\n   - Applicants struggle with this; answers often \"unpersuasive\"\n   - Fundamental mismatch: creative practice is exploratory and emergent, not predetermined\n   - *\"Practice in its nature is exploratory... it's very difficult to know before you've even set out on this journey where you're going to end up... in a lot of cases it's a bit of a thumb suck\"*\n   - Generates generic, \"floaty\" responses disconnected from actual practice\n\n   **b) \"Personal Motivation\" Question:**\n   - Misalignment between question framing and assessment criteria\n   - Question asks for background/interests (narrative), but rubric seeks evidence of \"values and priorities\"\n   - *\"You're looking for evidence of values and priorities... sometimes the personal motivation is framed more as just a narrative... you're being asked to look for something that isn't necessarily in evidence\"*\n   - Space constraints mean candidates often only cover background, not deeper values\n\n   **c) \"Plan to Achieve Vision\" Question:**\n   - Described as \"an odd question\"\n   - Confusing for both applicants and reviewers\n   - Difficult to map when degree programs have specific benchmarks and timelines\n   - Conflates generic life planning with specific academic planning\n\n4. **Overseas Study Assessment:**\n   - Tricky to verify whether comparable programs truly exist locally\n   - *\"As a reviewer, one doesn't necessarily know whether there are comparable degree programs... you're relying very much on the applicant saying there is nothing like it rather than your own kind of knowledge\"*\n   - Limited evidence base for claims of program uniqueness\n\n5. **AI-Generated Content Deception:**\n   - Increasing difficulty distinguishing authentic applicant voice from AI-generated text\n   - *\"With AI you know it's very easy to write a convincing proposal for a research project... and you're sort of wondering how much of that is really this the student being able to articulate\"*\n   - Creates misalignment between articulation and actual ability\n   - Can be difficult to detect even with writing samples, as institutions may miss AI generation\n\n6. **Reference Letter Reliability:**\n   - References often unhelpful or biased (e.g., from intimate partners)\n   - Only useful when reviewer personally knows referees as strong/credible\n   - Generic or unsubstantiated praise common\n\n7. **Inconsistency Detection:**\n   - Multiple submissions make it difficult to spot contradictions\n   - *\"Sometimes you know you don't notice the inconsistencies particularly if you're dealing with a lot of submissions\"* (mentioned by other reviewers)\n\n---\n\n## 4. What They Value in Applications\n\n**Critical Success Factors:**\n\n1. **Portfolio & Practical Excellence (Highest Priority):**\n   - *\"It's actually quite critical if one's looking at a student wanting to go into a creative degree... is their portfolio and that evidence of practical excellence\"*\n   - Described as \"highly detrimental\" if absent\n   - More reliable indicator than written articulation due to AI risk\n   - Preferred with artist annotation explaining context and development\n\n2. **Coherence & Internal Consistency:**\n   - *\"There needs to be a kind of coherence where the way that the person articulates their work aligns with the work aligns with how they see it developing\"*\n   - If coherent, automatically persuasive; if not, unconvincing regardless of cohort standing\n   - Alignment between:\n     - How they describe their work\n     - Actual work quality shown in portfolio\n     - How they envision development\n\n3. **Academic Transcript (Contextual Assessment):**\n   - Looked at with nuance, not mechanically\n   - Maureen considers:\n     - Performance in relevant subjects (e.g., fine art practice grades)\n     - Institutional context and rigor\n     - Progress trajectory\n\n4. **Strong Motivation Letter:**\n   - Demonstrates understanding of specific program\n   - Shows grounded reasoning (not generic)\n   - Reveals ability to articulate thinking\n\n5. **Evidence of Realistic Planning:**\n   - For overseas study: genuine evidence program is unavailable locally\n   - Clear understanding of what they'll build on or develop\n   - Specific rather than aspirational\n\n**Red Flags:**\n\n1. **Academic-Portfolio Mismatch:**\n   - *\"Very clear misalignment between the academic record and the portfolio\"*\n   - Strong grades but weak portfolio signals doubt about technical/conceptual sophistication\n   - May indicate previous support or AI use in academic work\n\n2. **Generic, Ungrounded Responses:**\n   - Floating statements about societal benefit (\"we all need nice things to look at\")\n   - Lack of specific engagement with personal practice\n   - Absence of disciplinary grounding\n\n3. **Missing Information:**\n   - No portfolio (absolutely critical)\n   - Incomplete academic records\n   - Vague or missing motivation\n\n4. **Weak References:**\n   - References from non-credible sources\n   - Lack of professional supervisor perspective\n   - Generic praise without specifics\n\n**Weighting & Decision Factors:**\n\nMaureen applies **contextual strictness** based on:\n- **Level of study:** More forgiving with Master's applicants than PhD candidates\n- **Funding type:** Stricter with expensive overseas study due to cost\n- **Application stage:** For year 2+ funding, requires evidence of year 1 progress\n- **Candidate pool:** Bench marks against internal standard of what constitutes a \"persuasive, coherent application\"\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Stance: Cautious Skepticism with Limited Specific Use Cases**\n\n**General Distrust:**\n- *\"I'm also like probably of the generation that is like a slightly distrustful of AI\"*\n- Direct negative experience: *\"Deputy dean in humanities sat on lot of recruitment and selection processes which relied... AI was used as a kind of screening mechanism and then there were these AI generated summaries of the applicants which kind of didn't help\"*\n\n**Core Concern - Interpretative Nature:**\n- *\"I think it would be immensely helpful, but I don't know how it's possible because so much of this is interpretative\"*\n- Fundamental problem: creative assessment requires human judgment that can't be systematized\n- *\"Creative practice is such a tricky thing... there's not a golden standard of how to look at it\"*\n\n**Specific Concerns:**\n\n1. **Loss of Professional Judgment:**\n   - Risk of over-reliance on automated assessment for inherently interpretative decisions\n   - Potential to short-circuit necessary human engagement\n\n2. **Applicant Gaming:**\n   - *\"One doesn't want a situation where applicants start being quite instrumentalist in how they're answering things in anticipation of an AI process... they start saying things about their values so that the values are there\"*\n   - Fear that knowing about AI screening will cause inauthentic responses\n\n3. **Hidden AI in Submissions:**\n   - Difficulty detecting AI-generated content\n   - *\"At their institution it wasn't picked up that this is AI generated\"*\n\n4. **Poor Experience with AI Summaries:**\n   - AI-generated summaries of applicants in recruitment didn't provide useful information\n\n**What She Would Accept - Limited Administrative Tasks:**\n\n1. **Initial Filtering/Screening (SUPPORTED):**\n   - *\"As an initial filtering or screening, yes, absolutely\"*\n   - Checking for missing documents\n   - Verifying required materials submitted\n   - Checking basic compliance with criteria\n   - *\"If somebody's documents aren't all there... if they haven't answered all the questions, if they don't meet criteria\"*\n\n2. **AI-Generated Summaries (CONDITIONAL):**\n   - *\"AI generated summaries of particular things which would make it kind of like so there's a little summary and then you can read the lengthier narrative\"*\n   - For lengthy motivation narratives (summary + full text)\n   - For reference letter summaries to extract key points\n\n3. **Program Information Research (HELPFUL):**\n   - *\"Things around students' motivations to study internationally... is there information about the program that could be... information can be made available that would help one to assess the merits of that program\"*\n   - Compiling information about international programs\n   - Helping verify whether comparable local options exist\n   - Flagging whether claimed uniqueness is accurate\n\n4. **Inconsistency Flagging (POTENTIALLY USEFUL):**\n   - AI identifying contradictions between different application sections\n   - Spotting discrepancies reviewers might miss with large volumes\n\n**What She Would NOT Accept:**\n\n- Core assessment/scoring by AI\n- Filtering based on subjective quality judgment\n- Replacing human evaluation of portfolio or conceptual coherence\n- Any system that reduces time spent on serious engagement with applications\n\n**Requirements for Trust:**\n- Process must maintain transparency about what AI is doing\n- Clear distinction between administrative vs. interpretative tasks\n- Demonstration that tool adds value without compromising judgment\n- Safeguards against applicant response distortion\n\n---\n\n## 6. Suggestions & Ideas\n\n**Immediate Improvements (Non-AI):**\n\n1. **Redesign Problematic Rubric Questions:**\n   - Reframe \"Personal Motivation\" question to explicitly ask for evidence of values/priorities rather than narrative background\n   - Better align question wording with assessment criteria\n   - Clarify distinction between generic life planning and specific academic planning\n\n2. **Portfolio Requirements Enhancement:**\n   - Make portfolio annotation/contextualization mandatory\n   - Request artist statement explaining:\n     - Disciplinary focus (sculpture, print-making, etc.)\n     - How practice is developing\n     - Alignment with interests/concerns to explore\n   - *\"A little annotation or something where the student speaks to what's in the portfolio\"*\n\n3. **Reference Letter Requirements:**\n   - Require at least one reference from current or prospective research supervisor\n   - Ensures professional perspective rather than personal bias\n   - *\"At least one of the reference letters should be by a current or prospective supervisor, research supervisor\"*\n\n4. **Significance Question Reframing:**\n   - Acknowledge exploratory nature of creative practice\n   - Ask for grounded vision rather than predetermined societal contribution\n   - Make question applicable to Master's as well as PhD candidates\n\n**Technology-Enabled Improvements:**\n\n1. **Leverage AI for Administrative Screening:**\n   - Pre-screening for completeness\n   - Flagging missing documents\n   - Basic compliance checking\n   - This would \"save time\" on routine checks\n\n2. **Summaries of Key Documents:**\n   - Generate summaries of lengthy motivation narratives (with full text available)\n   - Summarize reference letters to pull key themes\n   - Makes information scanning faster without replacing full reading\n\n3. **Program Verification Database:**\n   - Compile information about international study programs\n   - Help assess claims of program uniqueness/unavailability locally\n   - *\"Information can be made available that would help one to assess the merits of that program\"*\n   - Research whether comparable options truly exist\n\n4. **Inconsistency Detection:**\n   - Flag contradictions between application sections\n   - Highlight when portfolio doesn't align with stated goals\n   - Catch typos, date errors, credential mismatches\n\n**Broader Process Suggestions:**\n\n1. **Benchmarking Tool (Internal):**\n   - System to help reviewers remember what constitutes a \"persuasive, coherent application\"\n   - Consistency framework without constraining judgment\n   - Reference to previous year's benchmark standards\n\n2. **Consider Interim Review Layer:**\n   - Inspired by DHET model with peer reviewers + sub-panel review\n   - Could reduce individual reviewer bias\n   - Maureen noted this is helpful when seeing 2-3 independent assessments before final decision\n\n**Process Changes:**\n\n- Invest time in training reviewers on:\n  - How to interpret academic achievements across institutions\n  - What to look for in portfolios\n  - Red flags in creative work\n\n---\n\n## 7. Key Direct Quotes\n\n### On Portfolio Criticality\n> \"It's actually quite critical if one's looking at a student wanting to go into a creative degree a practice-based masters or PhD is their portfolio and that evidence of practical excellence... because you can talk things up and especially with AI you know it's very easy to write a convincing proposal for a research project um and then you're sort of wondering how much of that is really this the student being able to articulate and does it match their actual ability, their practical ability and the strength of a portfolio.\"\n\n**Context:** Explaining why portfolio is the most reliable assessment tool given the rise of AI-generated text\n\n---\n\n### On Creative Practice's Exploratory Nature\n> \"Practice in its nature is exploratory you know it kind of like opens up towards wards um realizations and and understandings and um you know it's very difficult to know before you've even set out on this journey where you're going to end up. Um and so I think in a lot of cases it's a bit of a thumb suck.\"\n\n**Context:** Explaining why questions about significance to South African society are problematic\u2014they ask for predetermined outcomes in a fundamentally open-ended discipline\n\n---\n\n### On Rubric-Question Misalignment\n> \"There's a slight misalignment in that you ask then as the reviewer um to look for evidence of values and priorities and sometimes the personal motivation is framed more as just a narrative... you've only got like a short space of time. And so a lot of candidates don't get much further than their background, schooling, academic history, and general interests, you know, but then you're being asked to look for evidence of their personal values and their priorities... we're looking for something that isn't necessarily in evidence.\"\n\n**Context:** Identifying how question design creates assessment problems\u2014the rubric asks reviewers to look for something the question structure doesn't elicit from applicants\n\n---\n\n### On Academic-Portfolio Mismatch\n> \"Somebody can articulate an idea for where they see their research going and it sounds great. It sounds very compelling, you know. Um, and then and then you look at at the caliber of the work that's being produced, you know, and this is also where sometimes there's a very clear misalignment for me between um the academic record and the portfolio... they've articulated um a reasonably strong kind of statement in regards to the significance of what they're intending to do. Um but then when you look at the at the portfolio, doubts creep in as to whether the person has the technical and conceptual sophistication to be able to pull off what they've articulated.\"\n\n**Context:** Describing what causes mind-changes during evaluation\u2014",
    "tokens_in": 10089,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Mohamed Cassim",
    "date": "2026_01_20 09_28 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Mohamed Cassim) \u2013 2026_01_20 09_28 SAST \u2013 Notes by Gemini.docx",
    "text_length": 8516,
    "analysis": "# OMT Discovery Interview Analysis: Mohamed Cassim\n\n## 1. Interviewee Role & Background\n\n**Role in OMT Review Process:**\nMohamed serves as an adjudicator for OMT, specifically reviewing applications in the \"financial applications\" category, though he notes he feels somewhat \"pigeonholed\" into this area despite his broader expertise.\n\n**Duration of Involvement:**\nThe transcript does not specify how long Mohamed has been adjudicating for OMT, though he mentions a relationship with Tracy (OMT leadership) through a fellows network.\n\n**Discipline/Expertise Area:**\nMohamed has exceptionally diverse experience across multiple sectors:\n- **Public Sector**: Built financial data warehouse for post-1994 South Africa; architected jobs fund for national treasury; ran NSF funding programs\n- **Private Sector Development**: Co-founded mergers & acquisition shop; invested in mobile wallet company (now operates across 43 countries, serves millions)\n- **Social Enterprise**: Ran KFC franchise operations in Eastern Cape creating ~50 stores and substantial employment\n- **Current Role**: Co-founder of Africa Climate Ventures (climate-tech investment fund, $10M+ raised)\n- **Academic Background**: Pharmacy undergraduate, MBA, studied strategy and innovation at Oxford (2013-14)\n- **Personal Orientation**: Strategy, innovation, development economics, job creation across African contexts\n\n---\n\n## 2. Current Review Process Description\n\n**Process Overview:**\nMohamed describes a thorough, methodical approach that is \"longish\" and \"old school.\"\n\n**Specific Steps:**\n\n1. **Complete Document Review**: Reads all submission documents in full, typically over several nights while family sleeps (Mohamed identifies as a night owl)\n\n2. **Two-Stage Assessment Framework**:\n   - **Stage 1 - Merit Assessment**: Evaluates submission \"on its own merit\" as a standalone case\n   - **Stage 2 - Calibration via Relativity**: When similar applications exist, uses comparative analysis to differentiate between candidates and provide nuanced perspective\n\n3. **Initial Depth Assessment**: Uses extensive sector experience to quickly determine submission depth\u2014can \"fairly quickly\" distinguish between \"blustering\" and actual achievement\n\n4. **Institutional Performance Review**: For candidates with substantial experience (postgrad + 10+ years), examines performance of institutions where they worked, particularly in relation to their stated functions\n\n5. **Logic and Intention Analysis**: Examines how candidates think and construct arguments, particularly distinguishing between genuine thesis focus versus funding-seeking as a bridge to other goals\n\n6. **Reference Letter Evaluation**: Uses reference letters as indicators of authenticity and motivation, valuing \"heartfelt proper letters from mentors or professors\" and detecting AI-generated or candidate-drafted letters\n\n7. **Rubric Application with Subjective Commentary**: Adheres to rubric while adding subjective judgment, particularly in areas like budget assessment and \"skin in the game\" evaluation\n\n8. **Relative Differentiation**: When scoring similar candidates, provides explicit comparative notes explaining differences in sector exposure, potential impact, or career intentions rather than simply scoring one higher than another\n\n**Tools/Systems Currently Used:**\n- OMT's adjudication guide/rubric\n- Reference letters and application documents\n- Professional judgment drawing on sector knowledge\n- Recently purchased AI tool for exploratory use (appears to be Claude or similar)\n\n**Time Spent:**\n- Several nights of full document reading per adjudication cycle\n- Unclear per-application time, but clearly substantial given depth of analysis described\n\n---\n\n## 3. Pain Points & Challenges\n\n**Frustrations with Current System:**\n\n1. **Pigeonholing into Financial Applications**: \n   - Mohamed feels confined to \"financial applications\" despite broader expertise across public sector, development, climate ventures\n   - Notes this \"left a little bit of value on the table\"\n   - Has never raised this concern vocally, indicating potential organizational communication barriers\n\n2. **Rubric Limitations**:\n   - **\"Extraordinary Talent\" Category Too Blunt**: \"I feel leaves a little bit too much room for adjudicators...I don't even know how you decide what talent is...I think it's a little bit blunt\"\n   - Lacks scientific definition, leading to inconsistency\n   - Conflates different types of talent (probability of amazing output vs. capability to deliver on high-value topics)\n\n3. **Bias Toward Non-SA Specialization**:\n   - Rubric implicitly devalues specializations \"available in SA\"\n   - Misses areas where South Africa excels (e.g., mining, extractive industries)\n   - Creates bias against candidates repatriating intellectual property to SA\n\n4. **\"Intended Study\" Category Subjectivity**:\n   - Heavily relies on adjudicator's broad experience\n   - Difficult to calibrate across different reviewers with different expertise\n\n5. **Personal Motivation Assessment Challenges**:\n   - Current focus on motivation letters is flawed\u2014\"a letter you can get a good friend to write...you can get AI to write it for you\"\n   - Requires deep reading of full application history, creating cognitive load\n\n6. **Reference Letter Verification**:\n   - Must distinguish between genuine letters and candidate-drafted letters signed by busy referees\n   - Time-consuming manual verification of authenticity\n\n7. **Institutional Performance Research**:\n   - Requires manual checking of institutional performance against candidates' claimed experience\n   - Must verify financial statements and sector performance to validate CV claims\n   - \"It's a pile of work. It's a pile of work.\"\n\n8. **Probability vs. Intent Confusion**:\n   - Difficulty assessing probability of delivery vs. merit of intent\n   - Must consider candidate's life circumstances (dependents, financial stress) that affect delivery capability\n   - No systematic framework for holistic capability assessment\n\n9. **Unclear Value Contribution**:\n   - Uncertain whether detailed comparative notes are actually useful to OMT decision-makers\n   - Describes providing \"explicit thought to whoever the recipient is\" but doesn't know if recipients value this depth\n   - Questions if the effort is \"worth their while\"\n\n10. **National Education System Context**:\n    - Must factor in South Africa's low NSF throughput (1 degree for every 5 students funded)\n    - Creates systemic barriers to probability of delivery that are beyond individual adjudicator control\n\n**Specific Bottlenecks:**\n- Manual verification of institutional performance and financial statements\n- Distinguishing genuine reference letters from AI-generated or candidate-drafted letters\n- Assessing psychological fit (psychographic analysis) between candidate and topic requirements\n- Cross-checking CV claims against institutional performance data\n- Calibrating subjective categories across multiple adjudicators with different expertise\n\n---\n\n## 4. What They Value in Applications\n\n**Key Criteria Mohamed Prioritizes:**\n\n1. **Probability of Delivery Over Intent**:\n   - \"It's not quite simply about the merit of the intent...everybody can write a brilliant piece of intent. It's also about the probability of that being delivered\"\n   - Draws from investment world perspective: \"either it is probable or it's not\"\n   - Looks for realistic assessment of candidate's capacity to complete work\n\n2. **\"Skin in the Game\"**:\n   - Strong believer that financial investment by candidate signals commitment\n   - Will comment if candidate hasn't invested own cash (when they have capacity to do so)\n   - Recognizes constraints: won't penalize those supporting family members\n\n3. **Authentic Achievement Evidence**:\n   - Can distinguish between \"blustering\" and actual accomplishment\n   - Values demonstrated impact through institutional context (e.g., did the organization perform well during candidate's tenure?)\n   - Looks at functions held: \"if they're in some kind of position of leadership,\" checks if that area performed well\n\n4. **Alignment Between Candidate Profile and Topic Requirements**:\n   - Assesses whether candidate's temperament/experience matches project needs\n   - Example: topic might require \"an extrovert of this kind with that type of experience\"\n   - Uses psychographic analysis to match person to project\n\n5. **Genuine, Heartfelt Reference Letters**:\n   - Values letters that feel authentic and come from different angles\n   - Looks for letters from \"mentors or professors\" that demonstrate real knowledge of candidate\n   - Uses reference letters to validate motivation beyond written statements\n   - Detects candidate-drafted letters (can tell by authenticity markers)\n\n6. **Depth of Intended Study**:\n   - Values candidates with genuine intellectual curiosity\n   - Looks for those pursuing study for its own merit, not as funding bridge\n   - Can detect candidates \"just collecting funds\" to \"spend a couple years filling up time\"\n   - Examines how candidates construct their logic and reasoning\n\n7. **Sector Specialization with Repatriation Potential**:\n   - Values specializations that can be brought back to benefit SA\n   - Example: mining expertise can create technology, associated industries, economic development\n   - Pushes back against rubric bias toward unavailable specializations\n\n8. **Holistic Life Context**:\n   - Considers dependents, family obligations, financial stress\n   - Recognizes this context affects delivery probability\n   - Suggests additional funding if necessary to ensure focus on academic work\n   - Views as investment decision: \"if this particular piece of study holds that much merit then I do want to make sure that the probability of being delivered is high\"\n\n9. **Clear Logic and Reasoning**:\n   - Values how candidates present arguments and construct ideas\n   - Looks for clarity in communication and thinking\n   - Distinguishes between confused thinking and language barriers\n\n10. **Institutional Track Record**:\n    - For senior candidates, examines institution performance during their employment\n    - Validates achievements by checking if organization performed well in stated areas\n    - Won't recommend someone from organization that failed in their stated domain\n\n**Red Flags Mohamed Watches For:**\n\n1. **Lack of Financial Commitment**: Candidate hasn't invested own resources when capable\n2. **Inconsistency Between Claims and Reality**: CV claims don't align with institutional performance during candidate's tenure\n3. **Generic Reference Letters**: Letters that appear candidate-drafted or AI-generated\n4. **Funding as Bridge Strategy**: Candidate seeking funds primarily to \"fill time\" rather than genuine research\n5. **Mismatch Between Profile and Topic**: Psychographic misalignment (e.g., introverted candidate for project requiring extroversion)\n6. **Overcommitted Candidates**: Those with excessive dependents/obligations that will interfere with delivery\n7. **Unclear Logic**: Poorly constructed arguments suggesting unclear thinking\n\n**What Makes Applications Stand Out:**\n\n1. **Authentic Passion with Realistic Planning**: Genuine motivation combined with practical assessment of delivery capability\n2. **Demonstrated Impact**: Track record showing not just credentials but actual achievement in relevant areas\n3. **Diversity in Perspective**: Different sector exposure, international experience, or unique angle on problem\n4. **Investment of Self**: Financial, time, or emotional investment showing genuine commitment\n5. **Clear Future Vision**: Specific, thoughtful articulation of how study will enable impact\n6. **Institutional Validation**: Strong reference letters that provide independent verification of capability and motivation\n\n**How Mohamed Weighs Different Factors:**\n\nUsing investment portfolio logic\u2014multifaceted considerations:\n- Probability of delivery weighted heavily (not just merit of intent)\n- Life circumstances factored into funding recommendations\n- Sector impact and repatriation potential considered\n- Institutional validation weighted significantly\n- Comparative advantage over similar candidates highlighted\n- Holistic assessment rather than isolated rubric scores\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Attitude Toward AI in Review Process:**\n\nMohamed is cautiously optimistic but deeply cautious about limitations. He sees AI as potentially valuable for specific tasks but fundamentally limited for assessment of future capability.\n\n**Specific Hopes for AI:**\n\n1. **Efficiency Gains on Binary Tasks**: \n   - \"Could be helpful if the binary stuff...could answer that in about 30 seconds and the adjudicator can decide\"\n   - Example: Budget questions are \"straightforward and simple\" and could be automated\n\n2. **Topic Commonality Assessment**:\n   - \"AI could help determine if a topic is uncommon amongst academic institutions by trolling databases\"\n   - Could scrape academic databases to position candidate's research in landscape\n\n3. **Psychographic Matching**:\n   - \"From the cognitive perspective, it would be fantastic if it said the topic requires an extrovert of this kind with that type of experience and this guy doesn't have it\"\n   - Could conduct profile analysis against topic requirements\n\n4. **Institutional Performance Verification**:\n   - \"I would love for it to tell me that from this guy's CV I checked on the institutions...I brought back the following financial statements\"\n   - Could verify CV claims against institutional records\n   - Could check company/organization performance during candidate's tenure\n\n5. **Reference Letter Authentication**:\n   - \"I'd like you to tell me that AI wrote the reference letters up front, right? So that I didn't have to think about that\"\n   - Could detect AI-generated or candidate-drafted letters\n\n6. **Reducing Superfluous Cognitive Load**:\n   - AI as \"cognitive assistant to handle binary tasks and reduce superfluous cognitive load\"\n   - Would free adjudicators to focus on \"deep dive reasoning on specific issues rather than initial fact-finding\"\n\n**Fundamental Concerns About AI:**\n\n1. **Cannot Assess Future Probability of Delivery**:\n   - **Central Concern**: \"I don't think what it can do is tell the probability of somebody delivering on something\"\n   - \"AI is built on data that is in the internet...by its very nature AI would go on historic trends when all of us sitting at this table know that the future is not told by history alone. It's told by capability for tomorrow\"\n   - AI \"is awfully equipped for that. It's just incapable to be frank\"\n\n2. **Bias Toward Historical Patterns**:\n   - \"AI is poorly equipped to determine the probability of somebody delivering on something\"\n   - Creates \"bias toward solving yesterday's problems\"\n   - Violates fundamental principle: \"even as adjudicators have to be careful about having a bias to solving yesterday's problems\"\n\n3. **Cannot Make Capability Judgments**:\n   - Cannot assess whether candidate has genuine capability vs. just credentials\n   - Cannot assess life circumstances' impact on delivery probability\n   - Cannot evaluate quality of thinking or authentic motivation\n\n**What Mohamed Would/Wouldn't Want Automated:**\n\n**Would Automate (Binary/Factual):**\n- Budget verification (amounts, previous funding, own investment, timeline)\n- Topic commonality research across academic institutions\n- Reference letter authenticity detection\n- Institutional performance verification\n- Basic CV validation against institutional records\n- Financial statement analysis for referenced organizations\n\n**Would NOT Automate (Judgment/Future-Oriented):**\n- Probability of delivery assessment\n- Capability evaluation\n- Authentic motivation assessment\n- Psychographic matching (though AI could assist by flagging potential mismatches)\n- Relative merit between similar candidates\n- Future impact potential\n- Assessment of thinking quality or logic\n\n**Trust and Transparency Requirements:**\n\n1. **Transparency About AI Limitations**:\n   - Must be clear about what AI is and isn't doing\n   - Cannot position AI as assessing delivery probability or capability\n   - Should be explicit about historical data bias\n\n2. **Verification Requirement**:\n   - AI findings should be verifiable by human adjudicator\n   - Adjudicator must be able to check AI's institutional performance claims\n   - Reference letter detection should be explainable\n\n3. **Responsibility for Judgments**:\n   - Adjudicator remains responsible for final assessment\n   - AI assists with fact-finding, not judgment-making\n   - Preserves adjudicator's professional accountability\n\n4. **Honest Labeling**:\n   - \"I bought this tool set recently...I said to it the other day, write me a legal letter\"\n   - Appreciates AI for what it can do; wants clear boundaries on what it cannot\n\n---\n\n## 6. Suggestions & Ideas\n\n**Improvements to Rubric:**\n\n1. **Redefine \"Extraordinary Talent\" Category**:\n   - Make definition \"more scientific\"\n   - Clarify what talent means: \"Are we talking about the probability that if this guy wrote some piece over a period of time and was funded, he'd do something amazing and the world would see it? Or talking about the probability of him taking a high high value topic and delivering it to the extent that the world can build on it?\"\n   - Reduce subjectivity and improve consistency across adjudicators\n\n2. **Revise \"Specialization Unavailable in SA\" Bias**:\n   - Recognize areas where SA excels (mining, extractive industries)\n   - Value repatriation of intellectual property to SA\n   - Create technology and associated industries from SA expertise\n   - Remove implicit bias toward foreign specializations\n\n3. **Improve \"Personal Motivation\" Assessment**:\n   - De-emphasize written motivation letters (easily AI-generated)\n   - Rely more on demonstrated history and actions\n   - Use reference letters as authenticity indicators\n   - Assess motivation through pattern of choices, not assertions\n\n4. **Clarify \"Intended Study\" Category**:\n   - Acknowledge need for adjudicator's broad experience\n   - Provide examples of what constitutes significant vs. trivial study\n   - Support adjudicators in applying spectrum experience\n\n**Features/Capabilities for New System:**\n\n1. **Cognitive Load Reduction Assistant**:\n   - AI module that handles institutional performance verification\n   - Automatically retrieves financial statements for referenced organizations\n   - Flags consistency issues between CV claims and institutional reality\n   - Reports: \"this guy's not very honest about his achievements because those companies didn't do so well for the following reasons\"\n\n2. **Reference Letter Authentication Tool**:\n   - Flags AI-generated or candidate-drafted reference letters up front\n   - Allows adjudicator to focus on authentic",
    "tokens_in": 12057,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Ndumiso Luthuli",
    "date": "2026_01_23 12_55 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Ndumiso Luthuli) \u2013 2026_01_23 12_55 SAST \u2013 Notes by Gemini.docx",
    "text_length": 1119,
    "analysis": "# OMT Discovery Interview Analysis: Ndumiso Luthuli\n\n## \u26a0\ufe0f CRITICAL NOTE\nThis transcript does **not contain an actual interview with Ndumiso Luthuli** (StrideShift). The transcript only captures pre-meeting technical troubleshooting between **Justin Germishuys** and **Barbara Dale-Jones**. Ndumiso (\"Indonesia\") was expected to join at 3:00 PM but the actual interview content is not included in the provided transcript.\n\n---\n\n## 1. Interviewee Role & Background\n**Cannot be determined from this transcript.** \n- Ndumiso Luthuli was invited but did not appear in the recorded portion\n- Email: ndumiso.luthuli.oba@said.oxford.edu\n- Expected to join the call at 3:00 PM (had car trouble)\n- No information about their role, tenure, or discipline is captured\n\n---\n\n## 2. Current Review Process Description\n**No data available.** This section would have been covered in the actual interview with Ndumiso.\n\n---\n\n## 3. Pain Points & Challenges\n**No data available from the interviewee.** However, the OMT team (Justin and Barbara) expressed concerns about:\n- **Technical infrastructure challenges**: Line speed issues, connectivity problems during calls\n- **Documentation management**: Current documents are disorganized and lack context\n- **Information synthesis bottleneck**: Manual effort required to compile and understand existing interview transcripts\n\n---\n\n## 4. What They Value in Applications\n**No data available from the interviewee.**\n\n---\n\n## 5. Views on AI/Technology\n\n### Observations from OMT Staff (not the interviewee):\n\n**Positive views:**\n- Claude performed \"quite good\" work when given transcripts to analyze\n- Some AI applications are \"absolutely insanely good\"\n\n**Concerns & Skepticism:**\n- **Inconsistent performance**: AI struggles with tasks that should be simple\n- **Fabrication issues**: AI \"fabricates\" answers rather than admitting uncertainty\n- **False confidence**: AI tries to \"act like it's on top of things\" and will \"spin...in the most convincing way some answer\" that may be inaccurate\n- **Reality misalignment**: Generated answers sometimes \"didn't really happen\"\n- **Lack of self-awareness**: AI reflects back incorrect information when corrected rather than genuinely acknowledging errors\n\n### Notable quote from Barbara Dale-Jones:\n> \"It really just tries to act like it's on top of things sometimes. Um, and it just will like spin, you know, in the most um convincing way some answer and then you're like, 'No, but that didn't really happen.'\"\n\n---\n\n## 6. Suggestions & Ideas\n**From OMT Staff (not the interviewee):**\n- **Document organization**: Create a structured project containing all existing documents to establish proper context\n- **AI-assisted synthesis**: Use AI tools (like Claude) to analyze batches of transcripts, but with human verification\n- **Verification process**: All AI-generated insights should be reviewed for accuracy before use\n\n---\n\n## 7. Key Direct Quotes\n\n1. **\"And I read quite a lot of what you uncovered and it's quite good. Um but it is always I always find the kind of jagged um sort of edge or coastline of AI very interesting.\"**\n   - Context: Justin reflecting on Barbara's work with Claude; captures the fundamental unpredictability of AI\n\n2. **\"There are some things it's absolutely insanely good at and then you give it like what should be a simple task and it's a dance.\"**\n   - Context: Justin explaining inconsistent AI performance; suggests frustration with unexplainable failures\n\n3. **\"And and and just like fabricate I I see again and again just like a little bit of fabrication...it really just tries to act like it's on top of things sometimes.\"**\n   - Context: Barbara identifying the core problem with AI\u2014false confidence and confabulation\n\n4. **\"It just will like spin, you know, in the most um convincing way some answer and then you're like, 'No, but that didn't really happen.'\"**\n   - Context: Specific concern about AI's persuasive tone masking inaccuracy\n\n5. **\"I've started using more create colorful expletives. Um, and then it actually reflects back the expletives and says, 'Yes, I did bleep.'\"**\n   - Context: Justin's humorous anecdote illustrating AI's tendency to echo user input rather than genuinely process criticism\n\n6. **\"I would actually like to take some time out to just take all the documents we have and just putting them in a project. I think once that's going um we'll have all the context set up.\"**\n   - Context: Justin identifying the foundational problem: lack of organized context\n\n---\n\n## 8. Unique Insights\n\n### Key Meta-Finding:\nThis transcript reveals a **critical issue with the research methodology itself**: The interview with the primary subject (Ndumiso/StrideShift) never actually occurred in the recorded portion. This is significant because:\n\n1. **No direct data on application review processes** - Only pre-meeting technical discussion captured\n2. **Bias toward team perspectives** - Only OMT staff observations about AI are recorded, not actual reviewer insights\n3. **Incomplete research** - The stated purpose (improving OMT's application review process with AI assistance) has not yet been addressed\n\n### Distinctive Concerns About AI in This Context:\n\nThe team's observed problem with **AI fabrication** is particularly concerning for an **application review system** where:\n- Accuracy about candidates is critical (false positive/negative assessments could harm applicants)\n- Confidence levels must be transparent (applicants deserve to know if decisions are based on verified or speculative information)\n- Explainability is essential (scholarship decisions require justification)\n\n**Most Critical Quote for AI Implementation:**\n> \"It just will like spin, you know, in the most convincing way some answer and then you're like, 'No, but that didn't really happen.'\"\n\nThis suggests that if AI is implemented in application review, there must be:\n- Explicit human verification of all AI-generated assessments\n- Clear separation between AI summaries and human judgment\n- Audit trails showing which claims are fact-checked vs. speculative\n\n---\n\n## Recommended Follow-Up Actions\n\n1. **Conduct the actual interview with Ndumiso Luthuli** to gather primary data on reviewer experience\n2. **Establish ground rules** for AI use in application assessment that address fabrication concerns\n3. **Design verification protocols** that don't assume AI analysis is accurate without human review\n4. **Document the baseline** review process before piloting any AI assistance",
    "tokens_in": 2353,
    "tokens_out": 1532,
    "skipped": false
  },
  {
    "name": "Philippe Burger",
    "date": "2026_01_13 13_58 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Philippe Burger) \u2013 2026_01_13 13_58 SAST \u2013 Notes by Gemini.docx",
    "text_length": 5639,
    "analysis": "# OMT Discovery Interview Analysis: Philippe Burger\n\n## 1. Interviewee Role & Background\n\n**Role:** Adjudicator for the Oppenheimer Memorial Trust scholarship review process\n\n**Duration of Involvement:** 2-3 years\n\n**Discipline/Expertise:** Economist and macroeconomist; reviews applications in the broader business management and economics field\n\n**Application Levels Reviewed:** Masters, PhD, and Scholar (postgraduate) applicants\n\n---\n\n## 2. Current Review Process Description\n\n### Process Flow:\n1. **Initial Classification:** Applications organized by degree level (Masters, PhD, Scholar) and category\n2. **Academic Record Review:** First filter examines applicants' academic performance\n3. **Motivation Essay Assessment:** Reviews written motivation/motivation essay for clarity, focus, realism, and ambition\n4. **Field Alignment Check:** Verifies intended field of study aligns with existing qualifications and disciplinary foundation\n5. **Scoring:** Assigns numerical scores on a scale of 5, using 3 as average baseline\n6. **Iterative Ranking:** Creates provisional ranking that is adjusted as more candidates are reviewed; revisits rankings after reviewing 5-6 candidates to establish relative positions\n7. **Benchmarking:** Reads through 10-12 applications before settling on final scores to establish relative benchmarking context\n\n### Tools/Systems:\n- Written motivation essays as primary assessment documents\n- Numerical scoring system (scale of 5, with 3 as average)\n- No mention of formal scoring rubric or structured evaluation framework\n- No formal bias mitigation tools currently in place\n- Use of Turnitin's AI detector mentioned for parallel university process (not currently used for OMT)\n\n### Time Considerations:\n- Annual process conducted \"from scratch\" each year due to 12-month gap between cycles\n- Estimates that stricter pre-screening could save time by eliminating clearly unqualified applicants earlier\n- No specific time estimates provided for individual application reviews\n\n---\n\n## 3. Pain Points & Challenges\n\n### Primary Frustrations:\n\n**AI-Generated Content Detection:**\n- Strong suspicion that some motivation essays are written by AI, evidenced by sudden improvements in English quality\n- Difficulty distinguishing genuine applicant voice from AI-generated content as models become more sophisticated\n- Increasing problem anticipated as text generation technology advances\n\n**Subjectivity in Scoring:**\n- Translation from subjective impression to numerical score always contains subjective elements\n- Difficulty quantifying what makes an application move from average (3) to above-average (4-5)\n- Challenge in identifying the \"extra little something\" that distinguishes borderline cases\n\n**High Application Volume with Complex Cases:**\n- Majority of applications are Masters and PhD level, which are \"more complicated to deal with\" than Scholar applications\n- Competition is stiff; only top few applicants pass initial academic filter\n- Must manage large cohorts while making nuanced judgments\n\n**Borderline/Edge Cases:**\n- Difficulty making definitive yes/no decisions on borderline applications\n- Ambiguity in assessing whether applicant demonstrates genuine ambition vs. generic motivation statements\n- Challenge distinguishing thoughtful motivation from superficial \"I want to change the world\" statements\n\n**Alignment Assessment Issues:**\n- Interdisciplinary applicants sometimes lack foundational disciplinary knowledge for proposed topics\n- Difficulty identifying gaps between intended study and academic preparation, particularly in quantitatively demanding fields\n- Weakness of South African university preparation in quantitative disciplines creates additional assessment burden\n\n**Benchmarking Challenges:**\n- Annual \"reset\" means context from previous years must be reconstructed after reviewing first few applications\n- Tension between relative benchmarking (comparing within current cohort) and absolute benchmarking (comparing against international standards)\n- Risk of ranking applicants highly within cohort while they still fall short of absolute standards for their intended programs\n\n**Inefficient Pre-Screening:**\n- Currently processes applications that lack sufficient academic credentials for their intended elite international universities\n- Time wasted on clearly unqualified applicants who consume reviewer time without realistic chance of success\n\n---\n\n## 4. What They Value in Applications\n\n### Key Assessment Criteria:\n\n**Academic Record (Primary Filter):**\n- Strong academic performance is first and most rigorous filter\n- Marks must align with demands of intended institution and program\n- Example given: 78% average insufficient for top international universities\n\n**Field Alignment:**\n- Applicant's intended field of study must align with existing qualifications\n- Academic background must provide sufficient disciplinary foundation for proposed topic\n- Quantitative capability must match program demands (e.g., accounting requires stronger quantitative skills than marketing or HR)\n\n**Motivation Essay Quality (Secondary Differentiation):**\n- **Focus:** Clear articulation of what applicant wants to study\n- **Realism:** Feasibility of proposed research/study direction\n- **Clarity:** Transparent communication of goals\n- **Authenticity:** Genuine voice and authorship (increasingly difficult to assess)\n- **Ambition:** Evidence of drive beyond generic statements like \"I want to change the world\"\n- **Thoughtfulness:** Demonstration that applicant has genuinely considered their motivation\n- **Flare/Drive:** Personal qualities that distinguish exceptional essays from mediocre ones\n\n**Value Added:**\n- What specific value does the applicant bring to their field of study?\n- Is the application showing genuine specialization or generic qualification-seeking?\n\n**Disciplinary Foundation:**\n- For PhD applicants: mastery of field literature and existing knowledge base\n- For Masters applicants: basic foundational skills and understanding\n- For Scholars: highest standard\u2014requires excellence even to achieve average score\n\n**Contextual Background Understanding:**\n- For South African applicants: recognition of educational disparities across socioeconomic backgrounds\n- Applicants from non-private school backgrounds may have less sophisticated language or grammar without diminished intellectual capacity\n\n### Red Flags:\n\n1. **AI-Generated Essays:** Sudden improvement in English quality, bland and generalist language, absence of personal voice, lack of ambition or distinctive insight\n\n2. **Academic Insufficiency:** Marks too low for intended program demands; inadequate preparation in quantitative skills for quantitatively demanding programs\n\n3. **Misalignment:** Interdisciplinary moves without foundational knowledge; proposed research not grounded in existing academic preparation\n\n4. **Generic Motivation:** Boilerplate statements (\"I am motivated,\" \"I want to change the world\") without evidence of genuine thought or specific focus\n\n5. **Lack of Disciplinary Knowledge:** For PhD applicants, unfamiliarity with field literature or limited understanding of existing knowledge base\n\n6. **Insufficient Depth:** Essay demonstrates surface-level thinking rather than genuine engagement with intended field\n\n### What Makes Applications Stand Out:\n\n1. **Distinctive Ambition:** Clear evidence of drive and thoughtful motivation beyond generic aspirations\n2. **Authentic Voice:** Personal, genuine communication that reveals applicant's genuine thinking\n3. **Specific Focus:** Clearly articulated, specific research interests rather than broad field interest\n4. **Demonstrated Fit:** Clear alignment between academic background and intended program; evidence of requisite foundational knowledge\n5. **Realistic Assessment:** Applicant demonstrates understanding of what their chosen field demands\n6. **Evidence of Thought:** Essay shows genuine intellectual engagement with proposed research direction\n\n### Weighting of Factors:\n\n1. **Academic Record:** Primary filter; most heavily weighted initially\n2. **Field Alignment:** Secondary filter; eliminates candidates with insufficient disciplinary foundation\n3. **Motivation Essay:** Tertiary differentiation; determines who among academically qualified applicants rises above average\n4. **Level-Specific Demands:** Weightings adjusted by application level\n   - **Masters:** Basic foundational skills sufficient\n   - **PhD:** High standards for mastery of field and literature\n   - **Scholar:** Highest standards; must demonstrate excellence to achieve even average score\n\n---\n\n## 5. Views on AI/Technology\n\n### Overall Attitude:\nPragmatic concern. Philippe acknowledges AI as an inevitable and escalating challenge but does not express ideological opposition. His tone is realistic about the problem and solution-oriented.\n\n### Specific Concerns:\n\n**Detection Problem:**\n- Current AI detection is difficult; approximately 80% of essays in one parallel university process flagged by Turnitin's AI detector at high probability\n- Increasing sophistication of generative models makes detection harder and will continue deteriorating\n- Cannot definitively prove AI authorship even with detection tools\n\n**Capability Escalation:**\n- Generative text was \"bland and generalist\" in 2023-2024, allowing distinguishing features (ambition, flare, drive, focus) to be identifiable\n- Current and future models can be prompted to generate essays reflecting sophistication, ambition, and field-specific knowledge\n- Problem will become \"more and more difficult\" as technology advances\n\n**Impact on Essay Validity:**\n- Essays will become unreliable as distinguishing mechanism between applicants in foreseeable future\n- \"Going forward I think that you're going to have a serious problem relying on essays as a mechanism for distinguishing between applicants\"\n\n### What Would Be Automated (Acceptable to Philippe):\n\n1. **Pre-Screening:** Stricter automated or systematic pre-screening to filter candidates whose academic records clearly insufficient for intended programs\n\n2. **Relative Benchmarking Calculations:** Computational support for establishing relative rankings within cohorts\n\n3. **Administrative Tasks:** Candidate data organization, scoring sheet management, etc.\n\n### What Should NOT Be Automated (According to Philippe):\n\n1. **Final Judgments on Borderline Cases:** Assessment of the \"extra little something\" that moves candidates from average to above-average requires human judgment and tacit knowledge\n\n2. **Holistic Academic Fit Assessment:** Determination of alignment between applicant background and intended field requires contextual understanding that humans provide better\n\n3. **Bias-Conscious Evaluation:** Assessment that accounts for socioeconomic background diversity and educational disparity context requires human sensitivity\n\n### Trust and Transparency Requirements:\n\n**Interviews as Verification Mechanism:**\n- Uses interviews as comparison point with essay content to detect discrepancies\n- \"It's you and the interview panel. It's not you and ChatGPT\"\n- Interview allows verification of whether applicant actually wrote/understands essay content\n\n**Benchmarking Transparency:**\n- Needs absolute standards in addition to relative ones\n- Wants assurance that top-ranked applicants in cohort will actually succeed at intended programs (not just rank well relative to peers)\n- Requires clear communication of standards and their justification\n\n**Human-in-the-Loop Philosophy:**\n- AI should support human judgment, not replace it\n- Critical decisions need human oversight and contextual understanding\n\n---\n\n## 6. Suggestions & Ideas\n\n### Process Improvements:\n\n**Address AI Challenge:**\n\n1. **Implement Interviews:**\n   - Use interviews as direct verification mechanism for essay authenticity\n   - Compare essay content against in-person responses to verify applicant understanding\n   - \"Interview panel. It's not you and ChatGPT\"\n\n2. **Timed, Proctored Essays:**\n   - Require essays written in real-time (e.g., \"log on at 9:00 and have 25 minutes\")\n   - Implement proctoring to ensure applicant authorship\n   - Parallels university oral defense model now used to verify thesis authorship\n\n### Stricter Pre-Screening:\n\n1. **Earlier Academic Filter:**\n   - Screen out applicants whose academic records clearly insufficient for intended high-level international universities\n   - Example: Applicant with 78% average seeking admission to top-tier international program should be filtered pre-review\n   - Save adjudicator time by eliminating clearly unqualified candidates\n\n2. **Quantitative Preparedness Assessment:**\n   - For programs with high quantitative demands (accounting, econometrics, etc.), pre-screen for adequate quantitative foundation\n   - South African universities weak in quantitative preparation; flag candidates with only first/second-year statistics modules applying to highly quantitative programs\n   - Earlier elimination of misaligned candidates\n\n3. **Strengthen Pre-Screening Rigor:**\n   - \"I think there is a pre-screening, but it could be a bit a bit stricter\"\n\n### Bias Mitigation:\n\n1. **Conscious Awareness Framework:**\n   - Review process should explicitly account for socioeconomic background diversity\n   - Avoid penalizing applicants from non-private school backgrounds for less sophisticated language or grammar\n   - Recognize legitimate intellectual capability despite language sophistication differences\n\n2. **Importance of Personal Essay:**\n   - Personal essays critical for understanding applicant background and context, particularly in diverse country context\n   - Should inform interpretation of language quality and expression sophistication\n\n### Benchmarking Framework:\n\n1. **Dual Benchmarking Approach:**\n   - Maintain both relative benchmarking (within cohort) and absolute benchmarking (against program/institutional demands)\n   - Ensure top-ranked applicants meet absolute standards for their intended programs, not just rank highest in cohort\n   - \"You're number one in our pack of 20 but then they will still fall short of what is needed\"\n\n2. **Context-Specific Absolute Standards:**\n   - Different programs require different quantitative/qualitative capability baselines\n   - Accounting more quantitatively demanding than marketing or HR\n   - Align assessment standards to specific program requirements\n\n### Rubric Enhancement:\n\nPhilippe did not propose specific rubric changes but implied that current system lacks:\n- Explicit, transparent criteria for distinguishing between adjacent scores (3 to 4, 4 to 5)\n- Articulated markers for \"the extra little something\"\n- Formalized guidance on level-specific weighting differences\n\n---\n\n## 7. Key Direct Quotes\n\n### On Subjectivity in Essays and Judgment:\n\n**Quote 1:**\n> \"So so so it's usually with when when you start reading the the the the the essay uh because that is you know it's like grading essays...when when it is more about the self-escription essay or or later on when you look at what what the motivation for what they want to study um there's a bit more to that uh but but when it's this motivation essay I think that is where the the biggest issue comes in and that that is very subjective\"\n\n*Context: Philippe identifying where judgment calls and gut feel most heavily influence scoring, specifically locating the challenge in motivation essay assessment*\n\n---\n\n**Quote 2:**\n> \"There's there's always uh in that because you you do a translation of what your impression into a into a score. Um so so there will always be something uh uh um in there that that's a bit subjective.\"\n\n*Context: Explaining why scoring inherently contains subjective elements despite efforts to standardize, highlighting the translation from impression to number*\n\n---\n\n### On Identifying Borderline Cases:\n\n**Quote 3:**\n> \"So as I said then then you look at let's say the essay or the motivation for what they want to go study and you look um and and and if it is a borderline um you know you look for for for that little extra that that that would carry it to the from a three to a four or four to a five. Um uh uh so so that is usually you know what's the value added that that that you bring\"\n\n*Context: Describing the process for distinguishing between average and above-average applications, using the metaphor of \"that little extra\"*\n\n---\n\n**Quote 4:**\n> \"Can you can you spot a bit of ambition or is this just, you know, a run-of-the-mill I'm going to change the world? Uh, has there's been some thought in it? I think that's the other thing you you you know, you can get you can write something very easily to say yes, I want to change this and that, but has there some been some thought in in in in in that motivates you to to to do this?\"\n\n*Context: Explaining what he looks for to elevate applications from generic to distinguished; identifying \"thoughtfulness\" as a key differentiator*\n\n---\n\n### On AI and Essay Assessment:\n\n**Quote 5:**\n> \"Uh you get very strong suspicion. Very good...you get students who...all of a sudden improved a lot uh for some of these cases which suggest uh that it might be Chad writing them instead of the candidate.\"\n\n*Context: Early identification of AI-generated essays based on sudden English quality improvement*\n\n---\n\n**Quote 6:**\n> \"The thing is that we had in the past but of course now the AI uh uh you know the text generation becomes way more sophisticated but uh let's say in 2023 2024 still know back then you you could still see that something like a chat GBT generated text was very bland and and generalist. Um, and then these things that I highlighted, you know, the the bit of ambition and so on, uh, the bit of flare, the bit of drive, the bit of focus, reflecting that you've understood the issue, you could still pick that up from from from your better essays. Um, but of course now that that will become more and more difficult\"\n\n*Context: Tracking the escalation of AI sophistication and its increasing threat to essay-based assessment; explaining how previous distinguishing features (ambition, flare, drive, focus) become harder to identify*\n\n---\n\n**Quote 7:**\n> \"So um so what we then need to need needed to do is to actually have an interview with these students uh because then it is you and the interview panel. It's not you and Chad GPT. Um and and then that way you sort of can compare uh what was said in the essay with what what what comes out in an interview. um if there's a big discrepancy there then then then that is an indicator\"\n\n*Context: Proposing interviews as a solution mechanism, emphasizing human-to-human verification over text-based assessment*\n\n---\n\n### On Conscious Bias and Context:\n\n**Quote 8:**\n> \"So look, so so the one thing that one really looks at is and that you need to keep in mind is the so you get students who are really brilliant but you know they did not necessarily have a model ski model C school training. So um their use",
    "tokens_in": 8344,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Pieter Pistorius",
    "date": "2026_01_19 13_24 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Pieter Pistorius) \u2013 2026_01_19 13_24 SAST \u2013 Notes by Gemini.docx",
    "text_length": 6749,
    "analysis": "# OMT Discovery Interview Analysis: Pieter Pistorius\n\n## 1. Interviewee Role & Background\n\n**Role in OMT Review Process:**\n- Reviewer/adjudicator for postgraduate scholarship applications\n- Provides scoring and written rationale for applications\n- Does not make final funding recommendations; only returns scores\n\n**Duration of Involvement:**\n- Approximately three years (as of January 2026)\n\n**Discipline/Expertise Area:**\n- Metallurgical Engineering Department, University of Pretoria\n- Background spanning industry (17-20 years) and academia (9 years initially, returned in 2015)\n- Teaches postgraduate honors program and supervises final-year research projects\n- Also reviews admissions for his own university's postgraduate program\n\n---\n\n## 2. Current Review Process Description\n\n**Overall Process Structure:**\n\nThe review process follows a three-phase approach:\n\n**Phase 1: Initial Scan (5 minutes)**\n- Quick overview of applicant pool to understand \"who they are\"\n- Gets a sense of the group before deep review\n\n**Phase 2: Detailed Document Review (45 minutes to 1 hour per applicant)**\n- Conducted in evenings, typically across 2-3 evening sessions\n- Reads every document in a single sitting for each application\n- Checks for discrepancies and consistencies\n- Does not take extensive notes during this phase\n- Completes the evaluation form without finalizing scores\n\n**Phase 3: Consistency Check & Finalization (~20 minutes)**\n- Reviews overall ratings after completing all applications\n- Looks for outliers and halo effects\n- Checks personal work for consistency before submitting\n- Finalizes scores at this point\n\n**Tools & Systems Currently Used:**\n- Web-based system (described as \"quite slick\")\n- Evaluation form with rubric\n- Spreadsheet previously used (described as \"very clunky\")\n- Manual timeline creation for tracking applicant academic history gaps\n\n**Time Commitment:**\n- 2-3 evenings total (approximately 45 minutes to 1 hour \u00d7 number of applicants + 20 minutes review)\n- Typically works within the given 2-week turnaround time, though prefers to complete detailed phase well ahead of deadline\n\n---\n\n## 3. Pain Points & Challenges\n\n**Primary Frustrations:**\n\n1. **Determining True Motivation (Most Significant Pain Point)**\n   - Difficulty distinguishing genuine commitment from last-resort applications\n   - Uncertainty about whether funding supports career development or enables emigration\n   - Example cited: suspecting sabbatical applicants may be using OMT as fallback\n\n2. **Timeline/Academic Record Compilation (Tedious/Superfluous Cognitive Load)**\n   - Compiling coherent timelines from diverse academic records across different institutions\n   - Tracking gaps in education/employment across formats (B.Tech, B.Sc, etc.)\n   - Must manually create diagrams to understand applicant's academic progression\n   - Describes this as \"quite tedious\"\n\n3. **Halo Effect Management**\n   - Risk of bias toward applications in familiar research areas\n   - Consciously forcing \"extreme\" scoring initially to counteract this\n\n4. **Tight Deadline Pressure**\n   - Preference to work well ahead of 2-week turnaround to allow \"sleeping on it\"\n   - Currently works closer to deadline than ideal\n   - Acknowledges this is self-management issue rather than OMT's fault\n\n5. **Lack of Context on Review Process**\n   - Doesn't know whether he's one of 2 or one of 5 reviewers\n   - Prefers not to know to avoid second-guessing his judgment\n   - Uncomfortable with calibration against previous cohorts due to time gaps and different applicant mixes\n\n6. **Rubric Limitations (Minor)**\n   - Acknowledges rubrics are \"by definition limiting\"\n   - Forces difficult binary decisions\n   - However, notes this isn't necessarily disadvantageous\n\n---\n\n## 4. What They Value in Applications\n\n**Key Evaluation Criteria:**\n\n1. **High Integrity & High Caliber (Primary)**\n   - \"OMT plays great store on sort of well clearly on high integrity, high caliber applications\"\n   - Seeking intellectually capable individuals\n\n2. **Community Contribution & Return to South Africa (Highly Important)**\n   - Assesses whether funding builds domestic or international talent pools\n   - Questions: \"How deep are the ties to South Africa?\"\n   - Explicit concern about \"funding them to immigrate or funding them to come back\"\n   - Views as \"somewhat old-fashioned\" but important consideration\n\n3. **Career Coherence & Trajectory**\n   - \"Does it make sense?\" - broad strokes assessment\n   - How the application moves specific applicant's career in a specific direction\n   - Examines whether postgraduate plans align with background\n\n4. **Applicant Context & Motivation**\n   - Considers socioeconomic background and personal circumstances\n   - Example: recognizes how medical emergency influenced applicant's field choice\n   - Acknowledges need to look at \"social context\" while not favoring mediocrity from disadvantaged backgrounds\n   - Distinction between excellent candidates and mediocre ones from impoverished contexts\n\n5. **Consistency & Integrity of Application Materials**\n   - Checks for discrepancies between documents\n   - Verifies department references against university websites\n   - Notes applications are \"vetted and scrubbed quite carefully\"\n   - Has found few major inconsistencies\n\n6. **Authenticity of Purpose**\n   - Struggles most with determining true motivation\n   - Seeks to understand what is \"really driving\" the applicant\n   - Concerned with distinguishing genuine pursuit from desperation\n\n**Red Flags:**\n- Applicants unlikely to return to/contribute to South Africa\n- Gaps in academic timeline without clear explanation\n- Applications that feel like \"last resort\" (no genuine motivation apparent)\n- Weak career narrative despite strong credentials\n\n**What Makes Applications Stand Out:**\n- Clear personal context that explains field choice/motivation\n- Coherent career trajectory with meaningful progression\n- Evidence of commitment to contributing to South African community\n- Strong intellectual capability paired with integrity\n- Memorable applications are \"very memorable\" and remain vivid years later\n\n**Weighting of Factors:**\n- Confidence high in academic merit assessment (own domain expertise)\n- Lower confidence in motivation assessment\n- Uses rubric to systematize evaluation but doesn't rely purely on it\n- Willing to critically re-examine when intuition conflicts with rubric score\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Attitude: Cautiously Skeptical/Lukewarm**\n\n**General Philosophy:**\n- \"I'm somewhat cynical about AI\"\n- Acknowledges it's useful but not a panacea\n- Personal experience with AI in undergraduate context (detecting AI-generated reports)\n- Recognizes AI \"is not going to go away\" and could help \"in ways I probably cannot imagine\"\n\n**Concerns About AI Implementation:**\n\n1. **Double Measurement Problem**\n   - \"It is like measuring something twice. The two measurements will differ. Now you have to explain three things: the first measurement, the second measurement, and the difference.\"\n   - Skeptical this adds value\n\n2. **Human Decision-Making Essential**\n   - \"Eventually the human being must make a final call\"\n   - Rules out AI for final decisions\n   - Firm on this requirement\n\n3. **Lack of Visibility into Other Reviewers**\n   - Prefers not to know number of reviewers to avoid second-guessing\n   - Concern about calibration and consensus\n\n**What AI Could Help With (Potential Use Cases):**\n\n1. **Initial Screening (High Value)**\n   - Reducing \"superfluous cognitive load\"\n   - Filtering out \"obviously low caliber ones\"\n   - Automatic elimination of inconsistencies\n\n2. **Timeline/Academic Record Compilation (High Value)**\n   - Compiling coherent timelines from diverse academic records\n   - Identifying and flagging gaps in progression\n   - Making sense of different qualification formats (B.Tech vs. B.Sc, etc.)\n   - Described as \"completely different level\" of tedious work in his university admissions role\n\n3. **Structural Support**\n   - Helping \"give structures to an issue\"\n   - Making initial \"skilling\" work easier\n\n**What Should NOT Be Automated:**\n- Final decision-making\n- Motivation assessment\n- Context evaluation\n- Integrity/caliber judgment\n- Edge case decisions (choosing between close candidates)\n\n**Required Trust & Transparency:**\n- Would want visibility into how AI reached its decisions\n- Needs clear explanation of AI scoring rationale\n- Must understand differences between AI assessment and human assessment\n- Concerned about \"second-guessing what other people may say or may not say\"\n\n**Experimental Interest:**\n- Open to running experiment to test whether AI-initial-scoring \"would really change things or make it easier\"\n- Willing to try if properly structured\n\n---\n\n## 6. Suggestions & Ideas\n\n**Process Improvements Suggested:**\n\n1. **Self-Management/Deadline Optimization (Primary)**\n   - Start detailed review phase well ahead of deadline\n   - Allow time to \"sleep on it\" before finalizing\n   - Benefit of web-based system: can leave scores sitting and return to review with fresh perspective\n   - Not OMT's responsibility but reviewer discipline (\"it depends on how I manage myself\")\n\n2. **AI-Assisted Initial Screening (Clear Priority)**\n   - Implement AI to handle tedious timeline/gap analysis\n   - Auto-flag inconsistencies and obviously low-caliber applications\n   - Reduce cognitive load on human reviewers for tedious documentation work\n   - Would significantly help with mixed academic record formats\n\n3. **Better Academic Record Standardization** (Implied)\n   - Applications currently come in different formats from different institutions\n   - Standardizing or AI-organizing these could save significant time\n\n4. **Preserving Human Judgment on Key Dimensions**\n   - Keep human review for motivation, context, integrity assessment\n   - Maintain human review for edge cases\n\n5. **Rubric Enhancement** (Minor)\n   - Current rubric is \"quite useful\" and \"reasonably well thought out\"\n   - No major changes suggested\n   - Pieter satisfied with 4-5 point scale (avoids obscure fine-tuning of 10-point scales)\n\n**Features/Capabilities Desired:**\n- Clear timeline visualization of applicant's academic/career progression\n- Automated consistency checking\n- Structured flagging of information gaps\n- Preservation of human decision authority\n- Transparency in how AI reaches initial scores\n\n**Priorities for Change:**\n1. **High Priority:** Automate tedious timeline compilation and gap analysis\n2. **High Priority:** Initial screening to filter obvious low-caliber candidates\n3. **Medium Priority:** Review process timing/deadline management (self-directed)\n4. **Low Priority:** Rubric revision (current system working well)\n\n---\n\n## 7. Key Direct Quotes\n\n### Quote 1: On True Motivation Assessment (Primary Pain Point)\n**Context:** Discussing where he feels least confident in judgment\n\n> \"The least confident is probably this consideration of what is the true motivation... Sometimes I work it out sometimes I don't... There was somebody that applied for support for a sabbatical leave... I got this sense that he or she was that this was the sort of point of application of the last resort... What I quite often struggle with is to find out the motivation. What is really driving this specific individual?\"\n\n**Significance:** Identifies the core challenge in application review\u2014understanding authentic intent versus desperation.\n\n---\n\n### Quote 2: On South Africa Retention Concern\n**Context:** Discussing career fit and what makes applications stand out\n\n> \"Something else that I tend also to check is you know what are the chances... but you know how deep are the ties to South Africa... These are high caliber people... Are you funding them to immigrate or are you funding them to come back and in some way enrich South Africa... You can spend a lot of money to build a US or a European talent pool. I don't think that is the intention.\"\n\n**Significance:** Reveals core value alignment with OMT mission and a major evaluation criterion that's subtle but essential.\n\n---\n\n### Quote 3: On Rubric vs. Intuition Resolution\n**Context:** Discussing how to handle discrepancies between gut feeling and rubric score\n\n> \"I would follow the rubric. If there's a discrepancy... I'll go and take a look and say why is it lower, and I tend to go back to the individual parameters and look at that again. But I wouldn't sort of push it towards my first intuition... Quite often what I see is that the rubric is actually okay... This individual looked very impressive, but if you look at the details, maybe it wasn't... So I wouldn't rework my scoring to get to my first answer.\"\n\n**Significance:** Shows sophisticated metacognitive approach to bias management and rubric use; prioritizes systematic evaluation over intuition.\n\n---\n\n### Quote 4: On Context & Social Background Evaluation\n**Context:** Justin asks about role of applicant's context; Pieter provides example\n\n> \"There was a student... his father had a major medical emergency and he was severely incapacitated when he was a teenager and based on what he saw there he decided to go into medical engineering... You need to look at that... We see the same with our students... Some come from absolutely middle class environments, some you can see that the clothes that they wear when they sit in front of you is the clothes that they have... You need to look at that... You need to balance this... Someone that comes from an impoverished neighborhood may be excellent or mediocre. I don't think our idea is to support a mediocre candidate but you need to look at the social context.\"\n\n**Significance:** Demonstrates nuanced understanding of contextual factors; shows willingness to consider hardship without lowering standards.\n\n---\n\n### Quote 5: On Cynicism Toward AI & Human Decision Authority\n**Context:** Discussing potential role of AI in review process\n\n> \"I'm somewhat cynical about AI... It is like measuring something twice. The two measurements will differ. Now you have to explain three things: the first measurement, the second measurement, and the difference... Eventually the human being must make a final call... It's not going to go away and I think it could help in ways which I probably cannot imagine.\"\n\n**Significance:** Captures the cautious pragmatism\u2014sees value but not cure-all; insists on human judgment authority.\n\n---\n\n### Quote 6: On Superfluous Cognitive Load\n**Context:** Discussing what AI could help with\n\n> \"I would think that is probably... if you look at a CV or the combination of a CV and academic record is to say but is there a gap... That sort of... can be quite tedious... I sometimes draw out a little diagram... Almost to compile a timeline... to look at gaps and understand what has happened... I think that would help quite a bit.\"\n\n**Significance:** Identifies specific, quantifiable pain point where automation would have highest impact.\n\n---\n\n### Quote 7: On The Humbling & Inspiring Nature of Review\n**Context:** Closing remarks reflecting on the process\n\n> \"It is always quite a humbling experience... The caliber of the candidates... I get a feeling sometimes you know it's quite often quite a humbling experience to realize that we are people with significant intellectual capability and they are really managing their lives well and they look towards other people around them as well... It's really quite inspiring... You know it is a lot of hours that you burn but you know it's once a year so it's not a catastrophe.\"\n\n**Significance:** Reveals deeper motivation for careful review; suggests emotional investment in process; frames effort as worthwhile despite time cost.\n\n---\n\n### Quote 8: On Not Benchmarking Against Previous Cohorts\n**Context:** Discussing whether memories of previous candidates help or hinder\n\n> \"I try to look at that cohort in itself... To benchmark would be difficult I think because the processes are so far apart and of course you know I may get a different mix... I on purpose I don't try to benchmark... I think they asked me because they value my opinion not to calibrate to support what the eventual decision was.\"\n\n**Significance:** Shows awareness of potential bias and commitment to independent judgment; resists institutional pressure for consistency.\n\n---\n\n### Quote 9: On Preference for Not Knowing Reviewer Ensemble\n**Context:** Asked about visibility into other reviewers and review structure\n\n> \"I don't have any visibility on that. And you know, I don't think it would affect my decision-making. So, I'd rather not know... I try to look at it based on my own judgment and not to sort of second guess what other people may say or may not say.\"\n\n**Significance:** Reveals preference for independence and potential concern about groupthink; wants autonomy.\n\n---\n\n### Quote 10: On The Writing of Rationale\n**Context:** Discussing extent of motivation/rationale documentation\n\n> \"I tend to limit myself to about two or three sentences, you know. No, not an essay... I don't have visibility on how many reviewers there are... I'd rather not know.\"\n\n**Significance:** Indicates efficient but minimal documentation; paired with rationale implies practical approach to process rather than comprehensive explanation-building.\n\n---\n\n## 8. Unique Insights\n\n### 1. **The \"Deep Ties to South Africa\" Criterion as Core Mission Alignment**\nPieter explicitly frames applicant retention/return to South Africa as a core criterion, but it's framed philosophically (\"somewhat old-fashioned and naive\") rather than operationally. This is distinctive\u2014most reviewers might not articulate this as clearly or defend it as persistently. **Insight for OMT:** This may not be systematically measured in the rubric but is clearly important. The AI initial screening could specifically flag indicators of this (permanent residence status, family ties, expressed career intentions in SA, etc.).\n\n---\n\n### 2. **The",
    "tokens_in": 10922,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Ryan Nefdt",
    "date": "2026_01_21 14_25 SAST",
    "filename": "OMT Discovery Interview with StrideShift (Ryan Nefdt) \u2013 2026_01_21 14_25 SAST \u2013 Notes by Gemini.docx",
    "text_length": 6423,
    "analysis": "# OMT Application Review Analysis: Ryan Nefdt\n\n## 1. Interviewee Role & Background\n\n**Role in Review Process:**\n- Philosophy professor at the University of Cape Town who reviews OMT postgraduate scholarship applications\n- Evaluates primarily humanities submissions but also receives applications in related areas\n\n**Duration of Involvement:**\n- Not explicitly stated, but implied to be ongoing with multiple evaluation cycles\n\n**Discipline/Expertise Area:**\n- Primary: Philosophy and Linguistics\n- Secondary: Fine Art, Sociology, Artificial Intelligence\n- Describes himself as bridging science and humanities with active involvement in projects on large language models and statistical models\n- Has qualifications and broad interests beyond philosophy\n\n---\n\n## 2. Current Review Process Description\n\n**Systematic Approach:**\n1. **Initial Read:** Reads submission once independently without predisposing it to rubric categories (when time permits)\n2. **Structured Review:** Reviews submission systematically while referencing the OMT rubric simultaneously\n3. **Background Analysis:** Examines the background section first, looking for cohesive narrative connecting life story to academic goals\n4. **Impact Assessment:** For humanities, specifically evaluates articulated societal impact pathways\n5. **Rubric Application:** Uses rubric to extract relevant information and assign numerical values\n6. **Comparative Adjustment:** Reviews across the application pool within a cycle and adjusts scores to ensure consistency and fairness\n\n**Tools/Systems:**\n- OMT rubric (primary tool)\n- OMT portal (where rubric is housed)\n- Google searches for discipline-specific terminology/literature (self-directed when needed)\n\n**Time Management:**\n- Manages cycles of 20-30 applications at a time\n- Uses rubric for efficiency rather than feeling burdened; describes the process as a \"joy\"\n- When time-constrained, reads submission and rubric simultaneously rather than reading separately first\n\n---\n\n## 3. Pain Points & Challenges\n\n**Challenge 1: Rubric Edge Cases for Theoretical Projects**\n- Rubric struggles with \"highly intricate theoretical projects\" where societal impact is difficult to express\n- Ryan characterizes these as \"puzzle master\" work\u2014intellectually valuable but not easily captured by impact-focused rubric categories\n- The rubric's structured approach to measuring societal impact doesn't fully capture merit of pure theoretical pursuits (00:08:52)\n\n**Challenge 2: Disciplinary Knowledge Gaps**\n- When reviewing outside direct expertise (e.g., ecology), lacks background on discipline-specific terms, literature, and issues\n- Must spend time Googling to understand context, which becomes impractical with large application volumes\n- Applicants must use precious word count explaining fundamental concepts to non-expert reviewers\n\n**Challenge 3: Recommendation Letter Limitations**\n- Views recommendation letters as largely \"testimonial\" and low-value\n- Cannot rely on comparative information (where student ranks among others)\n- Difficulty with cultural interpretation\u2014African contexts emphasize interpersonal skills over individualistic academic achievement, creating potential for misreading\n- Risk of reading subjective cues that may reflect recommender personality rather than applicant merit\n- Absence of guidance on comparative positioning (unlike Oxford/Cambridge systems) reduces utility\n\n**Challenge 4: Holistic Application Assessment Difficulty**\n- Online form structure encourages disconnected answers\u2014applicants fill boxes sequentially without seeing application as whole\n- Difficult to identify whether applicant approached application holistically or just processed form-filling\n- \"Level of preparation\" is important but hard to quantify for rubric purposes\n\n**Challenge 5: Identifying Inconsistencies in Unfamiliar Domains**\n- When reviewing outside direct field, may miss inconsistencies between problem identification and proposed solutions\n- Can overlook inappropriate methodological choices if unfamiliar with discipline\n\n---\n\n## 4. What They Value in Applications\n\n**Key Criteria:**\n\n1. **Cohesive Narrative**\n   - Connections between life experience, academic development, and current work\n   - Personal story should directly inform and explain academic trajectory\n   - Example given: Ryan's own multilingual upbringing directly led to linguistics interests (00:06:31)\n\n2. **Articulated Societal Impact (Humanities-specific)**\n   - Must demonstrate awareness of how work matters beyond theoretical puzzle-solving\n   - Doesn't require perfect fit, but applicant must show consideration of societal goals\n   - Recognition that impact pathways differ from sciences (publishing in high-impact journals)\n\n3. **Holistic Preparation**\n   - Evidence that applicant thought about application as unified whole, not separate form-filling exercise\n   - Application that appears thoughtfully composed rather than rushed/transactional\n   - (00:14:26)\n\n4. **Discipline-Specific Intellectual Merit**\n   - Recognition of intricate theoretical work even when societal impact is difficult to articulate\n   - Values \"puzzle master\" work\u2014intellectually rigorous pursuits even without obvious social application\n\n**Red Flags:**\n- Irrelevant life background that doesn't connect to academic pursuits\n- Lack of consideration for societal impact in humanities work\n- Disconnected answers suggesting form-filled approach rather than holistic thinking\n- Inappropriate tools/methods proposed for identified problems\n\n**What Makes Applications Stand Out:**\n- Clarity of narrative arc connecting person to purpose\n- Thoughtful engagement with discipline-specific questions\n- Evidence of deep preparation and coherent vision\n\n**How They Weigh Factors:**\n- Rubric scoring takes priority over gut instinct to neutralize personal bias\n- Will adjust scores comparatively within evaluation cycle to maintain consistency\n- When on fence, considers level of preparation and holistic approach as tiebreakers\n- Does not compare across years/cycles\u2014evaluates each pool on own merit\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Attitude:**\n- \"A bit of a lite\" (skeptical) about AI in this context (00:23:47)\n- Worried about AI summarization specifically\n- Does not experience cognitive overload with current process, so primary motivation for AI assistance is unclear\n\n**What He Would NOT Want Automated:**\n- Summarization of applications\n- Core evaluation decision-making\n- Any process that reduces transparency in how merit is assessed\n\n**What Could Be Helpful (With Specific Constraints):**\n1. **Discipline-Specific Context Provision** (strongest support)\n   - Providing background on specialized terminology, literature, and field-specific issues\n   - Explaining where research is situated within its discipline\n   - Could reduce burden on applicants to spend word count explaining fundamentals (00:25:53-26:47)\n   - \"An assistant could bridge a part of that gap for them\"\n\n2. **Inconsistency Flagging** (conditional support)\n   - Identifying where applications don't hold together holistically\n   - Flagging where proposed solutions don't match identified problems\n   - Particularly useful in non-specialist domains (00:27:42)\n   - \"If the tool could identify inconsistencies when you're marking or evaluating a lot of these things, sometimes you miss it\"\n\n**Trust & Transparency Requirements:**\n- Must understand what AI is doing and why\n- Should not replace human judgment\n- Should augment rather than automate\n- Concerned about objective validity of AI interpretations\n- Would need to trust the tool's reliability before relying on it\n\n---\n\n## 6. Suggestions & Ideas\n\n**Rubric Improvements:**\n\n1. **Include Level of Preparation/Holistic Approach**\n   - Current rubric doesn't capture whether application was approached thoughtfully as whole\n   - Suggested as tiebreaker criterion (00:14:26)\n   - Acknowledges this may be \"too subjective\" but sees value\n\n2. **Reconsider NRF Rating Scoring** (Already implemented)\n   - P rating (future leader potential) should score higher than C rating\n   - Fewer people achieve P rating; should reflect difficulty (00:12:17)\n   - Ryan has already communicated this change to OMT\n\n3. **Reconsider Budget Scoring Removal**\n   - Budget section previously helped score something \"felt objective\"\n   - Unclear why it was removed; valued this element\n\n**Process Improvements:**\n\n4. **Comparative Positioning in Recommendation Letters**\n   - Recommend adopting Oxford/Cambridge model with comparative questions\n   - Ask: \"Where would you place the student?\" and \"How long have you known them?\"\n   - Would increase cognitive load slightly but provide useful context\n   - Could offset issues with purely testimonial letters (00:21:20)\n\n5. **Provide Disciplinary Context for Reviewers**\n   - Create accessible background materials on research areas covered in application pool\n   - Reduce need for reviewers to Google discipline-specific terms\n   - Help reduce burden on applicants to explain fundamentals\n\n**Technology Suggestions:**\n\n6. **Inconsistency Detection Tool**\n   - Could flag logical inconsistencies between problem and solution\n   - Particularly valuable when reviewer is non-specialist\n   - Should work proactively during evaluation, not as post-hoc summarization\n\n7. **NOT Recommended:**\n   - Summarization features\n   - Automated scoring\n   - Black-box AI systems\n\n**Priority for Change:**\n- Ryan doesn't experience pressure for change but identifies rubric edge cases and recommendation letter limitations as most significant issues\n- Discipline-specific context provision seen as beneficial particularly for applicants in specialized fields\n- Improved recommendation letter framework highest priority among actionable changes\n\n---\n\n## 7. Key Direct Quotes\n\n**Quote 1: On Cohesive Narrative (foundational evaluation principle)**\n> \"I'm looking for if you can tell me a story that connects the different parts... tell me who you are as an academic and in so far as your life has informed that development tell me about your life... So that's one thing. Um I look for very often in humanities...\" (00:05:02-06:31)\n\n*Context: Explaining what he examines first in background section; shows how personal narrative is central to his evaluation framework*\n\n---\n\n**Quote 2: On Societal Impact in Humanities (discipline-specific insight)**\n> \"In the science you often like I'm going to publish in high impact journals and that kind of does the job for you... With humanities it's a little bit different. you have to figure out different conduits to um to getting some societal impact and that's something that OMT I've recognized over the years and they've been explicit that's something they care about.\" (00:06:31-07:41)\n\n*Context: Explaining unique evaluation criteria for humanities vs. sciences; shows sophisticated understanding of disciplinary differences*\n\n---\n\n**Quote 3: On Rubric Edge Cases and \"Puzzle Master\" Work (primary limitation)**\n> \"If you are engaging in a theoretical project that is somewhat lacking in terms of your ability to express the societal impact but that theoretical project is so intricate and interesting... sometimes there are issues that are just not going to be able to be translated that well, but they're very intricate. It's sort of like, maybe this is a personal thing, but I think if you're a puzzle master and you're working on an interesting puzzle, sometimes somebody else is going to realize the value of that puzzle.\" (00:08:52)\n\n*Context: Identifying where rubric fails to capture merit; articulates tension between structured evaluation and intellectual value recognition*\n\n---\n\n**Quote 4: On Rubric as Bias-Reduction Tool (key evaluation philosophy)**\n> \"I see the rubric as a measurement that allows comparison and that's why I'm a little reluctant to just form my own view outside of it because it's telling me I can use this... I would on the side of using the rubric because I'm assuming that is a tool to neutralize the sort of like personal bias that might creep in.\" (00:10:03-11:14)\n\n*Context: Explaining prioritization of rubric over gut instinct; shows deliberate strategy to counteract evaluator bias*\n\n---\n\n**Quote 5: On Recommendation Letters' Limited Value (critique)**\n> \"I don't put much stock in the recommendation letters... I think if you got somebody to write a letter for you, it's probably going to be fine... But that doesn't give me much information... I think reading between the lines isn't an objective linguistic practice.\" (00:16:48-19:11)\n\n*Context: Candid assessment of recommendation letters; shows skepticism of subjective interpretation methods*\n\n---\n\n**Quote 6: On Cultural Complexity in Reading Recommendation Letters (unique insight)**\n> \"In South Africa and in African context a little bit more than Southern Africa, people tend to place emphasis on interpersonal skills as an academic... Those cultural incongruities make me very nervous about reading between lines too often because I feel like I'm using a lot of stereotypes to do it.\" (00:19:11-20:05)\n\n*Context: Highlights cultural dimensions of evaluation that complicate recommendation interpretation; shows awareness of potential bias in cross-cultural assessment*\n\n---\n\n**Quote 7: On Why Recommendation Letters Should Include Comparative Data (concrete suggestion)**\n> \"Maybe if there was something where they actually like in certain context where they really are guided on like giving us a comparison point how like I pulled in some recommendations for Oxford and Cambridge recently and they have separate from the recommendation letter they have a bunch of questions like where would you place the student, how long have you known them, what is your level at the thing and that if that information is given to the evaluator that might counteract the other question.\" (00:21:20-22:36)\n\n*Context: Suggesting concrete model for improvement; shows familiarity with alternative systems*\n\n---\n\n**Quote 8: On When an AI Assistant Could Help (disciplinary context)**\n> \"I evaluate humanities generally... I don't always know background terms. Um I don't always know particular literatures. I don't always know particular issues. If I get a thing about ecology or whatever, maybe it would be nice... Maybe if there was some sort of assistant who could give you context of the discipline a little bit that might be useful like where this research proposal is situated in its discipline.\" (00:25:53-26:47)\n\n*Context: Only unsolicited suggestion for AI assistance; framed as helping with discipline-specific knowledge gaps*\n\n---\n\n**Quote 9: On Disciplinary Context as Benefiting Applicants (fairness dimension)**\n> \"It could help the applicant in a particular way because one of the things I'm looking for from the applicant then because I'm not a polymath who understands everything is for them to be able to translate whatever they're doing to me as a person who might not be an expert. This assistant could bridge a part of that gap for them. So they don't have to do all the work of saying listen I'm working on a particular kind of thing that you might not understand now I have to spend most of my word count which they don't have a lot of.\" (00:26:47-27:42)\n\n*Context: Frames AI assistance as fairness mechanism; shows concern for how knowledge gaps disadvantage specialist applicants*\n\n---\n\n**Quote 10: On Within-Cycle Comparative Evaluation (methodology)**\n> \"I don't think I ever think oh last year the crop was... I do see the pool as identifiable with each... I am comparative within [the cycle] and I will go back to other applications when I see something that I think oh wait you should have done this too and I gave you that mark but now it's not fair that I'm giving you that mark if I'm if this is what the standard should be.\" (00:29:00)\n\n*Context: Explains comparative approach within cycles; shows active calibration of standards across pool for consistency and fairness*\n\n---\n\n## 8. Unique Insights\n\n**Insight 1: \"Puzzle Master\" Philosophy of Merit**\nRyan articulates a distinctive view that intellectual merit in theoretical work exists independent of immediate societal impact applicability. His repeated metaphor of \"puzzle master\" work\u2014intricate, valuable intellectual puzzles that may later have uses\u2014captures a tension between utilitarian evaluation frameworks and academic research value systems. This is not just a complaint about the rubric, but a philosophical position about what constitutes merit in humanities scholarship. Few reviewers may articulate this so clearly.\n\n**Insight 2: Cultural Dimensions of Recommendation Letter Interpretation**\nWhile other reviewers may dismiss recommendation letters, Ryan uniquely identifies a *specific cultural explanation*: African contexts (particularly South African) emphasize interpersonal skills in academic recommendations more than Anglophone Western traditions that focus on individual achievement. This makes \"reading between the lines\" not just subjective but potentially stereotyping. This crosses into equity and cultural competency dimensions that go beyond typical evaluation critiques.\n\n**Insight 3: Application Form Structure Creates Disconnection**\nRyan identifies that OMT's online form structure (fill sequential boxes, close window, move to next question) inherently fragments the applicant's narrative and makes holistic thinking invisible to reviewers. This is a systemic design issue rather than an applicant quality issue. Few reviewers explicitly connect form design to evaluation difficulty. This suggests a technical/UX intervention point.\n\n**Insight 4: Applicant Burden of Translation in Specialist Fields**\nRyan uniquely frames disciplinary context-provision not as reviewer convenience but as *applicant fairness*. Specialists in niche fields must use scarce word count explaining fundamentals to generalist reviewers, disadvantaging them compared to applicants working in popular/accessible fields. This reframes the \"context assistance\" need as an equity issue. Most reviewers frame similar issues as personal cognitive load; Ryan frames it as fairness to applicants.\n\n**Insight 5: Self-Awareness About Own Bias-Reduction Strategy**\nRyan explicitly articulates using the rubric as a *tool to counteract personal bias* rather than just a compliance checklist. He acknowledges that theoretical topics he finds personally interesting might bias him upward, and uses rubric scoring as an objective brake on that tendency. This sophisticated approach to evaluation integrity is unusual; most reviewers describe rubric use more mechanically.\n\n**Insight 6: Comparative Calibration Within Cycles**\nRyan describes actively adjusting scores after reviewing other applications in the same cycle to maintain fairness\u2014catching instances where",
    "tokens_in": 9488,
    "tokens_out": 4096,
    "skipped": false
  },
  {
    "name": "Frasia Oosthuizen",
    "date": "2026_01_27 10_56 SAST",
    "filename": "_OMT Discovery Interview with StrideShift (Frasia Oosthuizen) \u2013 2026_01_27 10_56 SAST \u2013 Notes by Gemini.docx",
    "text_length": 5389,
    "analysis": "# OMT Discovery Interview Analysis: Frasia Oosthuizen\n\n## 1. Interviewee Role & Background\n\n**Role in OMT Review Process:**\n- Peer reviewer for OMT postgraduate scholarship applications\n- Reviews applications across health sciences disciplines (not limited to pharmacy despite background)\n- Evaluates applications at multiple levels: Masters, PhD, Postdoc, and Sabbatical categories\n\n**Duration of Involvement:**\n- Long-standing reviewer, though exact duration unclear to her (\"I honestly can't remember\")\n- Has been reviewing since before the current online system was implemented\n- Notes the previous system involved paper submissions rather than the current two-page format with integrated rubric\n\n**Discipline/Expertise:**\n- Registered pharmacist (required to maintain registration as she teaches in pharmacy program)\n- Master's degree and PhD in Pharmacology\n- Associate Professor in Pharmaceutical Sciences\n- Teaches both undergraduate and postgraduate students\n- Has extensive academic experience spanning many years\n- Reviews across health sciences broadly, not limited to her pharmacology background\n\n---\n\n## 2. Current Review Process Description\n\n**Overall Approach:**\n- Reviews applications in categorical batches (Masters together, PhDs together, Postdocs together, Sabbaticals together) to maintain objectivity and comparative consistency\n- Attempts to complete each category in a single day to maintain continuity and recall across candidates\n- Works through one complete application at a time, not scanning multiple candidates first\n\n**Step-by-Step Process:**\n1. **Read the full application first** - reads motivation letter, proposal, academic history, and all supporting materials before consulting the rubric\n2. **Then applies the rubric** - only after forming initial impressions\n3. **Takes detailed handwritten notes** - maintains notes throughout the process for reference and recalibration\n4. **Compares within batch** - if scoring a fifth candidate, may flip back to reconsider scores of earlier candidates\n5. **Recalibration** - explicitly adjusts previous scores if new candidates reveal that earlier scores were disproportionate (\"I gave this one a four but really then that one shouldn't have been a four. It should have been a three\")\n\n**Reading Process Sequence:**\n- Motivation letter first (to understand \"who this candidate is\")\n- Prior academic performance and marks\n- Proposal/study design\n- References/academic history\n- Photo (aids visual recall)\n- Referee reports (reviewed cautiously, not allowed to \"sway\" her too much)\n- Budget information\n\n**Tools/Systems Currently Used:**\n- Online application system with dual-screen capability\n- Two-page format with rubric displayed on one side, application on the other\n- Rubric integrated into the platform\n- Makes handwritten notes alongside digital review\n\n**Time Investment:**\n- \"A good few solid hours of work\" to get through an entire batch\n- Attempts to complete Masters applications in a single day\n- May require brief review period if not completed in one sitting to refresh memory\n\n---\n\n## 3. Pain Points & Challenges\n\n**Problem 1: Missing Academic Marks**\n- Applicants sometimes don't have final marks available by submission deadline (e.g., just completed exam sessions)\n- Current rubric requires all fields to be completed, which pulls the overall score up or down despite missing information\n- **Issue:** Applicants are penalized through no fault of their own\n- **Current workaround:** Frasia acknowledges the problem but appears to work around it inconsistently\n- **Her suggestion:** Option to exclude missing marks so they don't artificially affect the score\n\n**Problem 2: Rubric Interpretation Inconsistency**\n- Rubric language is ambiguous and open to interpretation\n- Example: Referee report criteria explicitly state need for \"exceptional\" language, but referees rarely use that exact terminology\n- Frasia has \"ignored it but consistently in a group\" - meaning she adapts the rubric interpretation within each batch but acknowledges this may differ from other reviewers\n- Some rubric questions are \"not possible to answer\" for certain application types (mentions marks for Masters going into PhD programs)\n- **Recognition:** She understands this is inherent to qualitative assessment, not necessarily a flaw, but it exists\n\n**Problem 3: Referee Report Rubric Criteria**\n- The rubric's expectation for referee reports is \"very explicitly stated\" but misaligned with actual referee language\n- Requires language like \"candidate is exceptional\" or \"academic standing is exceptional\" to score a 4\n- Frasia writes referee reports frequently and notes this explicit language is rarely used in practice\n- Creates a disconnect between what the rubric expects and what referees actually write\n- **Impact:** Forces reviewers to make judgment calls about equivalence that may not align across reviewers\n\n**Problem 4: Scope/Scale of Applications**\n- Implicit challenge: large number of applications to process\n- Masters applications noted as particularly voluminous (\"they tend to be a bigger pool\")\n\n**Problem 5: Consistency Between Reviewers**\n- Different reviewers may interpret the rubric differently\n- Frasia recognizes this may result in inconsistent standards across reviewers, though she prioritizes internal consistency within her own batch\n- Acknowledges this as inherent to qualitative, non-quantitative assessment\n\n**Problem 6: Limited Context for Specialized Fields**\n- When reviewing outside her immediate expertise, sometimes encounters submissions requiring background research\n- May need to \"look things up\" to understand field-specific context\n- No systematic provision of contextual information for reviewers less familiar with specialized areas\n\n**What She Doesn't Find Problematic:**\n- The current dual-screen system\n- The overall information provided in applications (finds it \"exactly what I need\")\n- The structure of the application itself\n- Individual components (motivation, proposal, budget, etc.) - all feed into the overall judgment\n- Nothing feels \"outstanding\" or unnecessarily difficult apart from the issues noted above\n\n---\n\n## 4. What She Values in Applications\n\n**Key Criteria & What She Looks For:**\n\n**1. Alignment Across Application Components**\n- **Most Important Judgment Factor:** Whether motivation letter, proposal, prior performance, and referee reports all \"tell the same story\"\n- Tests if everything \"makes sense from the motivation through to the referee reports\"\n- Uses this \"big picture\" coherence to validate her scoring confidence\n- If components align, she feels \"more confident that I have an understanding of who this candidate is and that my mark...is a better reflection\"\n\n**2. Motivation & Purpose**\n- Reads motivation letter first to establish \"who this candidate is\"\n- Evaluates if the candidate's stated goals are realistic and aligned with their project\n- Example of misalignment: \"candidate wants to cure cancer, but the project is so basic, there's a total disalignment\"\n- Assesses if motivation reveals genuine commitment vs. \"spin\"\n\n**3. Proposal Quality & Feasibility**\n- Reviews if the research proposal is achievable and appropriate in scope\n- Checks if proposal aligns with candidate's stated motivations and career goals\n- Looks for realistic project design given the candidate's level (Masters vs. PhD, etc.)\n- Evaluates contribution to society/field\n\n**4. Prior Academic Performance**\n- Considers marks and degree classifications (distinction, merit, pass)\n- Values consistency of performance over time\n- This is important context but she notes it's largely factual (not requiring interpretation)\n\n**5. Funding/Financial Commitment**\n- Wants to understand if candidate has sourced funding or applied for other bursaries\n- Interprets this as indicator of how much \"they want this degree\"\n- Views financial commitment as signal of genuine investment in the degree\n\n**6. Referee Reports**\n- Values reference reports highly but with caveats\n- Notes they are \"usually overwhelmingly positive\" \n- **Cautious approach:** \"I'm cautious not to let it sway me too much\"\n- Uses them as validation check rather than primary decision-maker\n- Important for triangulation (\"if everything...through to the referee reports...makes sense\")\n\n**Red Flags She Watches For:**\n\n1. **Misalignment between components:**\n   - Motivation doesn't match proposal\n   - Prior performance doesn't justify proposal ambition\n   - Referee reports conflict with application materials\n\n2. **Overstated claims:**\n   - Motivation that \"spins a lot of very nice things\" but proposal doesn't align\n   - Goals that exceed realistic project scope\n\n3. **Inconsistency in quality:**\n   - Excellent prior marks but weak proposal\n   - Weak prior performance but exceptionally positive references\n\n4. **Disconnection between aspiration and reality:**\n   - Grandiose goals with basic methodology\n   - Projects that don't reflect stated values or direction\n\n**What Makes an Application Stand Out:**\n\n1. **Clear, coherent narrative** across all components\n2. **Realistic ambition** - stretch goals that are achievable, not fantasy\n3. **Demonstrated commitment** - funding sourced, applications to multiple bursaries\n4. **Quality of writing** - in motivation letter and proposal\n5. **Evidence of genuine passion** - motivation that reads as authentic, not generic\n6. **Visual presentation** - notes the photo helps her recall and understand the candidate\n\n**How She Weighs Different Factors:**\n\n- **Hierarchical/Holistic:** She integrates factors into a \"big picture\" assessment rather than using strict weighting\n- **Alignment as the primary lens:** All factors are evaluated for consistency and coherence\n- **Within-batch comparison:** Relative to other candidates in the same category, not absolute standards\n- **Subjective integration:** Acknowledges all of this is \"very subjective\" but maintains internal consistency within each batch\n- **Quality of evidence:** What is explicitly stated (marks, funding status) is less interpretively demanding than alignment and narrative coherence\n\n---\n\n## 5. Views on AI/Technology\n\n**Overall Attitude: Cautious, Protective of Judgment**\n\nFrasia is not opposed to AI but has significant concerns about how it might be implemented. Her stance is sophisticated: she sees value in AI for certain technical tasks but is protective of her role in making judgment calls.\n\n**What She Would NOT Want Automated:**\n\n1. **Pre-analysis or flagging of inconsistencies:**\n   - Explicitly rejected the idea of AI identifying misalignments for her to validate\n   - Reasoning: \"If that person tells me what the red flags are, I already have decided in my mind no this is not going to work. So I'm really not objective.\"\n   - Concern: Pre-identified red flags create unconscious bias (\"you don't let the candidate speak\")\n   - \"I prefer getting that picture myself and make my own judgment call\"\n   - **Core principle:** She wants to form her own complete impression independently\n\n2. **Pre-summary or interpretation of materials:**\n   - Doesn't want someone else to have \"already created that picture\"\n   - Fears reviewer would come in with \"that person's preconceived judgment\"\n   - Values the unmediated reading of the application to avoid anchoring bias\n\n3. **Anything that structures her judgment prematurely:**\n   - Views independent reading as essential to objectivity\n   - Explicitly states: \"I don't start with the referee reports because they are usually overwhelmingly positive. If you start with something that has already pulled out negatives or positives you don't let the candidate speak.\"\n\n**What She WOULD Want Automated/Supported:**\n\n1. **Factual data extraction & pre-filling:**\n   - Academic marks and performance classifications (distinction, merit, pass)\n   - Funding status (has/hasn't sourced funding)\n   - Reasoning: These require no interpretation - \"anyone can look at a mark sheet\"\n   - \"I don't need to indicate that\"\n   - Would reduce \"superfluous cognitive load\"\n\n2. **Contextual background information:**\n   - \"Layman's abstract\" - simplified but not dumbed-down explanation of technical proposals\n   - Field-specific context for proposals outside reviewer's expertise\n   - Helps her understand specialized proposals without requiring research\n   - She framed this as helping her evaluate writing skills too (\"that also and that might also contribute to how we can see these applicants right their writing skills\")\n\n3. **Rubric clarity/standardization:**\n   - Better definition of terms (e.g., what constitutes \"exceptional\" in referee reports)\n   - Optional fields for missing information (marks not yet available)\n   - Clearer guidance on field-specific questions that might not apply to all applicants\n\n**Technology Concerns:**\n\n1. **Loss of objectivity through anchoring:**\n   - Critical concern: Pre-structured information could unconsciously bias her judgment\n   - Based on cognitive science understanding: Starting with negatives/positives predetermines evaluation\n   - This is about psychological safety and decision-making integrity\n\n2. **AI-generated motivation letters:**\n   - Notes that motivations \"can probably be AI'd\" - concerned that this changes what the motivation letter reveals about the candidate\n   - Implies awareness that AI could make it harder to discern authentic motivation\n\n3. **Consistency of interpretation:**\n   - Accepts that different reviewers will interpret qualitative rubrics differently\n   - Not opposed to this inherent variability but values her internal consistency\n   - Concerned about over-standardization that might mask legitimate judgment differences\n\n**Trust & Transparency Requirements:**\n\n- Wants to understand and validate any AI-assisted process\n- Would need clear explanation of what AI is doing and why\n- Appears to require that AI not substitute for her judgment on nuanced matters\n- Implicit: would need transparency about how recommendations are generated\n- Important that AI serves as tool, not authority (\"validate it and see if this aligns with your thinking\" - she wants validation role, not acceptance of AI judgment)\n\n**Motivation for Accepting AI Support:**\n\n- Not motivated by workload reduction (she finds the work \"rewarding\")\n- Motivated by enabling OMT to \"handle more scale\"\n- Motivated by potentially improving consistency or clarity of process\n- Interested in supporting other reviewers (helping them access contextual information)\n\n---\n\n## 6. Suggestions & Ideas\n\n**Explicit Suggestions for Improvement:**\n\n1. **Rubric Flexibility for Missing Data:**\n   - Add option to exclude marks that haven't been released yet\n   - These shouldn't penalize candidates when data is simply unavailable at submission time\n   - Allows the rubric to generate accurate scores despite timing issues\n\n2. **Referee Report Criteria Refinement:**\n   - Review the language used in referee report expectations (\"exceptional,\" \"academic standing,\" etc.)\n   - May need to add more lenient interpretation guidelines\n   - Consider what language referees actually use in practice and align rubric expectations accordingly\n   - Possibly allow for equivalent language that conveys same meaning without exact words\n\n3. **Layman's Abstract Requirement:**\n   - Request that applicants provide simplified (but not \"dumbed down\") explanation of technical proposals\n   - Not replacing the scientific proposal, but accompanying it\n   - Benefits: helps reviewers understand specialized fields, assesses candidate's ability to communicate across audiences\n   - Format suggestion: \"language that is more easy to understand for everyone that's not necessarily a biologist or a bioineticist\"\n\n4. **Contextual Information for Specialized Fields:**\n   - Provide background context when proposals fall outside standard disciplinary knowledge\n   - Could be brief field-specific primers or terminology guides\n   - Reduces need for reviewers to conduct external research\n\n5. **Pre-population of Factual Data:**\n   - Auto-populate marks/grade classifications from academic records\n   - Auto-populate funding status from application form data\n   - Reduces superfluous cognitive load for factual verification\n\n**Implicit Improvements Suggested:**\n\n6. **Application Sequence/Structure:**\n   - Current structure works well for her (motivation \u2192 proposal \u2192 prior performance \u2192 refs)\n   - Would maintain this as it supports her judgment process\n\n7. **Rubric Clarity:**\n   - Some questions in rubric are unanswerable for certain application types\n   - Need review of question relevance across different scholarship categories\n   - Acknowledge that not all questions apply equally to Masters vs. PhD vs. Postdoc vs. Sabbatical\n\n8. **Batch Grouping System:**\n   - Current system of grouping by type and attempting same-day review works well\n   - Supports consistency within reviewer judgments\n\n9. **Dual-Screen Interface:**\n   - Keep and maintain current two-screen split format (application + rubric)\n   - Already supporting her workflow effectively\n\n**Priorities for Change (Inferred from Emphasis):**\n\n1. **Highest Priority:** Rubric criteria alignment with practice (referee reports, missing marks)\n2. **High Priority:** Reducing superfluous cognitive load (data pre-population)\n3. **Medium Priority:** Supporting understanding of specialized fields (context, abstracts)\n4. **Lower Priority:** Changes to overall structure or interface (current system works well)\n\n**What She Explicitly Would NOT Change:**\n\n- The information required in applications (\"I've always found it exactly what I need\")\n- Any of the major components (motivation, proposal, refs, budget, prior performance)\n- The basic structure of the review process\n- The requirement for independent judgment of alignment\n\n---\n\n## 7. Key Direct Quotes\n\n### Quote 1: On Finding Meaning in Reviews\n**Context:** Discussion of why she continues to review despite significant time commitment\n\n> \"It's amazing to see what some young people are achieving in their lives. So it's I've actually thought it was a very nice experience. So I'm actually always glad that they asked me.\"\n\n**Significance:** Reveals intrinsic motivation for participation; this is not a burdensome task for her but genuinely rewarding work.\n\n---\n\n### Quote 2: On Alignment as Core Judgment Criterion\n**Context:** Explaining her primary framework for evaluation\n\n> \"I try and create a picture of this applicant...everything if the big picture makes sense from the motivation through to the referee reports, I feel more confident that I have an understanding of who this candidate is and that my mark whether good or bad is a better reflection.\"\n\n**Significance:** Reveals her holistic, coherence-based approach; alignment is THE central",
    "tokens_in": 9211,
    "tokens_out": 4096,
    "skipped": false
  }
]