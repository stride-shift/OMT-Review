# OMT Discovery Interview Analysis: Comprehensive Thematic Report

## 1. Executive Summary

This comprehensive analysis synthesizes 11 discovery interviews conducted with Oppenheimer Memorial Trust (OMT) review committee members between January 12-27, 2026. The interviews reveal a review process characterized by dedicated, thoughtful assessors who balance structured rubric-based evaluation with nuanced human judgment, while navigating significant tensions between standardization and contextual sensitivity.

**Key Findings:**

The review process operates as a **hybrid system** where formal rubrics provide structure but frequently prove inadequate for capturing the complexity of candidate assessment. Reviewers consistently describe a pattern of "retrofitting"—completing rubric scoring, then adjusting scores to align with their holistic judgment. This gap between mechanical scoring and authentic assessment represents the central tension in the current system.

**Critical Pain Points:**

1. **The Rubric Problem**: The 1-4 scoring scale lacks sufficient granularity, with reviewers struggling particularly to distinguish between scores of 3 and 4. Multiple reviewers described the scale as "blunt," "too coarse," and insufficient for meaningful differentiation, especially among high-performing candidates.

2. **AI-Generated Content Crisis**: A growing concern across reviewers is the inability to detect AI-generated motivation letters and proposals, undermining the validity of written assessments. This threatens one of the primary mechanisms for understanding authentic candidate voice and motivation.

3. **The Cold Start Problem**: Reviewers struggle with initial applications in each cycle, lacking reference points for calibration. This creates potential unfairness for candidates reviewed early in the process.

4. **Inconsistent Interpretation**: Despite standardized rubrics, significant variation exists in how criteria are interpreted across reviewers, with no formal calibration mechanism to ensure consistency.

**Consensus Areas:**

- **Portfolio/practical work** (for creative disciplines) and **academic records** provide the most reliable objective assessment data
- **Motivation and personal narrative** matter deeply but are increasingly compromised by AI generation
- **Alignment and coherence** across application components serves as a crucial validation mechanism
- **Context matters**: Reviewers universally attempt to assess "excellence relative to circumstance," though struggle with how to systematize this
- **Human judgment is irreplaceable** for nuanced assessment, particularly for motivation, fit, and probability of success

**Strategic Implications:**

The interviews reveal an assessment system under pressure from multiple directions: increasing application volumes, technological disruption (AI-generated content), disciplinary diversity requiring specialized knowledge, and inherent tensions between standardization (for fairness) and flexibility (for accuracy). 

Most critically, **reviewers do not want AI to replace their judgment**—they want it to reduce "superfluous cognitive load" on administrative tasks while preserving their role in nuanced evaluation. There is strong consensus that AI should handle: (1) initial screening for completeness and basic eligibility, (2) factual data compilation and verification, (3) providing disciplinary context for specialized fields, and (4) flagging potential inconsistencies. However, AI must not: (1) make final recommendations, (2) pre-structure judgment through summaries that create anchoring bias, (3) assess motivation or fit, or (4) replace human evaluation of probability of success.

The path forward requires **thoughtful augmentation rather than automation**—supporting reviewer judgment while addressing legitimate efficiency and consistency concerns. The system's future success depends on preserving what makes OMT distinctive (holistic, mission-driven candidate assessment) while addressing structural limitations that create reviewer burden and potential unfairness to applicants.

---

## 2. Methodology

**Study Design:**
- 11 substantive semi-structured discovery interviews conducted January 12-27, 2026
- Interviews led by StrideShift (Justin Germishuys and Barbara Dale-Jones) with OMT review committee members
- Duration: 30-60 minutes per interview
- Confidentiality assured to participants; transcripts not shared with OMT

**Data Collection:**
- Real-time transcription via Google Gemini
- Interviews covered: current review workflow, evaluation criteria, pain points, views on technology/AI, suggestions for improvement
- Open-ended questioning allowing emergent themes
- Probing for specific examples, edge cases, and tacit knowledge

**Data Analysis:**
- Iterative thematic analysis with AI-assisted coding
- Individual interview extraction performed using Claude Haiku
- Cross-interview synthesis performed using Claude Sonnet
- Manual validation and refinement of AI-identified themes
- Quote verification against raw transcripts
- Pattern identification across disciplinary boundaries

**Sample Characteristics:**
- Reviewers span humanities, sciences, engineering, business, and creative arts
- Experience ranges from 2-3 years to long-standing involvement
- Includes reviewers at different institutional contexts and career stages
- Mix of disciplinary specialists and those reviewing across fields

**Limitations:**
- 4 additional recordings contained scheduling/internal discussions and were excluded from analysis
- Self-reported processes may not fully capture unconscious decision-making
- Sample limited to active reviewers (does not include those who declined or stopped reviewing)
- Timing (January 2026) means recent process changes may not be fully reflected

---

## 3. Participant Profiles

| **Name** | **Institution** | **Discipline** | **Years Reviewing** | **Application Types Reviewed** |
|----------|----------------|----------------|---------------------|-------------------------------|
| **Dina Ligaga** | Wits University (VIT context mentioned) | Media and Cultural Studies, Literature, Gender Studies | ~4 years (since 2022) | Humanities; Masters and PhD levels |
| **Edzai Zvobwo** | Own analytics/AI company | Science and Mathematics (broad spectrum) | ~5-7 years (since pre-COVID, ~2019) | Natural sciences; full spectrum of levels |
| **Freedom Gumedze** | University of Cape Town | Biostatistics, Statistical Sciences | ~3-4 years | Statistics/data science; Masters, PhD, Postdoc, Sabbatical |
| **Frelet De Villiers** | University of the Free State | Music (Creative Arts) | ~4-5 years | Creative arts (primarily fine art, also performance, graphic design); postgraduate levels |
| **Martin Clark** | University of the Free State | Geology/Earth Sciences | ~3-4 years | Earth sciences broadly; Masters, PhD, Postdoc/Sabbatical |
| **Maureen De Jager** | Rhodes University | Fine Art (practice-based research) | ~3 years | Creative arts (fine art focus); Masters and PhD |
| **Mohamed Cassim** | Africa Climate Ventures (Co-founder) | Development Economics, Strategy, Public/Private Sector | Unspecified duration | Financial applications (notes feels pigeonholed); postgraduate levels |
| **Philippe Burger** | University of the Free State | Economics, Macroeconomics | ~2-3 years | Business management and economics; Masters, PhD, Scholar levels |
| **Pieter Pistorius** | University of Pretoria | Metallurgical Engineering | ~3 years | Engineering; postgraduate levels including sabbaticals |
| **Ryan Nefdt** | University of Cape Town | Philosophy, Linguistics | Ongoing (multiple cycles) | Humanities (philosophy, fine art, sociology, linguistics, AI); postgraduate levels |
| **Frasia Oosthuizen** | Pharmaceutical Sciences (institution not specified) | Pharmacology, Pharmacy | Long-standing (pre-online system) | Health sciences broadly; Masters, PhD, Postdoc, Sabbatical |

---

## 4. Detailed Thematic Analysis

### 4.1 The Review Workflow: From Application to Decision

#### Common Workflow Patterns

Despite disciplinary differences, reviewers converge on a **three-phase workflow structure**:

**Phase 1: Initial Orientation** (5-30 minutes)
- Most reviewers begin with an **overview scan** to understand the applicant pool
- Some read applications fully before applying rubric; others work simultaneously
- Dina Ligaga: Opens application and reviews "candidate's background, motivation, proposed study, CV, transcripts" before detailed scoring (Ligaga, Jan 12)
- Maureen De Jager: "Skims through all applications to get an overall lay of the land" before detailed assessment (De Jager, Jan 22)
- Pieter Pistorius: Conducts "quick five-minute scan of the applicants to see who they are" before deep review (Pistorius, Jan 19)

**Phase 2: Detailed Document Review** (30-60 minutes per application)
- **Reading sequence varies** but most follow: motivation letter → academic record → proposal → references → budget
- Ryan Nefdt: "First examines the background section, looking for a cohesive narrative that connects the applicant's life story to their studies and academic goals" (Nefdt, Jan 21)
- Freedom Gumedze: Reviews "core content" including what proposal is about, whether applicant has sufficient background, institutional acceptance status (Gumedze, Jan 26)
- Frasia Oosthuizen: Reads "motivation letter first (to understand 'who this candidate is'), then prior academic performance and marks, then proposal/study design, then references/academic history, then photo, then referee reports, then budget information" (Oosthuizen, Jan 27)

**Note-Taking Practices:**
- Extensive handwritten notes common (Ligaga, Pistorius)
- Notes often not fully translated to final written assessments
- Dina Ligaga: Takes "extensive notes in a notebook during review" but struggles with "converting notes to final written form" (Ligaga, Jan 12)

**Phase 3: Scoring and Calibration** (20-45 minutes post-review)
- **Iterative scoring**: Initial scores assigned, then adjusted based on comparative context
- Maureen De Jager: "Frequently returns to adjust final scores if calculated rating doesn't match strength of submission" (De Jager, Jan 22)
- Philippe Burger: "Creates provisional ranking that is adjusted as more candidates are reviewed; revisits rankings after reviewing 5-6 candidates" (Burger, Jan 13)
- Pieter Pistorius: "Reviews overall ratings after completing all applications; looks for outliers and halo effects" (Pistorius, Jan 19)

#### Individual Variations and Why They Exist

**Disciplinary Adaptations:**

**Creative Arts Reviewers** (De Villiers, De Jager):
- Portfolio examination is central and time-intensive
- Must contextualize visual/performance work requiring specialized interpretation
- Maureen De Jager: Portfolio review is "absolutely critical" and "highly detrimental if absent" for creative degrees (De Jager, Jan 22)

**STEM Reviewers** (Zvobwo, Gumedze, Clark):
- Focus on publication metrics, research standing, methodological rigor
- Freedom Gumedze: Uses external sources (Google Scholar, Scopus, bibliometrics) to verify research quality (Gumedze, Jan 26)
- May need to research specialized topics outside immediate expertise

**Interdisciplinary Reviewers** (Nefdt, Cassim):
- Must navigate multiple disciplinary standards
- Ryan Nefdt: Reviews across "fine art, sociology, linguistics, AI" requiring understanding of "different conduits to societal impact" (Nefdt, Jan 21)

**Batch Management Strategies:**
- Frasia Oosthuizen: Groups by level (Masters together, PhDs together) and attempts to complete each category in single day for consistency (Oosthuizen, Jan 27)
- Martin Clark: "Allocates time carefully to avoid adjudication fatigue, typically reading no more than 5-10 applications in one sitting" (Clark, Jan 15)
- Edzai Zvobwo: Works through multiple evenings, taking breaks to avoid "mind-numbing" effects (Zvobwo, Jan 13)

**Rubric-First vs. Read-First Approaches:**

Some apply rubric during reading (Nefdt, Burger), others read fully first then apply rubric (De Jager, Oosthuizen):
- Ryan Nefdt: "Reads submission and rubric simultaneously when time-constrained" but prefers reading "once independently without predisposing it to rubric categories" (Nefdt, Jan 21)
- Frasia Oosthuizen: "Reads the full application first...only after forming initial impressions" applies rubric (Oosthuizen, Jan 27)

#### Time Investment and Workload

**Per-Application Time:**
- **Masters applications**: 30-45 minutes (Ligaga, Pistorius)
- **PhD/Postdoc applications**: 45 minutes to 1 hour (Pistorius, Gumedze)
- **Sabbatical applications**: Variable, often less structured requiring more interpretation

**Total Time Commitment:**
- Pieter Pistorius: "2-3 evenings total" within 2-week turnaround (Pistorius, Jan 19)
- Frasia Oosthuizen: "A good few solid hours of work" to complete entire batch (Oosthuizen, Jan 27)
- Martin Clark: Emphasizes need to avoid "adjudication fatigue" by limiting consecutive reviews (Clark, Jan 15)

**Workload Context:**
- Dina Ligaga: Review occurs "during busy period when also marking exams" (Ligaga, Jan 12)
- Most reviewers note this is **volunteer work** conducted alongside primary employment
- Pieter Pistorius: "It is a lot of hours that you burn but you know it's once a year so it's not a catastrophe" (Pistorius, Jan 19)

#### Tools and Systems Currently Used

**OMT Platform:**
- Web-based system with **dual-screen capability** praised by most reviewers
- Frelet De Villiers: "The Oppenheimer's website is really very user friendly because you can have the split screen and everything is really available" compared favorably to NRF system (De Villiers, Jan 26)
- Frasia Oosthuizen: "Two-page format with rubric displayed on one side, application on the other" works effectively (Oosthuizen, Jan 27)

**Previous System Issues:**
- Dina Ligaga: "Excel sheets with comparative review process (no longer in use)" was more cumbersome (Ligaga, Jan 12)
- System has improved but **navigation remains challenging** for some

**External Tools:**
- Freedom Gumedze: Uses "Google Scholar, Scopus, bibliometrics systems, NRF rating information" to verify research quality (Gumedze, Jan 26)
- Mohamed Cassim: Manually verifies "institutional performance and financial statements" to validate CV claims (Cassim, Jan 20)
- Reviewers frequently use Google for discipline-specific terminology (Nefdt, De Villiers)

**Personal Tools:**
- Handwritten notes (Ligaga, Pistorius, De Jager)
- Timeline diagrams to track academic progression (Pistorius)
- Spreadsheets for comparative tracking (historical, no longer used)

#### How Scoring Works (Rubric Mechanics)

**Scale Structure:**
- Primary scale: **1-4 point system** (1=fail/poor, 2=fair, 3=good, 4=excellent)
- Some categories use **5-point final scale** for overall assessment
- Dina Ligaga: "Uses rubric with 4-point scale...for tick-box scoring" followed by "final judgment call on recommendation status" (Ligaga, Jan 12)

**Scoring Process:**
- Reviewers complete rubric section by section
- Each criterion scored independently, then synthesized
- Philippe Burger: Uses "3 as average baseline" with adjustments for above/below average performance (Burger, Jan 13)

**The "Retrofit" Pattern:**
- **Critical finding**: Many reviewers adjust rubric scores to align with holistic judgment
- Dina Ligaga: "Sometimes adjusts rubric scores to align with overall intuitive assessment...marks up or down scores to match overall conviction" (Ligaga, Jan 12)
- Maureen De Jager: Uses initial impression "to override mechanical scoring...described as 'retrofit'—tweaking scores to reflect actual quality assessment" (De Jager, Jan 22)
- This suggests **rubric serves as structured justification** for pre-existing holistic judgment rather than primary decision mechanism

**Comparative Calibration:**
- Ryan Nefdt: "Reviews across the application pool within a cycle and adjusts scores to ensure consistency and fairness" (Nefdt, Jan 21)
- Philippe Burger: "Reads through 10-12 applications before settling on final scores to establish relative benchmarking context" (Burger, Jan 13)
- Frasia Oosthuizen: "Explicitly adjusts previous scores if new candidates reveal that earlier scores were disproportionate" (Oosthuizen, Jan 27)

---

### 4.2 The Rubric Problem: Constraints and Workarounds

The rubric emerged as the **single most discussed frustration** across all interviews, revealing fundamental tensions between structured evaluation and authentic assessment.

#### How the 1-4 Scoring System Works and Its Limitations

**Scale Definition:**
- 1 = Fail/Does not meet criteria
- 2 = Poor/Marginally meets criteria  
- 3 = Good/Meets criteria satisfactorily
- 4 = Excellent/Exceeds criteria

**The "Fuzziness" Between 3 and 4:**

Dina Ligaga: "There's a **fuzziness between scores 3 and 4**—the distinction is unclear and subjective...only 4-point scale doesn't capture nuance of excellence" (Ligaga, Jan 12)

This "fuzzy zone" represents the critical differentiation point where most competitive candidates cluster, yet the rubric provides no clear guidance:

Philippe Burger: "Five points isn't always enough to discriminate one against the other...you look for that little extra that would carry it from a three to a four or four to a five" (Burger, Jan 13)

Edzai Zvobwo: "Numbers become 'fuzzy'—reviewers lose clarity on what distinguishes a 3 from a 4...lack of clear boundaries in scoring scale...consistency deteriorates as fatigue sets in" (Zvobwo, Jan 13)

**Insufficient Gradation Problem:**

The scale lacks resolution for **meaningful differentiation among strong candidates**:

Dina Ligaga: "Cannot distinguish between 75% excellent and 90% excellent...feels overly rigid and doesn't capture complexity of judgment" (Ligaga, Jan 12)

Edzai Zvobwo: "0-4 scale has bins that are 'too big'...'high quality three' and 'low quality three' bunched together indistinguishably...cannot adequately differentiate within quartiles" (Zvobwo, Jan 13)

Martin Clark: "4-point rubric scale (potentially divided into 25% increments) insufficient to distinguish between candidates at similar levels (e.g., difference between 75% and 100%)" (Clark, Jan 15)

**Level-Specific Problems:**

Different degree levels require different standards, but rubric doesn't adapt:

Freedom Gumedze: "The master's programs are changing. The rubric is built on a master's by dissertation...if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part..." (Gumedze, Jan 26)

- **Masters applications**: High volume makes fine differentiation crucial yet scale is too coarse
- **PhD applications**: Require demonstration of mastery, but rubric doesn't capture sophistication adequately
- **Postdoc/Sabbatical**: Different rubrics exist but still constrained

#### Workarounds Reviewers Have Developed

**1. The "Retrofit" Strategy:**

Reviewers complete mechanical scoring, then adjust to match authentic assessment:

Maureen De Jager: "Frequently returns to adjust final scores if calculated rating doesn't match strength of submission...uses initial impression (often reinforced through detailed review) to override mechanical scoring" (De Jager, Jan 22)

Dina Ligaga: Describes "phase as losing confidence—worries about whether scoring is accurate...rubric process 'boxes them in' and 'slows them down'" (Ligaga, Jan 12)

**2. Starting High/Low to Counter Halo Effects:**

Martin Clark: Consciously forces "extreme scoring initially" to counteract halo effect bias toward applications in familiar research areas (Clark, Jan 15)

**3. Ignoring Problematic Criteria:**

Frasia Oosthuizen: "Has 'ignored it but consistently in a group'—meaning she adapts the rubric interpretation within each batch but acknowledges this may differ from other reviewers" (Oosthuizen, Jan 27)

Ryan Nefdt: When rubric criteria don't fit discipline (e.g., theoretical humanities work with unclear societal impact pathways), applies professional judgment outside rubric structure (Nefdt, Jan 21)

**4. Comparative Recalibration:**

Philippe Burger: "Creates provisional ranking that is adjusted as more candidates are reviewed; revisits rankings after reviewing 5-6 candidates to establish relative positions" (Burger, Jan 13)

Frasia Oosthuizen: "If scoring a fifth candidate, may flip back to reconsider scores of earlier candidates...'I gave this one a four but really then that one shouldn't have been a four. It should have been a three'" (Oosthuizen, Jan 27)

**5. Detailed Written Commentary:**

When rubric fails to capture nuance, reviewers rely on **narrative justification**:

Martin Clark: "Adds comments explaining nuanced decisions, particularly where perception of grade conflicts with rubric requirements" (Clark, Jan 15)

However, this creates its own problems:
- Dina Ligaga: "Written reports have grown progressively shorter—often only 1-2 lines after rubric completion" despite extensive note-taking (Ligaga, Jan 12)
- Uncertain whether lengthy commentary is valued by OMT decision-makers

#### The Gap Between Rubric Scores and Holistic Judgment

This represents the **core tension** in the review process:

**The Rubric Cannot Capture:**

1. **Tacit Knowledge and Pattern Recognition:**

Martin Clark: Brings "expert judgment, or 'tacet knowledge,' to the process, which involves recognizing patterns, such as a positive outlook for change" (Clark, Jan 15)

Edzai Zvobwo: Can "fairly quickly distinguish between 'blustering' and actual achievement" based on sector experience (Zvobwo, Jan 13)

2. **Contextual Excellence:**

Martin Clark: "Excellence can be measured whether it is from a lay person's perspective or from an absolute a disciplined leader...there are backgrounds and life paths that are different than my own and it does not mean that a different life path or a life path that appears to be less developmental or less of a straight line towards excellence should be precluded from excellence" (Clark, Jan 15)

Philippe Burger: Must evaluate "excellence relative to circumstance" considering socioeconomic background diversity, but rubric provides no framework for this (Burger, Jan 13)

3. **Alignment and Coherence:**

Frasia Oosthuizen: "Tests if everything 'makes sense from the motivation through to the referee reports'...if components align, she feels 'more confident that I have an understanding of who this candidate is and that my mark...is a better reflection'" (Oosthuizen, Jan 27)

This **holistic coherence** assessment is central to most reviewers' confidence in their ratings but isn't explicitly scored in rubric.

4. **Probability of Delivery vs. Merit of Intent:**

Mohamed Cassim: "It's not quite simply about the merit of the intent...everybody can write a brilliant piece of intent. It's also about the probability of that being delivered" (Cassim, Jan 20)

Rubric assesses proposal quality but not realistic capability to execute.

**Consequences of This Gap:**

1. **Subjectivity Despite Structure:**

Philippe Burger: "There's always...in that because you do a translation of what your impression into a score. Um so there will always be something...in there that's a bit subjective" (Burger, Jan 13)

2. **Inconsistency Across Reviewers:**

Freedom Gumedze: "I would like to know...do they take a sort of like combination of scores from the group...I want to know at the end what they decide so that maybe we can calibrate" (Gumedze, Jan 26)

Without knowing how other reviewers interpret criteria, consistency cannot be ensured.

3. **Decision Uncertainty:**

Dina Ligaga: Describes "negotiation and uncertainty" where reviewer must "perform mental gymnastics to align scores with actual conviction" (Ligaga, Jan 12)

#### Specific Suggestions for Improvement

**1. Expand Scoring Range (Highest Priority Across Multiple Reviewers):**

Dina Ligaga: "Current 4-point scale insufficient...should allow differentiation between '75% excellent' and '90% excellent'...particularly critical for Masters applications due to high volume" (Ligaga, Jan 12)

Edzai Zvobwo: "0-4 scale...bin size is too big...high quality three and a low quality three, they're all bunched up...scale feels too coarse-grained for nuanced assessment" (Zvobwo, Jan 13)

**2. Add Dimensionality to Capture Nuance:**

Dina Ligaga: Current rubric is "too 'blunt'...needs more gradations to capture the full range of high-performing candidates" (Ligaga, Jan 12)

**3. Rubric Flexibility for Missing Data:**

Frasia Oosthuizen: "Add option to exclude marks that haven't been released yet...these shouldn't penalize candidates when data is simply unavailable at submission time" (Oosthuizen, Jan 27)

**4. Discipline-Specific Rubric Variations:**

Freedom Gumedze: "Rubric categories could 'change based off of discipline or category'...adjust expectations according to field-specific norms...different emphasis for Master's, PhD, and postdoctoral applications" (Gumedze, Jan 26)

Martin Clark: "Rubric is 'rather rigid' and doesn't accommodate disciplinary differences...specific metrics (e.g., Golden Key Society membership) disadvantage otherwise excellent candidates who lack access to resources" (Clark, Jan 15)

**5. Clearer Scoring Guidance:**

Frasia Oosthuizen: "Referee report criteria explicitly state need for 'exceptional' language, but referees rarely use that exact terminology...creates disconnect between rubric expectations and actual referee practice" (Oosthuizen, Jan 27)

Ryan Nefdt: "P rating (future leader potential) should score higher than C rating...fewer people achieve P rating; should reflect difficulty" (Nefdt, Jan 21)

**6. Reframe Problematic Questions:**

Maureen De Jager: "Personal Motivation question asks for background/interests but rubric seeks evidence of 'values and priorities'...space constraints mean candidates often only cover background, not deeper values" (De Jager, Jan 22)

**Significance to South African Society** question particularly problematic:

Maureen De Jager: "Practice in its nature is exploratory...it's very difficult to know before you've even set out on this journey where you're going to end up...in a lot of cases it's a bit of a thumb suck" (De Jager, Jan 22)

---

### 4.3 What Makes an Excellent Application: Criteria and Judgment

#### Universal Criteria (What Everyone Looks For)

Despite disciplinary differences, reviewers converge on several **core universal criteria**:

**1. Alignment and Coherence Across Application Components**

This emerged as the **single most important universal criterion**:

Frasia Oosthuizen: "Whether motivation letter, proposal, prior performance, and referee reports all 'tell the same story'...tests if everything 'makes sense from the motivation through to the referee reports'...if components align, feels 'more confident that I have an understanding of who this candidate is'" (Oosthuizen, Jan 27)

Ryan Nefdt: Looks for "connections between life experience, academic development, and current work...personal story should directly inform and explain academic trajectory" (Nefdt, Jan 21)

Dina Ligaga: Examines "'string through' from applicant's background to proposed study...evidence of logical progression and connection between past work and future plans...shows sustained commitment to research area" (Ligaga, Jan 12)

**2. Authentic Motivation and Personal Narrative**

All reviewers prioritize genuine motivation, though increasingly challenged by AI:

Dina Ligaga: "Seeks genuine, unique, honest motivation...values sincere personal statement about influences and experiences...red flag: Generic motivation immediately signals applicant isn't serious" (Ligaga, Jan 12)

Frelet De Villiers: "When you look at the motivation you can see when they are just complaining...you get other people who say I come from the background but I did this with what I have already and I want to achieve this" (De Villiers, Jan 26)

Edzai Zvobwo: "Personal narrative has 'the biggest weighting' in final decisions...looks for evidence of resilience and overcoming obstacles...values applicants showing they 'came from a broken home and against all odds...overcome those obstacles'" (Zvobwo, Jan 13)

**3. Academic Excellence (With Contextual Considerations)**

All reviewers consider academic record, but with sophisticated nuance:

Philippe Burger: "Strong academic performance is first and most rigorous filter...marks must align with demands of intended institution and program...example: 78% average insufficient for top international universities" (Burger, Jan 13)

Frelet De Villiers: "Academic record is not that important to me because there are circumstances...students just have the most horrible here...or they have a lecturer that doesn't fancy them...I must say that is the least important for me" (De Villiers, Jan 26)

Pieter Pistorius: "Some come from absolutely middle class environments, some you can see that the clothes that they wear when they sit in front of you is the clothes that they have...you need to look at that...someone that comes from an impoverished neighborhood may be excellent or mediocre. I don't think our idea is to support a mediocre candidate but you need to look at the social context" (Pistorius, Jan 19)

**4. Feasibility and Realism of Proposed Work**

Universally valued across all disciplines:

Frelet De Villiers: "Is it really viable? Is it really something that can work? Are sometimes these people they say they are going to have four articles and three conferences and two books. I mean it's just not possible" (De Villiers, Jan 26)

Mohamed Cassim: "It's not quite simply about the merit of the intent...everybody can write a brilliant piece of intent. It's also about the probability of that being delivered" (Cassim, Jan 20)

Martin Clark: "Always examines proposed budgets for alignment with project scope...flags excessively high budgets that don't match project needs...flags extremely modest budgets that may indicate unfeasibility" (Clark, Jan 15)

**5. Evidence of Commitment and Investment**

Reviewers look for signals of genuine dedication:

Mohamed Cassim: "Strong believer that financial investment by candidate signals commitment...will comment if candidate hasn't invested own cash (when they have capacity to do so)" (Cassim, Jan 20)

Frasia Oosthuizen: "Wants to understand if candidate has sourced funding or applied for other bursaries...interprets this as indicator of how much 'they want this degree'...views financial commitment as signal of genuine investment" (Oosthuizen, Jan 27)

#### Discipline-Specific Criteria

**STEM Disciplines (Sciences, Engineering, Statistics):**

**Publication Quality and Research Standing** (PhD/Postdoc/Sabbatical):

Freedom Gumedze: "Depth of understanding of their own research quality...current research impact (not just ratings)...publication venues and quality (not just H-index)...active research networks and collaborations" (Gumedze, Jan 26)

**Methodological Rigor:**

Freedom Gumedze: For quantitative fields, "quantitative preparedness assessment...South African universities weak in quantitative preparation; flag candidates with only first/second-year statistics modules applying to highly quantitative programs" (Gumedze, Jan 26)

Philippe Burger: "For PhD applicants: mastery of field literature and existing knowledge base...disciplinary foundation—applicant's intended field of study must align with existing qualifications" (Burger, Jan 13)

**Innovation and Technical Sophistication:**

Freedom Gumedze: "Innovation & Novelty—uses expertise from editing peer-reviewed journals to assess innovation...looks for novel statistical/methodological approaches" (Gumedze, Jan 26)

**Humanities (Philosophy, Media Studies, Literature):**

**Articulated Societal Impact Pathways:**

Ryan Nefdt: "In the science you often like I'm going to publish in high impact journals and that kind of does the job for you...with humanities it's a little bit different. you have to figure out different conduits to getting some societal impact" (Nefdt, Jan 21)

However, theoretical work creates tension:

Ryan Nefdt: "If you are engaging in a theoretical project that is somewhat lacking in terms of your ability to express the societal impact but that theoretical project is so intricate and interesting...sometimes there are issues that are just not going to be able to be translated that well, but they're very intricate...if you're a puzzle master and you're working on an interesting puzzle, sometimes somebody else is going to realize the value of that puzzle" (Nefdt, Jan 21)

**Narrative Coherence:**

Ryan Nefdt: "Connections between life experience, academic development, and current work...example: Ryan's own multilingual upbringing directly led to linguistics interests" (Nefdt, Jan 21)

Dina Ligaga: "Evaluates believability and authenticity—distinguishes between performed vs. genuine motivation...wants to see clear thinking about research direction, not vague statements" (Ligaga, Jan 12)

**Creative Arts (Fine Art, Music, Performance):**

**Portfolio Excellence (Highest Priority):**

Maureen De Jager: "It's actually quite critical if one's looking at a student wanting to go into a creative degree a practice-based masters or PhD is their portfolio and that evidence of practical excellence...because you can talk things up and especially with AI you know it's very easy to write a convincing proposal for a research project...and then you're sort of wondering how much of that is really this the student