# OMT Discovery Interview Analysis: Comprehensive Thematic Report

## 1. Executive Summary

This comprehensive analysis of 11 discovery interviews with Oppenheimer Memorial Trust (OMT) review committee members reveals a sophisticated but strained evaluation system balancing human judgment with standardized assessment. The research uncovers significant tensions between the current 1-4 scoring rubric and reviewers' nuanced evaluative practices, with most reviewers developing elaborate workarounds to capture their holistic judgments within rigid scoring constraints.

Key findings indicate that reviewers value authenticity, narrative coherence, and societal impact above pure academic metrics, employing what Martin Clark termed "tacit knowledge" to assess applications. However, the review process faces mounting challenges from AI-generated content, cultural biases, and inconsistent application quality across institutions. Most notably, reviewers demonstrate sophisticated understanding of their own limitations while expressing cautious optimism about AI assistance for routine tasksâ€”provided human judgment remains central to final decisions.

The analysis reveals consensus around AI's potential for preliminary screening, data verification, and administrative support, but strong resistance to automated decision-making. Reviewers consistently emphasize the irreplaceable value of human interpretation for assessing motivation, context, and potential. This creates a clear pathway for AI integration focused on augmenting rather than replacing human expertise, with particular emphasis on maintaining transparency and reviewer autonomy.

Strategic implications point toward a phased approach to AI integration, starting with low-risk administrative tasks while addressing immediate pain points through rubric refinement and improved reviewer feedback mechanisms. The research suggests that successful AI implementation will require careful attention to reviewer trust, cultural sensitivity, and the preservation of the humanistic values that define OMT's evaluation philosophy.

## 2. Methodology

- 11 substantive semi-structured discovery interviews (Jan 12-27, 2026)
- Interviews conducted by StrideShift with OMT review committee members
- Notes captured via Google Gemini transcription
- Analyzed using iterative thematic analysis with AI-assisted coding (Haiku for individual extraction, Sonnet for synthesis)
- Note: 4 additional recordings contained scheduling/internal discussions and were excluded

## 3. Participant Profiles

| Name | Institution | Discipline | Years Reviewing | Application Types |
|------|-------------|------------|-----------------|-------------------|
| **Dina Ligaga** | University of the Witwatersrand | Media & Cultural Studies, Literature, Gender Studies | 2022-present (4 years) | Humanities applications, Masters and PhD |
| **Edzai Conilias Zvobwo** | Not specified | Science and Mathematics | 2019-present (7 years) | Science/STEM applications across sub-disciplines |
| **Freedom Gumedze** | University of Cape Town | Biostatistics, Applied Health Research | Multiple years | Masters, PhD, Postdoc, Sabbatical across levels |
| **Frelet De Villiers** | University of Free State | Music/Arts | 4-5 years | Arts/Music applications |
| **Martin Clark** | University of Free State | Geology/Earth Sciences | 3-4 years | Masters, PhD, Sabbatical with interdisciplinary exposure |
| **Maureen De Jager** | Rhodes University | Fine Arts | ~3 years | Creative Arts, primarily Fine Arts applications |
| **Mohamed Cassim** | Independent Consultant | Finance/Strategy (Pharmacy, MBA background) | Multiple years | Financial applications, cross-disciplinary |
| **Philippe Burger** | University of Free State | Economics/Macroeconomics | 2-3 years | Business management, economics applications |
| **Pieter Pistorius** | University of Pretoria | Metallurgical Engineering | ~3 years | Engineering applications |
| **Ryan Nefdt** | University of Cape Town | Philosophy | Multiple years | Humanities (philosophy, linguistics, fine art, sociology) |
| **Frasia Oosthuizen** | Not specified | Pharmaceutical Sciences | Long-term (pre-online system) | Health Sciences, multiple disciplines |

## 4. Detailed Thematic Analysis

### 4.1 The Review Workflow: From Application to Decision

The analysis reveals three primary workflow patterns across reviewers, with individual variations reflecting disciplinary differences and personal preferences.

**Pattern 1: Sequential Deep Reading (Most Common)**
The majority of reviewers, including Freedom Gumedze, Frasia Oosthuizen, and Ryan Nefdt, follow a systematic approach: "I start by reading entire portfolio/proposal thoroughly" (Freedom Gumedze, Jan 26). This involves reading all materials first, then applying the rubric systematically. As Ryan Nefdt explained: "I read the submission and reviewing it systematically against the OMT rubric, particularly when dealing with a stack of 20 or 30 applications" (Ryan Nefdt, Jan 21).

**Pattern 2: Comparative Overview First**
Some reviewers, notably Maureen De Jager and Philippe Burger, begin with a scanning phase across all applications. Maureen described this as "comparative, particularly when dealing with multiple submissions, which helps establish an overall lay of the land before looking at each application specifically" (Maureen De Jager, Jan 22). Philippe Burger similarly uses "Initial Classification: Sorting applications by degree level (Masters, PhDs, Scholars)" before detailed review (Philippe Burger, Jan 13).

**Pattern 3: Batch Processing by Level**
Several reviewers, including Martin Clark and Frasia Oosthuizen, organize their work by application level. Martin noted: "Parcels applications by level (Master's, PhD, Sabbatical)" and "Reviews 5-10 applications per sitting to avoid 'adjudication fatigue'" (Martin Clark, Jan 15).

**Time Investment and Workload**
Time commitments vary significantly but are universally substantial. Pieter Pistorius reported "45-60 minutes per application over 2-3 evenings" plus "a final 20-minute consistency check across all applications" (Pieter Pistorius, Jan 19). Frasia Oosthuizen described spending "a good few solid hours" per batch (Frasia Oosthuizen, Jan 27), while Mohamed Cassim spreads his review "over several nights" (Mohamed Cassim, Jan 20).

**Current Tools and Systems**
All reviewers use OMT's online platform, with mixed satisfaction levels. Dina Ligaga expressed frustration with the "multi-window interface" calling it "frustrating and time-consuming" (Dina Ligaga, Jan 12). However, Edzai noted that "the past year was the first time software was used for the review process, as it was previously done manually with an Excel spreadsheet, which made the software quite convenient" (Edzai Conilias Zvobwo, Jan 13).

### 4.2 The Rubric Problem: Constraints and Workarounds

The 1-4 scoring system emerged as the most significant pain point across interviews, with reviewers developing sophisticated workarounds to manage its limitations.

**The Core Problem: Limited Granularity**
Dina Ligaga captured this perfectly: "Excellence can be 75% and sometimes it can be 90% - and those are very different types of excellence" (Dina Ligaga, Jan 12). She elaborated: "Sometimes I have to go back and either mark up or mark down just so the final tally kind of looks like what the person deserves" (Dina Ligaga, Jan 12). Martin Clark similarly noted the need for "more nebulous, flexible definition of excellence" and "broader understanding of candidate potential" (Martin Clark, Jan 15).

**Workaround 1: Retrofitting Scores**
Multiple reviewers described adjusting scores to match their holistic judgment. Maureen De Jager called this "retrofit" - "adjusting the final score if the initial impression suggests a different strength than the calculated rating" (Maureen De Jager, Jan 22). This represents a fundamental disconnect between the rubric's design and evaluative practice.

**Workaround 2: Starting High and Adjusting Down**
Frelet De Villiers revealed a unique approach: "I always start on a four. So, if I read through something, the candidate has a four. As I go further, I will then go, no, this is actually a three" (Frelet De Villiers, Jan 26). This inverts the traditional evaluation approach and suggests the rubric fails to capture initial positive impressions.

**Workaround 3: Narrative Compensation**
Several reviewers compensate for scoring limitations through detailed written feedback. As Pieter Pistorius noted: "Uses a 4-5 point scoring rubric" but "Writes 2-3 sentence rationale for scoring" (Pieter Pistorius, Jan 19).

**The Subjectivity Challenge**
Philippe Burger identified a core issue: "Assigning a score is always somewhat subjective, as it involves translating an impression into a numerical score" (Philippe Burger, Jan 13). Mohamed Cassim was more direct about rubric categories like "extraordinary talent," calling them "too subjective" (Mohamed Cassim, Jan 20).

### 4.3 What Makes an Excellent Application: Criteria and Judgment

Analysis reveals both universal and discipline-specific evaluation criteria, with significant emphasis on narrative coherence and authenticity.

**Universal Criteria: The Coherent Narrative**
Across all disciplines, reviewers prioritize what Ryan Nefdt called "cohesive narrative connecting life experience to academic goals" (Ryan Nefdt, Jan 21). Frasia Oosthuizen emphasized "Holistic alignment across: Motivation letter, Research proposal, Academic performance, Referee reports" (Frasia Oosthuizen, Jan 27). This narrative coherence transcends pure academic metrics.

**The Authenticity Imperative**
Multiple reviewers emphasized genuine motivation over polished presentation. Dina Ligaga valued "Genuine, unique personal motivation" and "Sincerity in personal statement" (Dina Ligaga, Jan 12). Martin Clark was even more explicit: "I always value passion higher than I value cleverness. You can always teach passion but you can't build passion always" (Martin Clark, Jan 15).

**Discipline-Specific Criteria**

*STEM Applications:*
Edzai Conilias Zvobwo looked for "Research with broad social impact" and "Solutions addressing fundamental problems," with "preference for work benefiting 'bottom of the pyramid'" (Edzai Conilias Zvobwo, Jan 13). Freedom Gumedze emphasized "Research innovation" and "Clear social impact potential" (Freedom Gumedze, Jan 26).

*Humanities Applications:*
Ryan Nefdt noted that "with humanities it's a little bit different. you have to figure out different conduits to um to getting some societal impact" (Ryan Nefdt, Jan 21). The emphasis shifts from direct measurable impact to cultural and intellectual contribution.

*Arts Applications:*
Maureen De Jager identified unique criteria: "Portfolio as critical assessment element," "Technical and conceptual sophistication," and "Evidence of practical excellence" (Maureen De Jager, Jan 22). She emphasized: "Assessing art requires time and a human element of interpretation, which cannot be easily short-circuited" (Maureen De Jager, Jan 22).

**Red Flags and Warning Signs**
Several reviewers identified common problems. Philippe Burger noted increasing concerns about "AI-generated essays" and applications with "weak foundational knowledge" (Philippe Burger, Jan 13). Maureen De Jager flagged "Misalignment between written proposals and portfolio quality" (Maureen De Jager, Jan 22).

**The Context Challenge**
Multiple reviewers emphasized the importance of contextual evaluation. Martin Clark noted: "Not every applicant has availability of word processing, spell-check engines" (Martin Clark, Jan 15). Edzai observed that "certain schools seem to teach how to answer applications effectively, while others produce applications with bad quality" (Edzai Conilias Zvobwo, Jan 13).

### 4.4 Bias, Subjectivity, and Fairness

Reviewers demonstrated sophisticated awareness of bias while struggling with practical mitigation strategies.

**Acknowledged Biases**
Several reviewers explicitly recognized their biases. Dina Ligaga admitted "Acknowledges unconscious bias based on applicant's background (race, class)" (Dina Ligaga, Jan 12). Edzai Conilias Zvobwo stated: "I have a personal bias towards impact programs that help the bottom of the pyramid" (Edzai Conilias Zvobwo, Jan 13). Martin Clark noted his approach to "adjudicate from multiple different angles because excellence... can be measured from different perspectives" (Martin Clark, Jan 15).

**Structural Inequalities**
Multiple reviewers recognized systemic disadvantages. Mohamed Cassim emphasized challenges in "evaluating candidates from different cultural/linguistic backgrounds" (Mohamed Cassim, Jan 20). Philippe Burger noted the need to be "conscious to not penalize people [for language differences]" (Philippe Burger, Jan 13).

**Cultural and Institutional Factors**
Edzai's observation about institutional preparation differences was particularly insightful: some schools "teach how to answer applications effectively, while others produce applications with bad quality where candidates cannot articulate the impact of their studies" (Edzai Conilias Zvobwo, Jan 13). This suggests systematic disadvantages for students from under-resourced institutions.

**Mitigation Strategies**
Reviewers employed various fairness strategies. Martin Clark described trying "not to push [scoring] towards my first intuition" (Pieter Pistorius, Jan 19). Frasia Oosthuizen emphasized trying "to be objective, but in something like this which is not based on quantitative values, there is a matter of subjectivity" (Frasia Oosthuizen, Jan 27).

**The Reference Letter Problem**
Several reviewers noted issues with recommendation quality. Ryan Nefdt observed that "recommendation letters lack comparative value" (Ryan Nefdt, Jan 21), while Frasia Oosthuizen noted that "referee reports don't always use 'exceptional' language expected by rubric" (Frasia Oosthuizen, Jan 27).

### 4.5 AI in the Review Process: Hopes, Fears, and Boundaries

This theme revealed the most nuanced and strategically important perspectives, with reviewers showing sophisticated understanding of AI's potential and limitations.

**The Spectrum of Attitudes**

*AI Optimists:*
Martin Clark and Mohamed Cassim showed the most positive attitudes. Martin saw potential for "Logic validation," "Stakeholder perception assessment," and "Data scraping" (Martin Clark, Jan 15). Mohamed wanted AI to "Handle budget verification," "Conduct database searches," and "Perform psychographic analysis" while serving as a "cognitive assistant to reduce superfluous work" (Mohamed Cassim, Jan 20).

*Cautious Adopters:*
Most reviewers, including Dina Ligaga, Freedom Gumedze, and Ryan Nefdt, were cautiously open. Dina wanted AI to "Conduct preliminary vetting of applications" and "Provide pre-populated rubric" while maintaining "human ability to override/modify" (Dina Ligaga, Jan 12). Ryan was "interested in AI assistance for: Providing discipline-specific context, Flagging inconsistencies in applications" (Ryan Nefdt, Jan 21).

*AI Skeptics:*
Frelet De Villiers and Maureen De Jager expressed stronger reservations. Frelet was "Skeptical of AI assistance in review process" believing "AI cannot replicate human judgment" (Frelet De Villiers, Jan 26). Maureen was "Somewhat skeptical of AI" while seeing "potential for Initial document filtering" (Maureen De Jager, Jan 22).

**Consensus: What Reviewers Want AI to Do**

1. **Administrative Tasks:** All receptive reviewers agreed on AI handling routine tasks. Mohamed wanted "budget verification" and "database searches" (Mohamed Cassim, Jan 20).

2. **Initial Screening:** Multiple reviewers supported preliminary filtering. Dina wanted AI to "identify which applications need attention" (Dina Ligaga, Jan 12).

3. **Data Verification:** Freedom Gumedze suggested "Research background verification" and "Bibliometric analysis" (Freedom Gumedze, Jan 26).

4. **Context Provision:** Ryan Nefdt valued the potential for "discipline-specific context" to help bridge understanding across domains (Ryan Nefdt, Jan 21).

**Explicit Boundaries: What AI Must Not Do**

The consensus was clear about maintaining human control over final decisions. Edzai emphasized wanting "human judgment for empathy, nuance, and final decision-making" (Edzai Conilias Zvobwo, Jan 13). Pieter Pistorius stated: "Eventually the human being must make a final call" (Pieter Pistorius, Jan 19).

**The AI-Generated Content Detection Problem**
This emerged as a significant concern. Philippe Burger noted "increasing prevalence of AI-generated essays" and "difficulty distinguishing genuine from AI-written motivation essays" (Philippe Burger, Jan 13). Dina Ligaga was "concerned about AI-generated motivational statements lacking authenticity" (Dina Ligaga, Jan 12).

**Trust Conditions and Transparency Requirements**
Several reviewers emphasized transparency needs. Ryan Nefdt wanted "transparency and trusted insights" and "assistance that doesn't replace human judgment" (Ryan Nefdt, Jan 21). Freedom Gumedze emphasized wanting "human oversight for final judgment" and "transparency and depth of analysis" (Freedom Gumedze, Jan 26).

**Sophisticated Understanding of AI Limitations**
Many reviewers demonstrated nuanced AI understanding. Edzai stated: "AI as it is can't reason. It's purely statistical, probabilistic" (Edzai Conilias Zvobwo, Jan 13). Mohamed noted that "AI right now... is built on data that is in the internet... by its very nature AI would go on historic trends when all of us know that the future is not told by history alone" (Mohamed Cassim, Jan 20).

### 4.6 The Feedback Gap and Reviewer Experience

This theme revealed significant gaps in reviewer support and development.

**Limited Feedback on Review Quality**
Most reviewers reported receiving minimal feedback about their evaluation performance. Dina Ligaga noted: "Limited feedback on review process" and questioned whether scores were combined across reviewers (Dina Ligaga, Jan 12). Freedom Gumedze explicitly asked: "I would like to know of the reviews that we have given, do they take a sort of combination of scores from the group?" (Freedom Gumedze, Jan 26).

**Cross-Reviewer Calibration Issues**
Several reviewers expressed uncertainty about consistency across evaluators. Edzai raised "a standing question regarding how many reviewers assess the same candidate to potentially debias results" (Edzai Conilias Zvobwo, Jan 13). This suggests limited coordination between reviewers.

**The Cold Start Problem**
Edzai identified a systematic issue: "identified the first application as the most difficult to judge due to the lack of a frame of reference, which they suggested could prejudice the person reviewed first" (Edzai Conilias Zvobwo, Jan 13). This suggests need for better benchmarking support.

**Reviewer Motivation and Satisfaction**
Despite challenges, most reviewers expressed satisfaction with the role. Frasia Oosthuizen noted: "I enjoy doing this because... it's amazing to see what some young people are achieving" (Frasia Oosthuizen, Jan 27). Pieter Pistorius called it "always quite a humbling experience... the caliber of the candidates" (Pieter Pistorius, Jan 19).

**The Volunteer Nature Challenge**
Several reviewers noted time pressures. Martin Clark mentioned "Time pressure of pro-bono work" (Martin Clark, Jan 15), while Pieter suggested to "Start review process earlier" to "Allow time to 'sleep on' applications" (Pieter Pistorius, Jan 19).

**Suggestions for Improvement**
Reviewers offered specific suggestions. Freedom Gumedze wanted "cross-reviewer score comparisons" and a "consensus/moderation process" (Freedom Gumedze, Jan 26). Mohamed suggested creating "a more curated scholarship experience" (Mohamed Cassim, Jan 20).

## 5. Cross-Cutting Tensions and Paradoxes

**Tension 1: Standardization vs. Flexibility**
The most prominent tension exists between OMT's need for standardized evaluation and reviewers' need for contextual flexibility. While the rubric provides comparability, reviewers consistently work around its limitations. As Dina Ligaga noted: "Sometimes I have to go back and either mark up or mark down just so the final tally kind of looks like what the person deserves" (Dina Ligaga, Jan 12). This suggests the current system forces reviewers to game their own evaluation process.

**Tension 2: Efficiency vs. Depth**
Reviewers want both streamlined processes and meaningful engagement with applications. Mohamed Cassim wanted AI to reduce "superfluous cognitive load" (Mohamed Cassim, Jan 20), while Maureen De Jager insisted that "Assessing art requires time and a human element of interpretation, which cannot be easily short-circuited" (Maureen De Jager, Jan 22).

**Tension 3: Technology vs. Human Judgment**
While most reviewers see AI's potential, they resist replacement of human insight. Edzai wanted AI for "algorithmic/rubric scoring" but "rejects fully autonomous systems" (Edzai Conilias Zvobwo, Jan 13). This creates a complex integration challenge.

**Tension 4: Individual Autonomy vs. Collective Consistency**
Reviewers value their independent judgment but recognize the need for consistency. Ryan Nefdt emphasized making personal interpretative decisions while acknowledging the rubric "allows comparison" (Ryan Nefdt, Jan 21).

**Paradox 1: The Rubric Workaround**
Reviewers simultaneously rely on and subvert the rubric system, suggesting it provides structure while constraining evaluation quality. This paradox indicates fundamental misalignment between tool design and user needs.

**Paradox 2: The Authenticity Detection Challenge**
Reviewers value authenticity but struggle to detect AI-generated content, creating an arms race between evaluation and manipulation that may undermine the entire premise of motivational assessment.

## 6. Strategic Recommendations

### 6.1 Quick Wins (Low effort, high impact)

**Expand Rubric Granularity**
Implement a 1-6 or 1-10 scoring scale to address the most common complaint. As Dina Ligaga noted, "Excellence can be 75% and sometimes it can be 90%" (Dina Ligaga, Jan 12). This simple change would reduce the need for scoring workarounds.

**Provide Reviewer Score Comparisons**
Address Freedom Gumedze's question: "I would like to know of the reviews that we have given, do they take a sort of combination of scores from the group?" (Freedom Gumedze, Jan 26) by providing anonymized peer comparison data.

**Improve Application Interface**
Fix Dina Ligaga's "multi-window interface" frustration (Dina Ligaga, Jan 12) with a single-screen design that reduces navigation complexity.

**Create Review Timeline Flexibility**
Address Pieter Pistorius's suggestion to "Start review process earlier" and "Allow time to 'sleep on' applications" (Pieter Pistorius, Jan 19) by extending review periods.

### 6.2 System Design Priorities

**Rubric Redesign**
Develop discipline-specific rubric variations while maintaining core comparability. Martin Clark's call for "categories [that] could be more nebulous or change based on discipline" (Martin Clark, Jan 15) points toward flexible evaluation frameworks.

**Reference Letter Standardization**
Address Ryan Nefdt's observation that "recommendation letters lack comparative value" (Ryan Nefdt, Jan 21) by providing reference writers with comparative frameworks and standardized questions.

**Application Quality Support**
Address Edzai's observation about institutional disparities where "certain schools seem to teach how to answer applications effectively, while others produce applications with bad quality" (Edzai Conilias Zvobwo, Jan 13) through application preparation resources.

**Reviewer Training and Calibration**
Develop systematic reviewer onboarding to address the "cold start problem" Edzai identified (Edzai Conilias Zvobwo, Jan 13) and improve cross-reviewer consistency.

### 6.3 AI Integration Strategy (phased approach)

**Phase 1: Administrative Automation**
Begin with low-risk, high-value tasks that all reviewers support:
- Budget verification and calculation (Mohamed Cassim's suggestion)
- Document completeness checking
- Basic data extraction and organization
- Timeline and deadline management

**Phase 2: Information Enhancement**
Implement AI tools that augment reviewer understanding:
- Discipline-specific context provision (Ryan Nefdt's suggestion)
- Institutional performance data compilation
- Publication and citation analysis (Freedom Gumedze's "bibliometric analysis")
- Comparative benchmarking data

**Phase 3: Preliminary Assessment**
Develop AI screening tools with human oversight:
- Initial application filtering based on minimum requirements
- Inconsistency flagging across application documents
- Potential quality indicators for reviewer attention prioritization

**Phase 4: Advanced Support (Future)**
Consider more sophisticated assistance only after establishing trust:
- Preliminary rubric population (as Dina Ligaga suggested) with full reviewer override
- Draft assessment summaries for reviewer review and editing
- Cross-application comparison insights

### 6.4 Change Management

**Gradual Introduction**
Follow Martin Clark's openness to AI while addressing Frelet De Villiers's skepticism through voluntary adoption and demonstrated value before mandatory implementation.

**Transparency and Control**
Ensure reviewers understand AI decision-making processes and maintain full override capabilities, addressing Ryan Nefdt's need for "transparency and trusted insights" (Ryan Nefdt, Jan 21).

**Training and Support**
Provide comprehensive training on AI tool capabilities and limitations, building on reviewers' sophisticated understanding demonstrated by Edzai's observation that "AI as it is can't reason. It's purely statistical, probabilistic" (Edzai Conilias Zvobwo, Jan 13).

**Feedback Loops**
Establish regular feedback mechanisms to understand AI tool effectiveness and reviewer satisfaction, addressing the current feedback gap multiple reviewers noted.

### 6.5 Risk Mitigation

**AI-Generated Content Detection**
Address Philippe Burger's concern about "increasing prevalence of AI-generated essays" (Philippe Burger, Jan 13) through:
- Advanced detection tools
- Interview components for high-stakes applications
- Writing sample verification processes

**Bias Amplification Prevention**
Address Mohamed Cassim's warning that "AI right now... by its very nature would go on historic trends" (Mohamed Cassim, Jan 20) through:
- Regular bias auditing of AI recommendations
- Diverse training data curation
- Human oversight requirements for all AI-assisted decisions

**Reviewer Agency Preservation**
Maintain the autonomy that reviewers like Frasia Oosthuizen value: "I prefer making those judgment calls because I don't want someone else to have told me" (Frasia Oosthuizen, Jan 27).

**Quality Assurance**
Implement safeguards against AI errors and ensure human reviewers can easily identify and correct AI mistakes, addressing Martin Clark's concern about "potential wrong answers" (Martin Clark, Jan 15).

## 7. Interviewee Quick Reference

**Dina Ligaga:** Brings humanities perspective with strong focus on authenticity and narrative coherence. Developed sophisticated rubric workarounds ("retrofitting scores") and is cautiously open to AI for preliminary screening while maintaining human override capabilities.

**Edzai Conilias Zvobwo:** Provides STEM evaluation expertise with explicit bias awareness and social impact focus. Champions "human in the loop" AI approach and identified the "cold start problem" in review sequencing.

**Freedom Gumedze:** Offers biostatistics background with comparative assessment experience across NRF and OMT systems. Advocates for systematic improvements including cross-reviewer calibration and consensus processes.

**Frelet De Villiers:** Represents arts evaluation perspective with unique "start high, adjust down" scoring approach. Most skeptical of AI integration while emphasizing the irreplaceable value of human artistic interpretation.

**Martin Clark:** Brings geological sciences expertise with strong anti-bias consciousness and holistic evaluation philosophy. Most AI-optimistic, seeing potential for logic validation and data enhancement while valuing "passion over cleverness."

**Maureen De Jager:** Provides creative arts assessment expertise with portfolio-centric evaluation approach. Demonstrates nuanced AI understanding, seeing potential for administrative tasks while insisting creative assessment requires human interpretation.

**Mohamed Cassim:** Contributes business/financial application expertise with emphasis on delivery probability over intent. Offers sophisticated AI vision as "cognitive assistant" while warning against historical bias in algorithmic decision-making.

**Philippe Burger:** Brings economics perspective with increasing concern about AI-generated applications. Advocates for stricter verification processes and systematic approaches to distinguish authentic from artificial content.

**Pieter Pistorius:** Provides engineering evaluation expertise with systematic approach emphasizing integrity and contextual assessment. Cautiously open to AI for preliminary screening while insisting humans make final decisions.

**Ryan Nefdt:** Offers philosophy background with emphasis on narrative coherence and comparative evaluation within cycles. Interested in AI for discipline-specific context while maintaining preference for human interpretative judgment.

**Frasia Oosthuizen:** Brings pharmaceutical sciences expertise with longest tenure and pre-digital system experience. Values independent judgment formation and holistic "picture creation" of candidates while being cautious about pre-interpretation by AI systems.