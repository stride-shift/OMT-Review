# Interview Analysis: Dina  Ligaga
**Date:** 2026_01_12 13_24 SAST
**Source:** OMT Discovery Interview with StrideShift (Dina  Ligaga) – 2026_01_12 13_24 SAST – Notes by Gemini.docx
**Transcript word count:** 7307
**Analysis model:** claude-haiku-4-5

# OMT APPLICATION REVIEW PROCESS ANALYSIS
## Interview with Dina Ligaga | January 12, 2026

---

## 1. INTERVIEWEE ROLE & BACKGROUND

**Role in OMT Review Process:**
Dina Ligaga serves as an adjudicator/reviewer for OMT scholarship applications.

**Duration of Involvement:**
Approximately 4 years (approached in 2022, as of January 2026)

**Discipline & Expertise:**
- Faculty: Humanities
- Field: Media and Cultural Studies (trained in literature)
- Specialist areas: Cultural studies, social sciences, work around gender
- Institutional affiliation: VIT (Vaal University of Technology, based on context)
- Brings interdisciplinary perspective spanning literature, cultural studies, and social sciences

---

## 2. CURRENT REVIEW PROCESS DESCRIPTION

**Overall Review Workflow:**

Dina begins by examining applications that have already been pre-screened (incomplete applications removed):

1. **Initial Reading Phase** - Opens application and reviews candidate's background, motivation, proposed study, CV, transcripts
2. **Motivation Assessment** - Reads motivation statement first to gauge applicant's thinking and sincerity
3. **Proposal Evaluation** - Reviews proposed study for originality and feasibility
4. **Academic History Review** - Examines transcripts and past work
5. **Extended Note-Taking** - Takes extensive notes in a notebook during review
6. **Rubric Completion** - Applies tick-box rubric scoring (4-point scale)
7. **Narrative Writing** - Writes narrative commentary explaining reasoning (though reports have become shorter)
8. **Reconciliation** - Sometimes adjusts rubric scores to align with overall intuitive assessment
9. **Final Recommendation** - Makes recommend/don't recommend decision with written justification

**Steps in Detail:**

- First looks at **motivation** to understand "how this person's thinking feels like" (00:08:17)
- Reviews **proposed study** for originality and excitement, especially at PhD level
- Examines **continuity** between applicant's history and proposed work
- Uses **notes** extensively but struggles with converting them to final written narratives
- Applies **rubric scoring** (1-4 scale) with tick-box sections
- Makes **final judgment call** on recommendation status
- Sometimes **backtracks** to adjust rubric scores to match intuitive sense

**Tools & Systems Currently Used:**

- **OMT submission platform** with multi-window interface (current system since approximately 2022)
- **Previous system**: Excel sheets with comparative review process (no longer in use)
- **Notebook** for extensive handwritten notes during review
- **Rubric** with 4-point scale (1=fail, 2=poor, 3=good, 4=excellent) for tick-box scoring
- **Narrative text fields** for final commentary
- **Document downloads** (multiple downloads required, described as "tedious")

**Time Spent on Reviews:**

- Approximately **30 minutes per application** for full reading and assessment (00:21:00)
- Time-intensive at **yearly start-up** due to platform navigation learning curve
- Becomes easier once familiar with interface rhythm
- Note-taking and final narrative writing adds significant time
- Occurs during busy period when also marking exams (00:16:06)

---

## 3. PAIN POINTS & CHALLENGES

**Critical Frustrations:**

**A. Platform/Interface Issues:**
- **Multi-window interface** is highly frustrating (00:06:03) - requires opening different windows to access information
- **Steep yearly learning curve** - significant time wasted at start of review cycle figuring out navigation
- **Tedious document downloads** required (00:33:28)
- Platform feels like an improvement over Excel sheets but still creates friction

**B. Rubric Limitations:**
- **"Fuzziness" between scores 3 and 4** (00:11:56) - the distinction is unclear and subjective
- **Insufficient gradation** - only 4-point scale doesn't capture nuance of excellence (00:21:00)
- **Lack of range for high-performing candidates** - cannot distinguish between 75% excellent and 90% excellent (00:23:04)
- **Bluntness of scoring system** - feels overly rigid and doesn't capture complexity of judgment
- **Scoring creates negotiation and uncertainty** (00:21:00) - reviewer must perform mental gymnastics to align scores with actual conviction

**C. Judgment-Rubric Disconnect:**
- Reviewer frequently needs to **mark up or down** scores to match their overall intuitive assessment (00:13:16, 00:14:41)
- Rubric process **"boxes them in"** and **"slows them down"** (00:19:59, 00:21:00)
- Describes phase as losing confidence - worries about whether scoring is accurate
- Feels rubric doesn't capture "all of the effort that I've put into reading through the whole thing" (00:21:00)

**D. Narrative Feedback Challenges:**
- **Written reports have grown progressively shorter** - often only 1-2 lines after rubric completion (00:04:41)
- Despite extensive note-taking, final narratives are minimal
- **No feedback mechanism** on quality or length of narrative assessments (00:04:41, 00:16:06)
- Uncertainty about expectations: "Am I supposed to make it longer? Am I supposed to make it shorter?" (00:16:06)
- Struggles with **converting notes to final written form**, especially when workload is heavy

**E. Volume & Time Pressure:**
- **Masters applications are particularly challenging** due to high volume (00:24:17, 00:25:22)
- PhD applications easier because applicants often near completion (00:24:17)
- Review occurs during **peak exam marking periods**, creating time crunch (00:16:06)
- Takes considerable time to "sink yourself into" applications and understand what people want (00:35:58)

**F. Process Clarity Issues:**
- **Uncertain about purpose of race/class disclosure** in applications (00:32:06)
- Ambiguity about **function of demographic information** collection
- Limited **transparency on decision synthesis** - unsure how multiple reviewers' assessments are combined

**Specific Bottlenecks:**

1. **Start-of-cycle learning curve** with platform navigation
2. **Rubric-to-judgment translation** step that causes hesitation
3. **Narrative writing phase** when workload is heaviest
4. **Masters volume** creating decision fatigue
5. **Edge cases** between 3 and 4 scores requiring difficult judgment calls

---

## 4. WHAT THEY VALUE IN APPLICATIONS

**Primary Evaluation Criteria (in priority order):**

**1. Applicant Motivation (Most Important)**
- Seeks **genuine, unique, honest motivation** (00:08:17)
- Values **sincere personal statement** about influences and experiences (00:09:30)
- **Red flag**: Generic motivation immediately signals applicant isn't serious (00:08:17)
- Looks for applicant's **personal story** and thinking process (00:08:17)
- Evaluates **believability and authenticity** - distinguishes between performed vs. genuine motivation

**2. Proposed Study (Highly Influential)**
- Prioritizes **originality and excitement** in proposed research, especially at PhD level (00:10:46)
- Examines **feasibility and thoughtfulness** of proposal (00:35:58)
- **Red flag**: Generic, unoriginal proposals with nothing exciting about them (00:10:46)
- At Masters level, more variable quality; at PhD level, generally stronger (00:10:46)
- Wants to see **clear thinking about research direction**, not vague statements

**3. Continuity Between History and Proposed Work**
- **"String through"** from applicant's background to proposed study (00:09:30)
- Evidence of **logical progression** and connection between past work and future plans
- Shows **sustained commitment** to research area (00:09:30)
- Influences overall judgment significantly

**4. Academic History & Transcripts**
- Reviews **past study performance** as context
- Less influential than motivation and proposal
- Can create tension when transcripts don't match application quality (00:25:22)
- Doesn't determine decisions but provides supporting evidence

**5. Budget (Least Important)**
- Reviewed but **minimally influential** on decision (00:19:59)
- Often dismissed casually: "Oo, that's a lot" or "Oh, that's little"
- Doesn't substantively impact recommendations

**Red Flags Watched For:**

1. **Generic motivation** - immediately raises concerns about seriousness
2. **AI-generated content** - describes concern about "fuzzier" genuine motivation in some applications where AI may have been used (00:09:30)
3. **Lack of originality** in proposed study
4. **Disconnect between transcripts and application quality** - applicant appears stronger on paper than grades suggest (00:25:22)
5. **No clear narrative arc** between past work and future plans
6. **Vague research direction** without specificity

**What Makes Applications Stand Out:**

- **Compelling personal narrative** with specific influences and experiences
- **Novel, clearly-thought-out research proposal** with genuine excitement about topic
- **Alignment between applicant's demonstrated interests** (in CV, previous work) and proposed study
- **Authenticity and honesty** in describing motivation
- **Evidence of sustained intellectual engagement** with research area
- **Clear understanding** of research problem and approach

**How Different Factors Are Weighted:**

- **Motivation + Proposed Study + Continuity** = Approximately 70-80% of decision-making weight
- **Academic history/transcripts** = Supporting evidence, approximately 15-20%
- **Budget** = Minimal weight (acknowledged as necessary but not determinative)
- **References/recommendations** = Provides "sense of who this person is" (00:34:38), valuable but secondary

**Tacit/Unwritten Criteria:**

Dina acknowledges using judgment factors **not formally captured in rubric**:

- **Sincerity and authenticity** in motivation (00:09:30)
- **Perceived difficulty of applicant's upbringing** - race and class angles influence reading (00:32:06)
- **Personal conviction about what person deserves** - synthesizes facts not fully in rubric (00:14:41)
- **Gut sense of commitment** and continuity (00:09:30)
- **"Feel"** for how applicant's thinking comes across

---

## 5. VIEWS ON AI/TECHNOLOGY

**Overall Attitude Toward AI in Review Process:**

Cautiously open but with significant concerns. Dina distinguishes between:
- AI she would welcome (preliminary vetting, pre-populated rubrics)
- AI she would not want (replacement of core judgment)

**Specific Concerns About AI:**

1. **Authenticity Detection** - Observes that AI-generated content creates "fuzzier" motivation statements that lack genuine sincerity (00:09:30). Concerned AI makes it harder to detect authentic personal motivation.

2. **Loss of Core Judgment** - Emphasizes that "the intellectual leg work is the fun part" (00:35:58) - doesn't want AI replacing the actual evaluation of ideas and candidates.

3. **Incomplete Capture of Decision Factors** - Recognizes that much of her judgment is intuitive and tacit (sincerity, continuity, personal conviction), which may be difficult for AI to systematize (00:09:30, 00:14:41).

4. **Dimension Reduction Risk** - Concerned that rubric lacks dimensionality; AI based on rubric would likely be even more reductive.

**What They WOULD Want Automated:**

1. **Preliminary Vetting** (00:35:58)
   - AI identifies which applications "need attention and which do not"
   - Focuses on originality and feasibility assessment
   - Reduces volume to manageable number
   - Described as helpful because sinking into applications is time-intensive

2. **Pre-Populated Rubric** (00:37:18)
   - Trusted partner/AI completes rubric scoring
   - Reviewer receives filled-in rubric as starting point
   - Allows focus on narrative feedback rather than tick-box completion
   - Provides "leeway to change scores if they disagreed" (00:37:18)

3. **Administrative/Technical Support**
   - Document aggregation to reduce tedious downloads
   - Platform interface improvements
   - Note organization assistance

**What They Would NOT Want Automated:**

1. **Final judgment call** on recommendation
2. **Narrative assessment and explanation** of reasoning
3. **Evaluation of sincerity and authenticity** in motivation
4. **Originality assessment** of proposed study (wants to read it themselves)
5. **Decisions involving tacit factors** (continuity, personal conviction)

**Trust & Transparency Requirements:**

1. **"Trusted source" or "trusted partner"** - repeatedly emphasized (00:35:58, 00:37:18)
   - Would need confidence in AI's training and logic
   - Would need to understand how decisions were made

2. **Flexibility to override** - essential requirement (00:38:21)
   - Pre-populated rubric must be changeable
   - Reviewer must retain authority to adjust scores
   - "I do need to have the kind of leeway or at least know that I could change something if I didn't agree" (00:38:21)

3. **Transparency about AI use** - notes applicants should state if they've used AI in applications (00:09:30)
   - Implies expectation same transparency should apply to review process

4. **Synthesis and feedback** - values knowing other reviewers' thoughts provided "somebody else is doing the work" of synthesis (00:29:08)
   - Wants assurance independent judgment isn't "too far off" (00:30:28)
   - Would accept AI synthesis if trusted

5. **Explainability** - wants to understand how AI rubric scores were determined so she can contextualize them

**Comparative Comfort with Technology:**

- Current system feels "faster" than old Excel comparative process but creates other friction
- Multi-window interface is frustrating but not a deal-breaker
- Comfortable with platform-based assessment rather than direct comparison with other reviewers
- Doesn't miss comparative review aspect ("mudies the water") but values knowing overall consensus exists

---

## 6. SUGGESTIONS & IDEAS

**Suggested Improvements to Rubric:**

1. **Expand Scoring Range** (primary suggestion) (00:24:17, 00:23:04)
   - Current 4-point scale insufficient, especially for distinguishing excellence
   - Should allow differentiation between "75% excellent" and "90% excellent"
   - Particularly critical for **Masters applications** due to high volume (00:25:22)
   - Note: Different rubrics exist for PhDs, Masters, and Postdocs; Postdoc rubric particularly constrained (00:24:17)

2. **Add Dimensionality to Scoring** 
   - Current rubric is too "blunt" (00:18:40)
   - Needs more gradations to capture the full range of high-performing candidates
   - Mentioned specifically wanting to distinguish excellence levels that reviewers intuitively recognize

**Suggested Improvements to Process:**

1. **Pre-Populated Rubric System** (strong recommendation) (00:37:18)
   - AI or trusted source completes rubric scoring first
   - Reviewer receives pre-filled rubric with ability to modify
   - Allows focus on **narrative feedback** rather than mechanical tick-box completion
   - Reduces back-and-forth adjustment currently required
   - "I'd love that. Absolutely." (00:38:21)

2. **Preliminary Vetting/Filtering** (strong recommendation) (00:35:58)
   - AI or research assistant reviews all applications
   - Identifies applications worthy of deep attention vs. those that don't require full reading
   - Outputs something like: "you need to pay attention to them and this person you don't need to pay attention to them"
   - Would dramatically reduce time spent on clearly-not-competitive applications
   - Caveat: Difficult to operationalize without reading full applications, but would still save time on partial vetting

3. **Platform Interface Improvements**
   - Reduce multi-window complexity (00:06:03)
   - **Consolidate information** so doesn't require opening multiple windows
   - **Reduce steep learning curve** at start of yearly cycle
   - Minimize **tedious document downloads** (00:33:28)
   - Create more intuitive navigation

4. **Feedback Mechanism on Narrative Assessments** (implied suggestion)
   - Provide feedback on length and quality of narrative reports
   - Clarify expectations for final commentary
   - Would help reviewers understand if brief (1-2 line) reports are acceptable or should be longer

5. **Support for Note Organization**
   - Assist with converting extensive notes into coherent narrative
   - Could be simple system for organizing notes by criteria
   - Would address bottleneck where notes exist but aren't translated to final written assessment

**Priority Features for New System (Ranked by Emphasis):**

1. **Wider scoring range** (repeatedly emphasized)