# Interview Analysis: Edzai Conilias Zvobwo
**Date:** 2026_01_13 09_26 SAST
**Source:** OMT Discovery Interview with StrideShift (Edzai Conilias Zvobwo) – 2026_01_13 09_26 SAST – Notes by Gemini.docx
**Transcript word count:** 6737
**Analysis model:** claude-haiku-4-5

# OMT Application Review Process Analysis
## Interview with Edzai Conilias Zvobwo

---

## 1. Interviewee Role & Background

**Position & Expertise:**
- Science reviewer for OMT scholarship applications
- Background in science and mathematics
- Reviews applications across the full science spectrum (natural science and others)
- Runs own company specializing in analytics and AI solutions

**Duration:**
- Reviewing applications since approximately 2019 (pre-COVID)
- "It's been a while" - exact timeline unclear, but multiple years of experience
- Also involved in the OMT Two Fellows process (mentioned as comparable to scholarship review)

**Key Context:**
- Familiar with previous manual processes (Excel spreadsheets) and recent software adoption
- Currently consulting on AI solutions for clients, giving him informed perspective on AI capabilities and limitations

---

## 2. Current Review Process Description

**Process Flow:**
1. Receives applications through software system (first year using this after manual Excel-based process)
2. Reviews candidate certificates and transcripts (high school, undergraduate performance)
3. Assesses applications against detailed rubric with multiple dimensions
4. Assigns numerical scores (0-4 scale)
5. Conducts "final assessment" where written summation of all evaluation is required
6. System allows reviewers to revisit and revise scores as pattern emerges

**Tools & Systems:**
- Recently adopted software (described as "quite convenient" compared to manual Excel spreadsheets)
- Uses a detailed, algorithmic rubric provided by OMT
- Scoring system: 0-4 scale
- System allows flexibility to change/revise scores after initial assignment

**Time & Workflow:**
- Process described as "mind-numbing" after extended periods
- Struggles particularly with:
  - First application review (no reference point)
  - Final written assessment/summation phase
- Can revisit and adjust scores as evidence accumulates across application batch
- No explicit mention of time per application, but implied to be significant given "mind-numbing" nature

**Standing Questions:**
- Unclear how many reviewers assess the same candidate for bias mitigation
- Unsure about inter-year institutional memory practices

---

## 3. Pain Points & Challenges

**The "Cold Start Problem":**
- First application is "always the most difficult one because there's no frame of reference"
- "You are flying in blind" with initial assessments
- "Could actually prejudice the person who's judged first"
- Only after 1-2 applications does a pattern emerge
- Requires going back and revising early scores when quality becomes clearer
- System flexibility to revise is necessary intervention for this problem

**Scoring Fatigue & Fuzzy Numbers:**
- Scoring becomes "mind-numbing" over time
- Numbers become "fuzzy" - reviewers lose clarity on what distinguishes a 3 from a 4
- Lack of clear boundaries in scoring scale
- Consistency deteriorates as fatigue sets in

**Final Assessment Summation:**
- Described as "quite a task" and "quite difficult"
- Struggle to write comprehensive summary despite having just reviewed all materials
- Attributes this to "human memory issue" - can't synthesize what was fresh during detailed review
- Process is "quite easy" while reviewing step-by-step, but summation phase creates sudden difficulty
- Uncertain if this is widespread issue or individual problem

**Bias Issues:**
- Personal bias toward impact-driven research helping "bottom of the pyramid"
- Bias from reviewing candidate transcripts with poor early performance
- Bias toward fundamental problems over niche/first-world oriented research
- Trying to "self-regulate" against these biases with mixed success
- Difficulty comparing disparate subjects (e.g., nanotechnology vs. animal husbandry)

**Subject Matter Expertise Gaps:**
- Covers wide range of science disciplines but isn't expert in all
- May inadvertently dismiss "next Nobel Prize winner" due to lack of subject expertise
- "I'm not a water purification expert"
- Risk of bias based on subject appeal rather than actual merit

**Saturation & Topic Clustering:**
- Hot topics (e.g., AI applications) see multiple similar submissions
- Need to identify "most impactful" within saturated topic areas
- Intra-year saturation requires contextualizing impact within South African context
- Makes comparison and differentiation more difficult

**Lack of Institutional Memory:**
- No inter-year contiguity - doesn't remember what was adjudicated previous years
- Feels like starting "on a little island" each year
- Cannot leverage patterns or benchmarks from previous cohorts
- Uncertainty whether this is intentional design or system limitation

**Application Quality Variance:**
- Significant disparity in application quality between schools
- Some schools teach effective application-writing; others produce poor quality
- Applicants struggle to articulate impact (focus on "I I I" rather than world-facing impact)
- Difficulty distinguishing between candidates who can't articulate vs. those without clear ideas

**Scoring Scale Resolution:**
- 0-4 scale has bins that are "too big"
- "High quality three" and "low quality three" bunched together indistinguishably
- Cannot adequately differentiate within quartiles
- Scale feels too coarse-grained for nuanced assessment

---

## 4. What They Value in Applications

**The Personal Story (Highest Weight):**
- Personal narrative has "the biggest weighting" in final decisions
- Looks for evidence of resilience and overcoming obstacles
- Values applicants showing they "came from a broken home and against all odds...overcome those obstacles"
- This is the critical element that "has to resonate with you as the adjudicator"
- Described as "very human" and "can't be automated"
- Will sometimes rate higher (4) even if articulation is weak if story implicitly resonates: "I get what you're saying and I'll give you four because I get what you're saying, but you're not saying it, but I get it"

**Meeting Minimum Requirements:**
- Must meet all baseline qualification requirements
- "They have to be competitive in terms of grades"
- Rubric minimum thresholds are disqualifying if not met
- "If they don't qualify then I discard literally"
- Academic credentials checked: high school performance, undergraduate GPA, transcript quality

**Research Impact & Scope:**
- Research must "help as many people as possible"
- Values fundamental problem-solving over niche applications
- Prefers immediate, visible impact: "someone who's creating a new way to clean jobic water" over "a way to multiply matrices better"
- Explicitly stated: "I would go for the job water because I can see the immediate"
- Bias toward research benefiting broader population vs. first-world oriented work
- Values South African contextual impact specifically

**Compelling Narrative Arc:**
- Application must demonstrate clear research direction and purpose
- Applicants must articulate (or at least implicitly convey) what they're trying to accomplish
- Connection between personal story and research focus matters
- Story that is "neutral" (nothing amazing or bad) gets lower weighting without strong narrative

**Quality of Presentation:**
- Ability to articulate ideas clearly matters
- Some applicants from certain schools demonstrate this; others struggle
- Recognition that some strong researchers lack application-writing training
- Appreciates when ideas are explained from external impact perspective vs. self-centered framing

**Red Flags (Implicit):**
- Poor undergraduate performance without compelling explanation
- Inability to articulate research direction or impact
- Research focused solely on niche academic advancement without broader application
- Applications suggesting candidates cannot present themselves effectively
- Lack of evidence of resilience or character development when reviewing transcripts

---

## 5. Views on AI/Technology

**Overall Position: "Human in the Loop" Model Essential**

Edzai is cautiously optimistic about AI support but emphatic about human oversight requirements. He describes himself as conversant with AI capabilities through his consulting work and has strong technical opinions.

**What AI Could/Should Do:**
- Handle "drudge work" - the algorithmic, rubric-based scoring
- Apply detailed rubrics consistently without emotional fatigue
- Process documents and extract information objectively
- Score applications using pre-programmed system prompts and South African context as guidelines
- Provide consistent baseline scoring as starting point for human review
- Manage the "mind-numbing" repetitive scoring work

**What AI Should NOT Do:**
- Make final decisions autonomously
- Evaluate personal stories or resilience narratives
- Assess "human stuff" like empathy or character
- Handle contextual judgment about South African impact
- Make truly subjective determinations about who "deserves" funding
- Replace human judgment on non-automatable elements

**Specific AI Implementation Concerns:**

*Reasoning Limitations:*
- "AI as it is can't reason... It's purely statistical. It's probabilistic."
- "It doesn't even understand what it's doing itself"
- "Autonomous end to end doesn't work"
- AI can only handle "things that require statistical pattern matching"

*Decision-Making Limitations:*
- AI cannot handle "intangibles like empathy"
- Cannot make "actual decision making" - this must remain in human domain
- "AI is not there yet maybe one day but it's not there"

*Transparency & Verification:*
- Implicit requirement that AI scores be explainable and reviewable
- Human must be able to understand and potentially override AI scoring
- "Leaving room for the human to actually review and almost give a second opinion basically"

**Trust & Confidence Requirements:**
- Willing to trust AI for algorithmic, rule-based tasks
- Insists on domain expertise for subject-matter assessment
- Would want subject matter experts to "shadow" and provide second opinions on specialized topics
- Trust based on clear system prompts and predetermined rubric criteria

**System Requirements:**
- Must have "system prompt literally where those instructions...are pre-programmed"
- Should be "pre-programmed...give it quant South African context such that it can go in and actually use that as a rubric"
- South African context is essential - not generic AI scoring
- Needs to maintain flexibility for human correction and revision

**Key Insight from Experience:**
- Has written a book on AI limitations
- Builds AI solutions for clients - understands both capabilities and constraints intimately
- This expertise makes him more realistic about what AI can achieve
- Clear distinction between marketing claims and actual AI capability

---

## 6. Suggestions & Ideas

**For Addressing the Cold Start Problem:**

1. **System flexibility is partial solution:**
   - Current ability to revise scores is "an intervention in itself"
   - Described as Bayesian approach: "change your beliefs as you gather more evidence"
   - Sufficient as interim solution but not ideal

2. **Institutional memory system:**
   - Would benefit from being able to review previous years' adjudications
   - Could establish benchmarks across years
   - "If there's that institutional memory literally for every adjudicator that would help actually"
   - Would reduce the "little island" problem of starting fresh annually

**For Reducing Bias & Improving Comparability:**

1. **Thematic grouping by subject area:**
   - "If there's a theme where it's almost closely neat around a subject...that would help in terms of objectivity"
   - Group similar research areas together for comparison
   - Instead of comparing "nanotechnology to someone who's doing animal husbandry"
   - Example: "if you had 10 submissions dealing with water and finding the best solution around water then it's easier to judge to compare"
   - Mitigates bias toward personally resonant topics
   - Allows for more meaningful relative comparison

2. **Subject matter expert consultation:**
   - Have domain specialists "shadowing" for specific research areas
   - Not broad field expertise but narrow specialization
   - "A subject matter expert in the particular research not in a broad area"
   - Would prevent dismissing breakthrough research due to personal unfamiliarity
   - "I might be throwing away the next Nobel Prize winner because it doesn't look appealing"

**For Addressing Scoring Scale Issues:**

1. **Increase bin granularity:**
   - Current 0-4 scale is "too blunt" with "bin size is too big"
   - "High quality three and a low quality three, they're all bunched up"
   - Suggested movement toward finer-grained scale
   - Would allow better differentiation within quality bands
   - No specific number suggested, but clearly wants more distinction points

**For Improving Final Assessment Process:**

1. **Support for summation task:**
   - Acknowledged as difficult but didn't propose specific solution
   - Possibly could benefit from:
     - Structured template for synthesis
     - Review guidance or checklist
     - System that surfaces key notes/highlights from review process
   - Noted uncertainty if this is widespread need or individual issue

**For Application Preparation & Support:**

1. **Training for applicants and schools:**
   - Some schools teach effective application articulation; others don't
   - "Maybe some of those people...need guidance or support or training"
   - Suggested intervention: support for universities in teaching application-writing skills
   - Particularly for institutions producing lower-quality applications
   - Goal: level playing field across geographic/institutional backgrounds

**What NOT to Change:**

- Rubric dimensionality: "I think the rubric is quite comprehensive...mutually exclusive cumulatively exhaustive...don't think there any dimensions that are missing currently"
- Overall process: "The process is amazing. I wouldn't change anything"
- Basic structure: The approach is sound, just needs refinement in execution

**Priorities for AI Implementation:**

1. **First priority:** Rubric-based scoring automation (explicitly mentioned as "mind-numbing drudge work")
2. **Second priority:** Freeing human cognitive capacity for non-automatable elements (story, resilience, empathy)
3. **Foundational requirement:** Maintain human oversight and revision capability

---

## 7. Key Direct Quotes

**On the Core Challenge of Scoring:**
> "The most difficult one is always the first one because there's no frame of reference. There's no standard set. So literally you are flying in blind. So that could actually prejudice the person who's judged first because there's no frame of reference." (00:04:10)

*Context: Explains the cold start problem and its impact on fairness*

---

**On AI's Appropriate Role:**
> "I would suggest a situation where there is AI with a human in the loop where AI does the drudge work the mind numbing stuff but leaving room for the human to actually review and almost give a second opinion basically." (00:08:19)

*Context: Proposes the ideal AI-human collaboration model*

---

**On What Humans Must Assess:**
> "On the personal story where applicants are trying to show their resilience their story where they came from a broken home and against all odds they tended to overcome those obstacles. So that is very human because it has to resonate with you as the adjudicator and I don't think that element that little element in the process can be automated. So that's the one that I would bring to the table as a human being." (00:13:46)

*Context: Defines what is non-automatable and requires human judgment*

---

**On Justifying Higher Scores:**
> "I've had a situation where I literally held their hand to say, 'Okay, I get what you're saying and I'll give you four because I I I get what you're saying, but you're not saying it, but I get it.'" (00:14:52)

*Context: Illustrates the gap between explicit articulation and implicit understanding, showing human judgment in action*

---

**On What "Deserving" Means:**
> "Deserve is where they have a really compelling story. They have all the requirements and their research is going to help as many people as possible. So in a way you find that in terms of impact the natural bias where is almost towards how many people is it going to help you know and how fundamental is the problem." (00:15:56)

*Context: Defines his weighting criteria and reveals impact-focused bias*

---

**On the Personal Story's Weight:**
> "I think the story the story matters. For me the story matters...If it's a case of there's nothing amazing or bad, it's just neutral kind of thing. Then the story comes into the frame to say, 'Okay, what is this person's story?' So that's the one with the biggest weighting here." (00:18:20)

*Context: Confirms personal narrative as the heaviest factor in his decision-making*

---

**On the Summation Challenge:**
> "When you're doing the process it's quite easy but that final assessment where you have to write to say okay I don't know somehow it's so difficult...I struggle with that. Yeah the final assessment where you're writing and just finding the summation of everything...For a candidate. Yeah. I find it quite a task." (00:18:20, 00:19:38)

*Context: Identifies a significant cognitive bottleneck in the review process*

---

**On Subject Matter Expert Needs:**
> "Because of the current configuration where you have many subjects, I'm not a water purification expert. So if I could have a water purification expert shadowing and giving their second opinion around that particular one then yes that would add value because I might be throwing away the next Nobel Prize winner because it doesn't look appealing because you know so that would actually help of having a subject matter expert in the particular research not in a broad area." (00:26:06)

*Context: Identifies the risk of bias from lack of specialized knowledge*

---

**On AI's Fundamental Limitations:**
> "I'm quite aware of what AI can