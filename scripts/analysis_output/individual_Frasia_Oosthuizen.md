# Interview Analysis: Frasia Oosthuizen
**Date:** 2026_01_27 10_56 SAST
**Source:** _OMT Discovery Interview with StrideShift (Frasia Oosthuizen) – 2026_01_27 10_56 SAST – Notes by Gemini.docx
**Transcript word count:** 5389
**Analysis model:** claude-haiku-4-5

# OMT Discovery Interview Analysis: Frasia Oosthuizen

## 1. Interviewee Role & Background

**Role in OMT Review Process:**
- Peer reviewer for OMT postgraduate scholarship applications
- Reviews applications across health sciences disciplines (not limited to pharmacy despite background)
- Evaluates applications at multiple levels: Masters, PhD, Postdoc, and Sabbatical categories

**Duration of Involvement:**
- Long-standing reviewer, though exact duration unclear to her ("I honestly can't remember")
- Has been reviewing since before the current online system was implemented
- Notes the previous system involved paper submissions rather than the current two-page format with integrated rubric

**Discipline/Expertise:**
- Registered pharmacist (required to maintain registration as she teaches in pharmacy program)
- Master's degree and PhD in Pharmacology
- Associate Professor in Pharmaceutical Sciences
- Teaches both undergraduate and postgraduate students
- Has extensive academic experience spanning many years
- Reviews across health sciences broadly, not limited to her pharmacology background

---

## 2. Current Review Process Description

**Overall Approach:**
- Reviews applications in categorical batches (Masters together, PhDs together, Postdocs together, Sabbaticals together) to maintain objectivity and comparative consistency
- Attempts to complete each category in a single day to maintain continuity and recall across candidates
- Works through one complete application at a time, not scanning multiple candidates first

**Step-by-Step Process:**
1. **Read the full application first** - reads motivation letter, proposal, academic history, and all supporting materials before consulting the rubric
2. **Then applies the rubric** - only after forming initial impressions
3. **Takes detailed handwritten notes** - maintains notes throughout the process for reference and recalibration
4. **Compares within batch** - if scoring a fifth candidate, may flip back to reconsider scores of earlier candidates
5. **Recalibration** - explicitly adjusts previous scores if new candidates reveal that earlier scores were disproportionate ("I gave this one a four but really then that one shouldn't have been a four. It should have been a three")

**Reading Process Sequence:**
- Motivation letter first (to understand "who this candidate is")
- Prior academic performance and marks
- Proposal/study design
- References/academic history
- Photo (aids visual recall)
- Referee reports (reviewed cautiously, not allowed to "sway" her too much)
- Budget information

**Tools/Systems Currently Used:**
- Online application system with dual-screen capability
- Two-page format with rubric displayed on one side, application on the other
- Rubric integrated into the platform
- Makes handwritten notes alongside digital review

**Time Investment:**
- "A good few solid hours of work" to get through an entire batch
- Attempts to complete Masters applications in a single day
- May require brief review period if not completed in one sitting to refresh memory

---

## 3. Pain Points & Challenges

**Problem 1: Missing Academic Marks**
- Applicants sometimes don't have final marks available by submission deadline (e.g., just completed exam sessions)
- Current rubric requires all fields to be completed, which pulls the overall score up or down despite missing information
- **Issue:** Applicants are penalized through no fault of their own
- **Current workaround:** Frasia acknowledges the problem but appears to work around it inconsistently
- **Her suggestion:** Option to exclude missing marks so they don't artificially affect the score

**Problem 2: Rubric Interpretation Inconsistency**
- Rubric language is ambiguous and open to interpretation
- Example: Referee report criteria explicitly state need for "exceptional" language, but referees rarely use that exact terminology
- Frasia has "ignored it but consistently in a group" - meaning she adapts the rubric interpretation within each batch but acknowledges this may differ from other reviewers
- Some rubric questions are "not possible to answer" for certain application types (mentions marks for Masters going into PhD programs)
- **Recognition:** She understands this is inherent to qualitative assessment, not necessarily a flaw, but it exists

**Problem 3: Referee Report Rubric Criteria**
- The rubric's expectation for referee reports is "very explicitly stated" but misaligned with actual referee language
- Requires language like "candidate is exceptional" or "academic standing is exceptional" to score a 4
- Frasia writes referee reports frequently and notes this explicit language is rarely used in practice
- Creates a disconnect between what the rubric expects and what referees actually write
- **Impact:** Forces reviewers to make judgment calls about equivalence that may not align across reviewers

**Problem 4: Scope/Scale of Applications**
- Implicit challenge: large number of applications to process
- Masters applications noted as particularly voluminous ("they tend to be a bigger pool")

**Problem 5: Consistency Between Reviewers**
- Different reviewers may interpret the rubric differently
- Frasia recognizes this may result in inconsistent standards across reviewers, though she prioritizes internal consistency within her own batch
- Acknowledges this as inherent to qualitative, non-quantitative assessment

**Problem 6: Limited Context for Specialized Fields**
- When reviewing outside her immediate expertise, sometimes encounters submissions requiring background research
- May need to "look things up" to understand field-specific context
- No systematic provision of contextual information for reviewers less familiar with specialized areas

**What She Doesn't Find Problematic:**
- The current dual-screen system
- The overall information provided in applications (finds it "exactly what I need")
- The structure of the application itself
- Individual components (motivation, proposal, budget, etc.) - all feed into the overall judgment
- Nothing feels "outstanding" or unnecessarily difficult apart from the issues noted above

---

## 4. What She Values in Applications

**Key Criteria & What She Looks For:**

**1. Alignment Across Application Components**
- **Most Important Judgment Factor:** Whether motivation letter, proposal, prior performance, and referee reports all "tell the same story"
- Tests if everything "makes sense from the motivation through to the referee reports"
- Uses this "big picture" coherence to validate her scoring confidence
- If components align, she feels "more confident that I have an understanding of who this candidate is and that my mark...is a better reflection"

**2. Motivation & Purpose**
- Reads motivation letter first to establish "who this candidate is"
- Evaluates if the candidate's stated goals are realistic and aligned with their project
- Example of misalignment: "candidate wants to cure cancer, but the project is so basic, there's a total disalignment"
- Assesses if motivation reveals genuine commitment vs. "spin"

**3. Proposal Quality & Feasibility**
- Reviews if the research proposal is achievable and appropriate in scope
- Checks if proposal aligns with candidate's stated motivations and career goals
- Looks for realistic project design given the candidate's level (Masters vs. PhD, etc.)
- Evaluates contribution to society/field

**4. Prior Academic Performance**
- Considers marks and degree classifications (distinction, merit, pass)
- Values consistency of performance over time
- This is important context but she notes it's largely factual (not requiring interpretation)

**5. Funding/Financial Commitment**
- Wants to understand if candidate has sourced funding or applied for other bursaries
- Interprets this as indicator of how much "they want this degree"
- Views financial commitment as signal of genuine investment in the degree

**6. Referee Reports**
- Values reference reports highly but with caveats
- Notes they are "usually overwhelmingly positive" 
- **Cautious approach:** "I'm cautious not to let it sway me too much"
- Uses them as validation check rather than primary decision-maker
- Important for triangulation ("if everything...through to the referee reports...makes sense")

**Red Flags She Watches For:**

1. **Misalignment between components:**
   - Motivation doesn't match proposal
   - Prior performance doesn't justify proposal ambition
   - Referee reports conflict with application materials

2. **Overstated claims:**
   - Motivation that "spins a lot of very nice things" but proposal doesn't align
   - Goals that exceed realistic project scope

3. **Inconsistency in quality:**
   - Excellent prior marks but weak proposal
   - Weak prior performance but exceptionally positive references

4. **Disconnection between aspiration and reality:**
   - Grandiose goals with basic methodology
   - Projects that don't reflect stated values or direction

**What Makes an Application Stand Out:**

1. **Clear, coherent narrative** across all components
2. **Realistic ambition** - stretch goals that are achievable, not fantasy
3. **Demonstrated commitment** - funding sourced, applications to multiple bursaries
4. **Quality of writing** - in motivation letter and proposal
5. **Evidence of genuine passion** - motivation that reads as authentic, not generic
6. **Visual presentation** - notes the photo helps her recall and understand the candidate

**How She Weighs Different Factors:**

- **Hierarchical/Holistic:** She integrates factors into a "big picture" assessment rather than using strict weighting
- **Alignment as the primary lens:** All factors are evaluated for consistency and coherence
- **Within-batch comparison:** Relative to other candidates in the same category, not absolute standards
- **Subjective integration:** Acknowledges all of this is "very subjective" but maintains internal consistency within each batch
- **Quality of evidence:** What is explicitly stated (marks, funding status) is less interpretively demanding than alignment and narrative coherence

---

## 5. Views on AI/Technology

**Overall Attitude: Cautious, Protective of Judgment**

Frasia is not opposed to AI but has significant concerns about how it might be implemented. Her stance is sophisticated: she sees value in AI for certain technical tasks but is protective of her role in making judgment calls.

**What She Would NOT Want Automated:**

1. **Pre-analysis or flagging of inconsistencies:**
   - Explicitly rejected the idea of AI identifying misalignments for her to validate
   - Reasoning: "If that person tells me what the red flags are, I already have decided in my mind no this is not going to work. So I'm really not objective."
   - Concern: Pre-identified red flags create unconscious bias ("you don't let the candidate speak")
   - "I prefer getting that picture myself and make my own judgment call"
   - **Core principle:** She wants to form her own complete impression independently

2. **Pre-summary or interpretation of materials:**
   - Doesn't want someone else to have "already created that picture"
   - Fears reviewer would come in with "that person's preconceived judgment"
   - Values the unmediated reading of the application to avoid anchoring bias

3. **Anything that structures her judgment prematurely:**
   - Views independent reading as essential to objectivity
   - Explicitly states: "I don't start with the referee reports because they are usually overwhelmingly positive. If you start with something that has already pulled out negatives or positives you don't let the candidate speak."

**What She WOULD Want Automated/Supported:**

1. **Factual data extraction & pre-filling:**
   - Academic marks and performance classifications (distinction, merit, pass)
   - Funding status (has/hasn't sourced funding)
   - Reasoning: These require no interpretation - "anyone can look at a mark sheet"
   - "I don't need to indicate that"
   - Would reduce "superfluous cognitive load"

2. **Contextual background information:**
   - "Layman's abstract" - simplified but not dumbed-down explanation of technical proposals
   - Field-specific context for proposals outside reviewer's expertise
   - Helps her understand specialized proposals without requiring research
   - She framed this as helping her evaluate writing skills too ("that also and that might also contribute to how we can see these applicants right their writing skills")

3. **Rubric clarity/standardization:**
   - Better definition of terms (e.g., what constitutes "exceptional" in referee reports)
   - Optional fields for missing information (marks not yet available)
   - Clearer guidance on field-specific questions that might not apply to all applicants

**Technology Concerns:**

1. **Loss of objectivity through anchoring:**
   - Critical concern: Pre-structured information could unconsciously bias her judgment
   - Based on cognitive science understanding: Starting with negatives/positives predetermines evaluation
   - This is about psychological safety and decision-making integrity

2. **AI-generated motivation letters:**
   - Notes that motivations "can probably be AI'd" - concerned that this changes what the motivation letter reveals about the candidate
   - Implies awareness that AI could make it harder to discern authentic motivation

3. **Consistency of interpretation:**
   - Accepts that different reviewers will interpret qualitative rubrics differently
   - Not opposed to this inherent variability but values her internal consistency
   - Concerned about over-standardization that might mask legitimate judgment differences

**Trust & Transparency Requirements:**

- Wants to understand and validate any AI-assisted process
- Would need clear explanation of what AI is doing and why
- Appears to require that AI not substitute for her judgment on nuanced matters
- Implicit: would need transparency about how recommendations are generated
- Important that AI serves as tool, not authority ("validate it and see if this aligns with your thinking" - she wants validation role, not acceptance of AI judgment)

**Motivation for Accepting AI Support:**

- Not motivated by workload reduction (she finds the work "rewarding")
- Motivated by enabling OMT to "handle more scale"
- Motivated by potentially improving consistency or clarity of process
- Interested in supporting other reviewers (helping them access contextual information)

---

## 6. Suggestions & Ideas

**Explicit Suggestions for Improvement:**

1. **Rubric Flexibility for Missing Data:**
   - Add option to exclude marks that haven't been released yet
   - These shouldn't penalize candidates when data is simply unavailable at submission time
   - Allows the rubric to generate accurate scores despite timing issues

2. **Referee Report Criteria Refinement:**
   - Review the language used in referee report expectations ("exceptional," "academic standing," etc.)
   - May need to add more lenient interpretation guidelines
   - Consider what language referees actually use in practice and align rubric expectations accordingly
   - Possibly allow for equivalent language that conveys same meaning without exact words

3. **Layman's Abstract Requirement:**
   - Request that applicants provide simplified (but not "dumbed down") explanation of technical proposals
   - Not replacing the scientific proposal, but accompanying it
   - Benefits: helps reviewers understand specialized fields, assesses candidate's ability to communicate across audiences
   - Format suggestion: "language that is more easy to understand for everyone that's not necessarily a biologist or a bioineticist"

4. **Contextual Information for Specialized Fields:**
   - Provide background context when proposals fall outside standard disciplinary knowledge
   - Could be brief field-specific primers or terminology guides
   - Reduces need for reviewers to conduct external research

5. **Pre-population of Factual Data:**
   - Auto-populate marks/grade classifications from academic records
   - Auto-populate funding status from application form data
   - Reduces superfluous cognitive load for factual verification

**Implicit Improvements Suggested:**

6. **Application Sequence/Structure:**
   - Current structure works well for her (motivation → proposal → prior performance → refs)
   - Would maintain this as it supports her judgment process

7. **Rubric Clarity:**
   - Some questions in rubric are unanswerable for certain application types
   - Need review of question relevance across different scholarship categories
   - Acknowledge that not all questions apply equally to Masters vs. PhD vs. Postdoc vs. Sabbatical

8. **Batch Grouping System:**
   - Current system of grouping by type and attempting same-day review works well
   - Supports consistency within reviewer judgments

9. **Dual-Screen Interface:**
   - Keep and maintain current two-screen split format (application + rubric)
   - Already supporting her workflow effectively

**Priorities for Change (Inferred from Emphasis):**

1. **Highest Priority:** Rubric criteria alignment with practice (referee reports, missing marks)
2. **High Priority:** Reducing superfluous cognitive load (data pre-population)
3. **Medium Priority:** Supporting understanding of specialized fields (context, abstracts)
4. **Lower Priority:** Changes to overall structure or interface (current system works well)

**What She Explicitly Would NOT Change:**

- The information required in applications ("I've always found it exactly what I need")
- Any of the major components (motivation, proposal, refs, budget, prior performance)
- The basic structure of the review process
- The requirement for independent judgment of alignment

---

## 7. Key Direct Quotes

### Quote 1: On Finding Meaning in Reviews
**Context:** Discussion of why she continues to review despite significant time commitment

> "It's amazing to see what some young people are achieving in their lives. So it's I've actually thought it was a very nice experience. So I'm actually always glad that they asked me."

**Significance:** Reveals intrinsic motivation for participation; this is not a burdensome task for her but genuinely rewarding work.

---

### Quote 2: On Alignment as Core Judgment Criterion
**Context:** Explaining her primary framework for evaluation

> "I try and create a picture of this applicant...everything if the big picture makes sense from the motivation through to the referee reports, I feel more confident that I have an understanding of who this candidate is and that my mark whether good or bad is a better reflection."

**Significance:** Reveals her holistic, coherence-based approach; alignment is THE central