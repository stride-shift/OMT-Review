# Interview Analysis: Freedom Gumedze
**Date:** 2026_01_26 11_29 SAST
**Source:** OMT Discovery Interview with StrideShift (Freedom Gumedze) – 2026_01_26 11_29 SAST – Notes by Gemini.docx
**Transcript word count:** 4932
**Analysis model:** claude-haiku-4-5

# OMT Discovery Interview Analysis: Freedom Gumedze

## 1. Interviewee Role & Background

**Role in OMT Review Process:**
- Active reviewer for OMT postgraduate scholarship applications for "a few years"
- Also an applicant to OMT programs (received sabbatical funding in 2020; shortlisted for New Frontiers Award two rounds prior)
- Experienced across multiple review contexts

**Duration of Involvement:**
- Multiple years as reviewer (exact duration not specified, but indicates "three or four years" of accumulated review experience at minimum)

**Discipline/Expertise Area:**
- Professor of Statistical Sciences at University of Cape Town
- Biostatistician specializing in applied health research
- Research focus: innovative statistical methods in dermatology, cardiology, and infectious diseases
- Advanced credentials: C1-rated researcher (NRF rating), Master's in Mathematical Statistics, PhD in Statistics from UCT
- Editorial experience: Associate Editor for *Journal of Applied Statistics* and *Biometrics* (premier journal, newly appointed for 3-4 years)
- Additional academic roles: Department head (5 years, continuing for 3 more), sits on NRF rating panels, University of Cape Town promotions committee, recruitment committees

---

## 2. Current Review Process Description

**Overall Approach:**
Freedom describes a **mixed process** combining initial holistic reading with structured rubric application:

> "I'll read a portfolio. Obviously other things like you're looking at references, you're looking at whether there's a budget or not and things like that, but one kind of starts off looking at the core sort of content as to what the proposal what the proposal is about."

**Specific Steps:**

1. **Initial familiarization with rubric** - reviews rubric before/during assessment (looked at it "a week ago")
2. **First read-through** - reads the portfolio holistically "as I would read in a portfolio," getting a sense of the proposal's substance
3. **Core content assessment** - evaluates:
   - What the proposal is about
   - Whether applicant has sufficient/good background for the work
   - Whether they're already accepted at the proposed institution (for most categories)
   
4. **Criterion-based assessment** - returns to rubric to check against:
   - Social impact
   - Academy contribution
   - Applicant benefits
   - Innovation level

5. **Detailed evaluation elements** - reviews:
   - References
   - Budget
   - Institutional fit (overseas school/university quality)
   - Research quality (particularly for postdocs and sabbaticals)

6. **Comparative benchmarking** - applies historical comparison:
   - Compares against "previous one[s]" across multiple years
   - Uses accumulated 4-5 year records from similar processes
   - Acknowledges this introduces bias but considers it necessary for calibration

7. **Star rating assignment** - determines whether application meets 1-star through 4-star criteria

**Tools/Systems Currently Used:**
- OMT rubric (standardized across categories with level-specific criteria modifications)
- Personal/historical records (Freedom maintains summary reports from multi-year reviews)
- External sources for publication assessment:
  - Google Scholar
  - Scopus
  - Bibliometrics systems (used at UCT)
  - NRF rating information
- Communication via letter (receives thank-you letters for reviews, but limited feedback)

**Time Spent:**
Not explicitly stated, but context suggests:
- Typically reviews up to 20 applications per round (mixed categories)
- No indication of time per application or total time commitment provided
- Indicates reviewers have "limited time" (mentioned as constraint)

---

## 3. Pain Points & Challenges

**Challenge 1: Ambiguous Publication Assessment Criteria**
> "Publications by the way that's loose because you're not told how you should do this publication. Should you be looking at Google Scholar? should be scopas... it's going to pick up something from MDPI or something. It's still going to be in Google Scholar. Um, so we're not told how you should assess the publications."

This requires subjective judgment and is time-consuming for thorough review.

**Challenge 2: NRF Rating as Outdated/Imperfect Metric**
> "if you can look at their work now maybe they are not they not at that at at at at that rating... the rating is dwindling in the sense that that's not the only metric that you you would measure someone's uh research impact."

- Ratings can be 5+ years old
- Doesn't reflect current research quality
- Overreliance on ratings can be misleading
- Universities like UCT moving away from ratings

**Challenge 3: Missing Standardization for Key Submissions**
- Applicants aren't instructed on publication sources to provide
- Social impact/sustainability goal contributions "not asked of the candidates—they can just put it in any way that they want"
- Biblometrics not requested despite being valuable for assessment
- No standardized guidance on what metrics should be included

**Challenge 4: Disciplinary Mismatch in Rubric Design**
> "The rubric is built on a master's by dissertation... if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part..."

- Rubric assumes dissertation-heavy master's (traditional model)
- Many disciplines (statistics, data science) are course-work heavy with small dissertation components
- Cannot fairly evaluate coursework-based vs. dissertation-based programs using same criteria

**Challenge 5: NRF Model Over-Reliance**
> "it's built on a NRF kind of the only thing that has been added is that you have got this social responsive uh social sort of like impact that you're looking for in fact everywhere you are looking for this social impact which I think it's a good thing but... it's kind of leans towards NRF sort of like way of assessing proposals."

- Rubric structure mirrors NRF assessment heavily
- May not be optimal for scholarship assessment
- Social impact addition is positive but broadly applied

**Challenge 6: Lack of Reviewer Calibration & Feedback Loop**
> "I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group because I'm assuming that I'm I look at the statistics uh sort of uh uh projects or rather proposals but maybe there's another statistician that looks at at that but I want to know at the end what they they desire so that maybe we can calibrate that from this side"

- No visibility into how other reviewers scored same applications
- No indication of whether consensus or moderation process exists
- Unclear if scores are averaged or reviewed by committee
- Cannot calibrate scoring without feedback on outcomes

**Challenge 7: Sabbatical Proposals Less Constrained**
> "What's more kind of like loose is when it comes to sabaticals people will find that I want to go on sabatical but it's not necessarily going to a particular sort of like institution overseas... they just putting together a a sort of a a research project"

- Less structural clarity than degree-seeking categories
- Applicants have more flexibility but less guidance

**Challenge 8: Potential for Subconscious Bias in Benchmarking**
> "that does happen. I don't think it's completely independent that you you one can't compare with past that they have seen."

- Acknowledges that historical comparison introduces bias
- Different from context-appropriate benchmarking (e.g., scoring against current cohort standards)
- Not done systematically for OMT but questions whether it should be

---

## 4. What They Value in Applications

**Core Research Quality Elements:**

1. **Innovation & Novelty**
> "I can sort of have a feel as to whether something is innovative or not"
- Uses expertise from editing peer-reviewed journals to assess innovation
- Looks for novel statistical/methodological approaches in their discipline

2. **Applicant-Proposal Fit**
> "the person has chosen a school has got has got sufficient background has good background to actually pursue the work they saying they're going to be pursuing"
- Sufficient background for the proposed work
- Good background (quality of preparation)
- Alignment between applicant's training and proposed research

3. **Institutional/School Fit**
> "looking at they want to go overseas. Is that a good school or not? Is it aligned with what they are currently doing?"
- Quality of the overseas institution
- Strategic alignment with applicant's current work and career trajectory

4. **Research Standing (for Postdocs/Sabbaticals)**
- Depth of understanding of their own research quality
- Current research impact (not just ratings)
- Publication venues and quality (not just H-index)
- Active research networks and collaborations

5. **Social Impact Alignment**
> "Are these things there for this? Because I I remember in the last not last round this last round there were applicants who even in the scoring obviously there will be one star candidate but there's some information that's probably missing or not properly articulated in the portfolio"
- Social impact goals clearly articulated
- Alignment between stated social activities and career direction
- Sustainability goals clearly linked to proposal

6. **Academic Background Quality**
- Educational trajectory
- Prior training adequacy
- CV quality and completeness

**Critical Assessment Criteria by Category:**

**For Master's Students:**
- Should NOT primarily focus on publications (though publications are a bonus)
- Academy contribution potential
- Applicant's personal benefit from degree
- Social impact/benefit
- Clarity of proposal articulation

**For Postdocs/Sabbaticals:**
- High impact of work
- Publication venues/quality (not just quantity)
- Current research standing and trajectory
- NRF rating (with caveats about outdatedness)
- Peer references and endorsements

**Red Flags/Negative Indicators:**

1. **Poorly Articulated Proposals**
> "there were applicants who even in the scoring obviously there will be one star candidate but there's some information that's probably missing or not properly articulated in the portfolio"
- Missing educational background information
- Unclear social activity alignment
- Incomplete or vague proposal descriptions

2. **Misaligned Institution Selection**
- Choosing overseas school not aligned with current work
- Poor strategic fit for career development

3. **Low Research Quality Masked by Metrics**
> "just it's just not up to standard... you can just look at the numbers and say h index of 30 this person is a very good when in fact what they are publishing is just it's just not up to standard"
- High H-index with low-quality publications
- Numbers don't reflect actual research caliber

4. **Outdated Credentials**
- NRF ratings that are 5+ years old
- No evidence of current research activity

**How They Weight Different Factors:**

The hierarchy appears to be:
1. **Core proposal quality/innovation** (primary driver)
2. **Applicant-institution fit and quality** (secondary)
3. **Research standing** (for senior categories)
4. **Social impact alignment** (important but secondary to research quality)
5. **Budget** (reviewed last, described as "the last thing")

> "Budget is the last thing actually that I that I that I look at even though it's part of the it's part of the as part of the portfolio."

---

## 5. Views on AI/Technology

**Openness to AI Assistance:**
Freedom does not explicitly reject AI but is cautious and sets clear boundaries.

**What Would Be Delegated (AI/Assistant Tasks):**

1. **Research Quality Digging** (HIGH PRIORITY)
> "I would like to say the research because there's information that needs to be dug out there that I have to dig. And if it's not in the if it's not it is going to be in the pack, they're going to list that they're going to list their papers, which is what people normally do"

- Extracting and compiling publication metrics (bibliometrics)
- Calculating research impact indicators not requested from applicants
- Verifying publication quality and venue impact
- Assessing research output quality beyond what applicants provide

2. **Checklist/Completeness Verification (for Masters & Lower Categories)**
> "I would like there is perhaps just a summary of their academic background... it's almost like checks academic background... it's really maybe just a checklist. Are these things there for this?"

- Confirming all required information is present
- Summarizing academic background
- Verifying institutional fit
- Cross-checking proposal consistency
- Flagging missing or poorly articulated elements

**What Would NOT Be Delegated:**

1. **Core Proposal Evaluation** (MUST REMAIN WITH HUMAN)
> "I still think as a reviewer you still want to read the portfolio"

- Assessing innovation and novelty
- Judging research quality
- Determining star ratings
- Final synthesis and judgment

2. **Disciplinary Expertise Application**
- Evaluating appropriateness of methodology
- Assessing research quality within field context
- Making judgment calls on innovation

**Concerns & Trust Requirements:**

While not explicitly stated as concerns, Freedom's approach implies several requirements:

1. **Need for Expert-Level Assessment**
- Task delegation assumes sophisticated understanding of research quality
- Cannot be just surface-level data extraction
- Requires going "deep" as stated in the interview question: "going deep and doing some preparation"

2. **Disciplinary Specificity**
- Different fields assess publications differently
- Need field-aware bibliometric interpretation
- Statistical/data science disciplines have unique characteristics

3. **Transparency & Feedback**
> "I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group"
- Needs visibility into how recommendations are used
- Wants to know outcomes to calibrate future reviews
- Seeks transparency in scoring aggregation process

**Model Precedent References:**

Freedom references other established processes as models for AI/tech integration:

1. **NRF Rating Panels**
> "I sit on NRF rating panel we actually share scores and then there kind of like moderators where there's kind of a a decision right at the end where I don't want to say one gets forced to to to agree but some moderation where you kind of have to come to a consensus"
- Shows comfort with structured moderation processes
- Not uncomfortable with formalized reviewer calibration

2. **EU/MRC UK Scientific Committee Model**
> "There's something called a scientific committee that actually sits and deliberates on the scoring that actually... they'll come to a consensus that maybe they're funding two or three out of maybe 20 research proposals"
- Appreciates deliberative group processes
- Willing to invest time in consensus-building for high-stakes decisions
- Recognizes this adds burden but provides confidence

**Implicit Trust Markers:**
- Values process transparency
- Wants accountability and feedback loops
- Expects any tool/process to maintain rigor equivalent to peer-review standards
- Comfortable with moderation but requires systematic approach

---

## 6. Suggestions & Ideas

**Major Improvements Suggested:**

**1. Separate Rubric Categories by Master's Type**
> "The master's programs are changing. The rubric is built on a master's by dissertation... So the masters were seen at least in my discipline whether locally or overseas... if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part... definitely you can't be evaluating the masters by dissertation the same way as you would evaluate the masters by coursework... So that so I would like that to be separated."

**2. Implement Reviewer Score Sharing & Moderation Process**
> "I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group... maybe we can calibrate that from this side... reviewers definitely I sit on NRF rating panel we actually share scores and then there kind of like moderators where there's kind of a a decision right at the end where... you kind of have to come to a consensus of some sort"

- Share scores across reviewers of same applications
- Implement formal moderation/consensus process (model on NRF approach)
- Unknown if this exists downstream; should clarify
- Acknowledge burden but emphasize value for high-stakes funding

**3. Standardize Publication Assessment Guidance**
- Specify whether evaluators should use Google Scholar, Scopus, or other sources
- Provide guidance on assessing publication quality
- Move beyond simple H-index counting

**4. Formalize Data Submission Requirements**
- Request bibliometrics in applicant submissions
- Specify what social impact data should be provided
- Request specific publication venue information
- Standardize how applicants present research credentials

**5. Create Feedback Loop for Reviewers**
> "So in terms of the process itself, there's nothing that I think one can sort of like maybe change but I would like to know... what they they desire so that maybe we can calibrate... maybe we can give feedback that there's a set of proposals that really didn't have to go through this stage"

- Share outcomes of funding decisions with reviewers
- Provide aggregate scoring feedback
- Indicate which applicants actually received funding
- Allow reviewers to calibrate future assessments

**6. Caution Against Over-Reliance on NRF Ratings**
> "I would imagine other sort of research organization in South Africa are still using rating but if they use the rating is dwindling... top university... we are not relying on rating. We're relying on peers who write about the individual"

- Don't weight NRF ratings too heavily
- Emphasize referee reports
- Account for rating currency/recency
- Consider peer endorsements as primary metric

**7. Improve Clarity on Sabbatical Expectations**
- Provide more structure for what constit