# Interview Analysis: Frelet De Villiers
**Date:** 2026_01_26 12_54 SAST
**Source:** OMT Discovery Interview with StrideShift (Frelet De Villiers) – 2026_01_26 12_54 SAST – Notes by Gemini.docx
**Transcript word count:** 5655
**Analysis model:** claude-haiku-4-5

# OMT Discovery Interview Analysis: Frelet De Villiers

## 1. Interviewee Role & Background

**Role:** Head of Music Department at University of the Free State; OMT reviewer for postgraduate funding applications

**Duration of Involvement:** Approximately 4-5 years with OMT

**Additional Experience:** Also serves as a reviewer for the National Research Fund (NRF); has been in academia for approximately 12 years; has experience as a UNISA examiner; personally applied for bursaries and funding as a younger academic

**Discipline/Expertise:** Music (Arts discipline); however, also reviews across disciplines (e.g., NRF reviews are not music-specific)

---

## 2. Current Review Process Description

**Process Flow:**

1. **Initial Contact & Commitment:** OMT contacts reviewer to ask if they're available; interviewee prioritizes OMT work due to belief in the organization's mission

2. **Initial Scanning:** Reviews all applicants at once in an initial pass to "get a sense of who the students are" (note: minimum requirements screening is already completed by OMT staff)

3. **Detailed Review Approach:** 
   - Works through applications one by one
   - Randomly selects a candidate to start with
   - Reads through "the questions one by one"
   - Refers to reference letters from supervisors as key information source
   - Compares candidates iteratively as progressing through batch

4. **Scoring & Calibration:**
   - Provides "very honest feedback on what I see in the application regarding merit"
   - Scores immediately upon first reading (influenced by UNISA examiner background of giving first impressions)
   - Does NOT submit immediately; saves and returns later
   - Reviews and adjusts scores after reading all applications to ensure consistency across cohort
   - Identifies top 3 candidates intuitively, then cross-references with numerical scores
   - May revise scores during this calibration phase

5. **Feedback Documentation:**
   - Writes detailed comments explaining scores and recommendations
   - "Beautifies" sentences and refines language after initial scoring
   - Provides specific improvement suggestions for unsuccessful applicants

**Batch Management:**
- Does not review many applications at once
- Explicitly avoids doing multiple applications in a single day session
- Maintains focus and "fresh thoughts"

**Tools/Systems Currently Used:**
- OMT online platform with split-screen functionality (praised as "very user friendly")
- Compares favorably to NRF system which requires managing separate PDF and rubric documents

**Time Commitment:** Not explicitly stated, but implies significant investment in detailed feedback writing

---

## 3. Pain Points & Challenges

**Challenge 1: Rubric Limitations**
- Rubric can be "very restrictive and not really compelling"
- Sometimes things reviewers judge "does not really fit into the rubric"
- Has suggested rubric improvements (though cannot recall specifics during interview)

**Challenge 2: Lack of Fine-Grain Calibration Guidance**
- Unclear scoring thresholds (what constitutes a 3 vs. 4)
- No explicit percentage equivalents provided
- Reviewer develops own internal mental scale

**Challenge 3: Reference Letter Authenticity**
- Cannot determine if letters are genuinely written by referees or drafted by students
- Cannot detect if letters have been processed through AI
- Must take all documents "at face value"

**Challenge 4: Comparing Value Across Disciplines**
- Subjective nature of "value" differs by reviewer framework
- Generic statements about "adding knowledge to existing knowledge" are unhelpful
- Has to evaluate quality across fields where they lack deep expertise

**Challenge 5: Inconsistent Financial Realism**
- Some proposals have inflated budgets (e.g., claiming €35,000 for European travel when actual costs are much lower)
- Difficult to assess feasibility of research plans (e.g., claiming 4 articles, 3 conferences, 2 books during postgraduate study)
- Red flags about whether applicant genuinely needs to be overseas vs. seeking trip

**Challenge 6: Benchmarking Across Years**
- Cannot remember previous year's cohort quality/standards
- Does multiple reviews across different organizations, making comparison difficult
- Essentially benchmarks relative to current cohort only

**Challenge 7: Process Consistency Concerns**
- Worried about inconsistent rubric application across reviewers
- Notes that what seems valuable to one reviewer may not be to another
- No way to ensure all reviewers start from same calibration point

---

## 4. What They Value in Applications

**HIGH VALUE CRITERIA:**

**A. Motivation Letter / Personal Statement:**
- "The motivation letter tells me a lot about the person itself"
- Look for evidence of resilience and agency, NOT victim mentality
- Differentiates between: (1) people who blame circumstances vs. (2) people who acknowledge circumstances but demonstrate what they've achieved despite them
- Values honesty about background paired with demonstrated initiative ("I come from this background but I did this with what I have already")
- Writing style reveals authenticity; inconsistency between motivation and proposal can suggest outsourced writing

**B. Supervisor Reference Letters:**
- Quality of reference letter is crucial indicator
- Detailed, substantive letters (multiple paragraphs) indicate supervisor's genuine investment
- Generic, short letters (1-2 paragraphs) raise concerns
- Personally experienced writing reference letters, so understands effort required for good ones

**C. Research Proposal/Project Viability:**
- "Very important" - "the openers must get value for their money"
- Internal coherence: Does title align with aims? Do aims align with methodology? Does methodology align with research question?
- "Golden thread" within proposal itself is essential
- Realistic scope and deliverables (red flag: claiming 4 articles + 3 conferences + 2 books)
- Feasibility assessment: Can this actually be accomplished?

**D. Demonstrated Value/Impact:**
- Must articulate specific, meaningful value (not generic "contributing to knowledge")
- Examples of strong value statements: "community program that will uplift 20 community members"
- Value judgments are subjective but should be clear and defensible
- OMT's significant investment requires confidence funds will be used effectively

**E. Academic Record (LOW priority):**
- "Not that important to me"
- Acknowledges contextual factors: students may have "horrible time" due to circumstances beyond their control, unsupportive lecturers, or different institutional grading scales
- Cautions against penalizing students from institutions with different marking standards
- Some variation between institutions makes direct comparison unreliable

**RED FLAGS:**

1. **Unrealistic Budgets:** Inflated costs for travel/resources suggest either dishonesty or lack of due diligence
2. **Unnecessary Travel:** Proposing to go abroad for things that could be done online (e.g., meetings)
3. **Victim Mentality:** Complaining about circumstances rather than demonstrating agency
4. **Unrealistic Deliverables:** Overpromising outputs that cannot possibly be achieved in timeframe
5. **Disconnected Proposal:** Aims/methodology/research question not aligned; lacks internal "golden thread"
6. **Generic Value Statements:** Vague claims that apply to any research project
7. **Weak Reference Letters:** Short, generic, uninformed letters suggest lack of supervisor engagement

**WEIGHTING/PRIORITY:**
1. Proposal quality & feasibility (most important)
2. Motivation/personal statement & resilience (very important)
3. Quality of reference letters (important)
4. Academic record (least important)

---

## 5. Views on AI/Technology

**Overall Attitude:** Cautiously skeptical; concerned about AI's role in review but pragmatic about current limitations

**Specific Concerns:**

1. **Authenticity Problems:**
   - "With AI these days even the reviewer or the reference referee can also put that through AI and I mean you can't even see if it's a student writing or whatever"
   - Cannot distinguish AI-generated from human-written text in reference letters
   - Impossible to verify authenticity of any document submitted

2. **Value of Automated Assistance:**
   - Does not believe an AI assistant could meaningfully screen for "red flags" in her work
   - "If it's a red flag for the assistant, it's not necessarily a red flag for me because we will never be on the same level of thinking of conceptualization of experience"
   - Concerned about false positives: AI might flag things as inconsistent that aren't, or miss context

3. **What Automation COULD Help With:**
   - Checking minimum requirements (already being done by OMT)
   - Possibly doing basic fact-checking on budgets/costs
   - Organizing/presenting information more clearly (though current OMT system already good)
   - Expert consultation in unfamiliar disciplines (if expert is human, not AI)

4. **What Should NOT Be Automated:**
   - Judgment calls on value and merit
   - Evaluation of proposal coherence
   - Assessment of applicant motivation/character
   - Final scoring/recommendations

5. **Procedural Concerns:**
   - Worried about time delays if relying on assistant: "they don't always stick to your timeline. So now I have to wait for the assistant to come back to me and in the meantime I would have been finished already"
   - Values autonomy and control over process
   - Self-describes as "perhaps I'm a control freak"

6. **Philosophy on Trust:**
   - "If they lie to me it's on their black book" - takes philosophical approach that she can only review what's in front of her; if information is false, responsibility is on applicant
   - This extends to accepting AI-generated text: since it's impossible to detect, must treat all submissions as authentic

7. **Preference for Human Expertise:**
   - "If I'm not sure about something, I will go and ask an expert in a certain field. I will not trust an assistant."
   - Values personal, expert consultation over algorithmic support

---

## 6. Suggestions & Ideas

**What Should Be Changed:**

1. **Rubric Improvements:**
   - Current rubric sometimes "restrictive and not really compelling"
   - Reviewer considers values outside rubric that should potentially be incorporated
   - Offered to provide written feedback on rubric if sent specific version
   - Implied need for clearer scoring guidance (what distinguishes 3 from 4)

2. **System Strengths to Maintain:**
   - "The Openheimer's website is really very user friendly because you can have the split screen and everything is really available"
   - Explicitly praised compared to NRF system
   - No requests to change current OMT platform interface

3. **Process-Level Suggestions:**
   - Continue using multiple reviewers (different values/frameworks mean plurality is valuable)
   - Consider providing pre-review calibration session for reviewers (though not explicitly suggested, implied by benchmarking challenge)
   - Ensure feedback is detailed and specific to help applicants improve for future applications

4. **Assistance with Unfamiliar Disciplines:**
   - When reviewing outside her expertise, could benefit from disciplinary expert consultation
   - But would prefer to initiate this herself rather than have system mediate it
   - Does not need AI to flag when she's out of depth; recognizes this herself

5. **Information Organization:**
   - Current system already solves this well with split-screen
   - Unlike NRF's fragmented document approach
   - No improvements needed in this area

**What Should NOT Change:**

- Core review responsibility and judgment
- Final recommendation/scoring authority
- Detailed feedback-writing process
- Applicant evaluation criteria

---

## 7. Key Direct Quotes

**Quote 1: On OMT's Mission and Impact**
> "I really believe in the Open Trust. So I really want to make time for them because I I really think they're also gracious in their funding. And some of our students have also received funding and it really really makes a difference. One of our students have received it now as well and if it wasn't for them this person just couldn't do her M's degree you know so they are really setting students on their path for life."

*Context: Explains her motivation for investing significant time in OMT reviews despite other demands*

---

**Quote 2: On Weighing Proposals vs. Academic Records**
> "The academic record is not that important to me because there are circumstances um I know from our own students they just have the most horrible here and just nothing works out for them or they have a lecturer that that doesn't fancy them and they get lower marks you know so academic record for me and then also I mean if you for example music um what we have in our module for say example music theory is not the same as UC has so now as oh all our students usually get 80 and now I get someone that gets 60 so this is a bad candidate so I I the academic record I must say that is the least important for me."

*Context: Explains her holistic approach to candidate evaluation and refusal to over-weight GPA*

---

**Quote 3: On Identifying Resilience vs. Victimhood**
> "When you look at the motivation you can see when they are just um complaining if that makes sense. You you get some people and they will say I come from this very bad background and I don't have this and I don't have that. Um then you get other people who say I come from the background but I did this with what I have already and I want to um achieve this."

*Context: Describes her ability to differentiate between applicants who use circumstances as excuses versus those who demonstrate agency*

---

**Quote 4: On OMT's Fiduciary Responsibility**
> "Is there really value in the study if they've done that so well? But you I'm not really looking between a connection between the motivation and the proposal. Um, the openers must get value for their money. So I cannot say oh this is a wonderful project and then nothing can come from it. So is it really viable? Is it really something that can work? Are sometimes these people they say they are going to have four articles and three conferences and two books. I mean it's just not possible. So when you you look at the proposal, you can also see this person is really so far removed from reality and then are they really going to to keep their promise and you know because it's it's a lot of money that they really give away a lot of money."

*Context: Explains her assessment criteria and concern for due diligence with large funding amounts*

---

**Quote 5: On Red Flags in Budget Proposals**
> "I have this preposterous or or what is it like exuberant amounts and then I know a a ticket to um you know uh Europe is definitely not 35,000 rand and I know a hotel or a whatever um will not cost that. So if if they really inflate the prices like that that is a red flag for me. And then when I see that I will be more cautious when I look at the value and the proposal and things like that."

*Context: Concrete example of how she identifies dishonesty or lack of due diligence in applications*

---

**Quote 6: On Reference Letters as Trust Signals**
> "I was also asked a few times to to write reference letters. So I know what it feels like. You really want this person to get something you will really put everything in that. So that's also for me important."

*Context: Shows how her own experience informs her interpretation of reference letter quality as indicator of supervisor engagement*

---

**Quote 7: On AI and Authenticity Detection**
> "And especially with AI these days even the reviewer or or the you know the reference referee um can also put that through AI and I mean you can't even see if it's a student writing or whatever. So um yeah you you have to believe what you get and you also have to believe the facts that you get. I mean so I I always have I have the philosophy um that is on their black book. If if they lie to me it's on their black book."

*Context: Pragmatic stance on inability to verify authenticity, combined with philosophical acceptance of risk*

---

**Quote 8: On Why Automation Cannot Replace Expert Judgment**
> "Well, once again, if it's a red flag for the assistant, it's not necessarily a red flag for me because we will never be on the same level of thinking of conceptualization of experience. Um the the assistance we need is the people that that go through applications and see that minimum requires requirements are are met."

*Context: Clear articulation of why AI/automated assistance is limited; only appropriate for clerical tasks already being done*

---

**Quote 9: On Subjectivity in Value Assessment**
> "You see that is now why hopefully I I think they have more than one reviewer for the whole process. Now it's not only one. So once again what is valuable for me in a study is not valuable for you in a study. So it all depends on your own um reference your own framework."

*Context: Acknowledges fundamental subjectivity while endorsing multiple reviewers as solution*

---

**Quote 10: On Feedback as Essential for Applicant Development**
> "I had um applied for many, you know, burseries and applications things when I when I was younger as well, and it was so sad never to get really feedback because what what must I do? Why didn't I get it? What must I do better next time? So, I'm very I'm really very um you know um de detailed in my comments and what what why is this not the best application or what must be improved?"

*Context: Personal motivation for writing detailed feedback; pays forward her own experience of rejection without explanation*

---

## 8. Unique Insights

**Insight 1: The "Black Book" Philosophy**
De Villiers articulates a distinctive epistemological stance