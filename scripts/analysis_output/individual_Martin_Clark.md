# Interview Analysis: Martin Clark
**Date:** 2026_01_15 13_55 SAST
**Source:** OMT Discovery Interview with StrideShift (Martin Clark) – 2026_01_15 13_55 SAST – Notes by Gemini.docx
**Transcript word count:** 6520
**Analysis model:** claude-haiku-4-5

# OMT Discovery Interview Analysis: Martin Clark

## 1. Interviewee Role & Background

**Role in OMT Review Process:**
Martin Clark is an adjudicator/reviewer for the Oppenheimer Memorial Trust, responsible for evaluating postgraduate scholarship applications across different academic levels (Master's, PhD, sabbatical/postdoctoral).

**Duration of Involvement:**
Approximately 3-4 years of experience as an adjudicator.

**Discipline/Expertise Area:**
- Geologist with an Earth Science BSc
- Based at the University of the Free State in Bloemfontein, South Africa
- Originally from Canada
- Studied in Germany (reference to "pedantic reviews")
- Runs a laboratory and manages students
- Self-describes as having broad knowledge across earth sciences but acknowledges gaps in biology and other specialized fields

---

## 2. Current Review Process Description

**Step-by-Step Process:**

1. **Initial engagement:** Receives a link to all publications on the OMT website
2. **Application parcellation:** Separates applications by level (Master's, PhD, postdoc/sabbatical) to calibrate expectations of excellence appropriately for each category
3. **Time allocation:** Allocates time carefully to avoid adjudication fatigue, typically reading no more than 5-10 applications in one sitting
4. **Fresh-minded review:** Reads each application with a fresh mind to properly value both the proposal and the proposing person
5. **Rubric completion:** Completes a rubric using a 1-4 point scale across specific categories
6. **Final assessment:** Provides a final 5-point scale assessment of the candidate
7. **Contextual commenting:** Adds comments explaining nuanced decisions, particularly where perception of grade conflicts with rubric requirements

**Tools/Systems:**
- OMT website/portal for accessing applications
- Standard rubric provided by OMT
- AI tools (personal use): Uses AI to verify logic and validate decision-making approaches
- Manual processes: Reading, note-taking, deep research for unfamiliar subjects

**Time Spent:**
- Allocates time strategically to read 5-10 applications maximum per sitting
- Views this as a pro bono commitment undertaken in addition to primary work responsibilities
- Notes that pressure to move faster through applications on some days can impact quality

---

## 3. Pain Points & Challenges

**Rubric Rigidity:**
- The rubric categories are "rather rigid" and don't accommodate disciplinary differences
- Specific metrics (e.g., Golden Key Society membership) disadvantage otherwise excellent candidates who lack access to resources
- Binary scoring system (1-4 point scale, then 5-point final assessment) lacks sufficient granularity to make meaningful distinctions

**Adjudication Fatigue:**
- Risk of fatigue affecting quality if reviewing too many applications consecutively
- Trade-off between speed and thoroughness when there's time pressure

**Subject Matter Expertise Gaps:**
- Difficult to evaluate applications in disciplines outside geological expertise (e.g., butterfly subspecies in biology)
- Requires significant "deep diving" to understand niche topics, consuming additional time
- Cannot always appreciate specialized significance within unfamiliar fields

**Edge Cases and Undefined Scenarios:**
- Applications that don't fit neatly into defined boxes are challenging
- Tension between local/community-focused projects (showing passion) and metrics requiring national/global significance
- Difficulty weighing variables when they're not equally weighted or when circumstances are novel (e.g., candidates with 20 years of industry experience vs. typical age progression)

**Benchmarking Across Institutions:**
- Significant variation in grading standards between institutions (e.g., "C" at UCT vs. "B" at Nelson Mandela University)
- Misalignment between academic grades and how candidates express themselves in applications
- Limited ability to contextualize these differences within the current system

**Spelling/Presentation Issues:**
- Personal bias toward penalizing spelling mistakes (German education background) conflicts with fairness—not all applicants have access to spell-check technology
- Must constantly weigh presentation quality against actual capability and potential

**Limited Scoring Granularity:**
- 4-point rubric scale (potentially divided into 25% increments) insufficient to distinguish between candidates at similar levels (e.g., difference between 75% and 100%)
- 5-point final scale similarly restrictive

**Bias and Frame of Reference:**
- Acknowledged that adjudicators naturally find it easier to evaluate applications following career paths similar to their own
- Difficulty understanding life experiences different from own (e.g., township backgrounds) despite efforts to remain objective

---

## 4. What They Value in Applications

**Key Criteria:**

1. **Passion (prioritized over cleverness)**
   - Values passion as difficult to teach and highly indicative of potential
   - Looks for evidence of genuine enthusiasm for the work

2. **Positive Outlook for Change**
   - Seeks evidence of commitment to local, continental, or global change
   - Indicates future leadership potential

3. **Aspiration**
   - Key metric for identifying next generation of world leaders in respective fields

4. **Circumstance Consideration**
   - Takes into account applicant's background, opportunities, and life circumstances
   - Evaluates "excellence relative to what is possible with the time the candidate has had"
   - Values tenacity and evidence of overcoming adversity

5. **Character and Life Path**
   - Attempts to understand the applicant as a person through available materials
   - Values non-linear paths to excellence
   - Recognizes that different life experiences can lead to "true excellence"

6. **Project Feasibility**
   - Always examines proposed budgets for alignment with project scope
   - Flags excessively high budgets that don't match project needs
   - Flags extremely modest budgets that may indicate unfeasibility
   - Views feasibility as critical even for potentially transformative work ("cure for cancer")

7. **Clarity and Articulation**
   - Values clear expression of ideas and projects
   - Looks for applicants who can articulate where their work fits within broader contexts (especially important at postdoc/sabbatical level)

**Red Flags:**

1. **Spelling and presentation errors** (though tempered by consideration of access to resources)
2. **Grades-to-presentation misalignment** across institution types
3. **Excessively high or extremely modest budgets** suggesting lack of realistic planning
4. **Unclear project significance** or inability to articulate relevance
5. **Lack of evidence of passion or aspiration**

**What Makes Applications Stand Out:**

- Clear evidence of passion combined with feasible, well-thought-out projects
- Applicants who overcome disadvantage to pursue ambitious goals
- Projects with explicit connection to local or global change
- Clear articulation of how work fits within discipline (especially for advanced degrees)
- Realistic budget planning showing understanding of project scope
- Multiple "dots" in the application that create a coherent picture of the person and their potential

**How They Weigh Different Factors:**

- Uses a balancing approach across multiple non-equally-weighted variables
- Prioritizes passion over academic credentials or presentation quality
- Adjusts expectations based on level (Master's vs. PhD vs. Postdoc)
- Considers "excellence relative to circumstance" rather than absolute metrics
- Willing to provide commentary justifying ratings that don't align with rubric scores when compelling mitigating factors exist
- Views final decision as judgment call requiring holistic consideration of multiple factors

---

## 5. Views on AI/Technology

**Overall Attitude:**
Enthusiastically positive. Martin Clark states "I love AI" and believes it "can help in almost any element" of the adjudication process. Views AI as a tool to improve accuracy, coverage, and fairness.

**Current AI Use:**
- Uses AI to verify and validate his own logic in real-time during adjudication
- Employs AI to test how his evaluations might be perceived by different stakeholders
- Uses AI for project management on a separate system
- Familiar with the concept of training AI on personality profiles of people one admires to receive personalized feedback

**Specific Uses Envisioned:**

1. **Logic Validation**
   - Testing whether his reasoning is sound and well-articulated
   - Evaluating how to best present nuanced evaluations to decision committees
   - Helping articulate appreciation of candidates when there's tension between rubric scores and perceived merit

2. **Subject Matter Expertise Supplementation**
   - Data scraping and research assistance for unfamiliar fields
   - Acts as "CliffsNotes" equivalent: providing summaries and contextual information on specialized topics (e.g., caterpillar diseases, butterfly subspecies)
   - Enables verification of sources and significance claims in unfamiliar disciplines
   - Reduces need for time-consuming "deep diving" research

3. **Data Management and Analysis**
   - Could assist with data scraping from online sources
   - Potential for verifying sources and information
   - Could help organize and cross-reference application materials

4. **Presentation and Communication**
   - Helping present candidate value more effectively
   - Assisting in articulating nuanced decisions in ways decision-makers will understand
   - Supporting clearer communication of reasoning for grade decisions

**Concerns and Caveats:**

1. **Uneven Implementation**
   - Risk that different adjudicators will use AI differently, creating inequitable evaluation processes
   - One adjudicator's AI use may not equal another's, potentially affecting consistency
   - Questions about training and standardization across adjudicators

2. **Accuracy Issues**
   - AI can "give wrong answers" depending on implementation level
   - Reliability concerns with certain AI systems

3. **Technical Infrastructure**
   - Connection issues to AI servers
   - Potential downtime or accessibility problems
   - Dependence on system stability

4. **Transparency and Trust**
   - Mentions "comfortability argument" that would need to be made regarding AI use
   - Implicit concern about decision-makers and funders accepting AI-assisted adjudication
   - Questions about whether AI recommendations should be binding or advisory

5. **Scope Limitations**
   - Not clear whether AI could help with rubric interpretation or scoring decisions
   - Expresses uncertainty about AI's role in the core judgment-making aspects

**What He Would NOT Want Automated:**

- The fundamental judgment call of adjudication
- Core decision-making about candidate merit
- Weighing passion against credentials
- Contextual evaluation of excellence relative to circumstance
- The human interpretation of life circumstances and potential

Implicit in his comments: AI should support, verify, and enhance human judgment—not replace it.

---

## 6. Suggestions & Ideas

**Primary Recommendation: Redefine "Excellence" More Flexibly**

> "The definition of what they want to fund can be articulated in a manner in which I don't necessarily have explicit boxes."

- Move away from rigid, explicitly boxed categories
- Provide broader definitional framework allowing discipline-specific and context-specific interpretation
- Allow adjudicators to apply professional judgment within a clear but flexible framework
- Should define *what* the foundation is trying to fund rather than *how* to identify it

**Specific Improvements:**

1. **More Sensitive Definition of Excellence**
   - Accommodate circumstances like tragedy, tenacity, and unconventional paths
   - Less explicitly defined categories, particularly around academic performance
   - Account for excellence that emerges from disadvantage or non-traditional backgrounds
   - Recognize that life paths "different from my own" can lead to "true excellence"

2. **Discipline-Specific and Level-Specific Rubrics**
   - Rubric categories could "change based off of discipline or category"
   - Adjust expectations according to field-specific norms
   - Different emphasis for Master's, PhD, and postdoctoral applications

3. **Address Equity Issues in Existing Metrics**
   - Review metrics like Golden Key Society that require financial resources to access
   - Ensure excellence indicators aren't biased toward privileged backgrounds
   - Acknowledge that excellent candidates may not have access to certain traditional markers

4. **Improved Scoring Granularity**
   - Consider finer-grained scoring system (mentioned that 4-point scale with 25% increments is insufficient)
   - Allow for better discrimination between candidates at similar levels

5. **Institutional Benchmarking Guidance**
   - Provide framework for understanding grade variations across institutions
   - Help adjudicators contextualize grades from different universities
   - Guidance on how to interpret whether grades reflect institutional standards or individual performance

6. **Formalize Contextual Commentary**
   - Explicitly encourage and value the contextual comments adjudicators provide
   - Make clearer that these comments influence final funding decisions
   - Provide templates or frameworks for important contextual factors (circumstance, tenacity, etc.)

**Features Desired in New System:**

1. AI-assisted subject matter expertise lookup for unfamiliar fields
2. Logic validation tools to test reasoning and articulation
3. Presentation/stakeholder analysis features (how will decision-makers perceive this evaluation?)
4. Budget feasibility checking tools
5. Source verification capabilities
6. Data scraping and research assistance for niche topics

**Priorities for Change (implicit):**

1. **Highest Priority:** Flexibility in excellence definition to prevent exclusion of deserving candidates
2. **High Priority:** AI tools for subject matter expertise gaps
3. **Medium Priority:** Improved scoring granularity
4. **Medium Priority:** Institutional benchmarking guidance
5. **Ongoing:** Training and consistency in adjudicator approaches

---

## 7. Key Direct Quotes

### Quote 1: On Adjudication Approach and Fresh Perspective
**"Every application you read, you need to read with a fresh mind. You need to find a way to value what is being proposed but also value the person who is proposing that."**
- *Context: 00:03:16*
- *Significance: Captures the holistic, person-centered approach Martin takes and the importance of cognitive freshness in fair evaluation*

### Quote 2: On Valuing Passion Over Credentials
**"I always value passion higher than I value cleverness. You can always teach passion but you can't build passion always."**
- *Context: 00:08:11*
- *Significance: Reveals core values in assessment; prioritizes intrinsic motivation and drive over academic credentials, which can be acquired*

### Quote 3: On the Golden Key Problem—Equity in Metrics
**"One metric for excellence of a candidate is being part of the golden key society...you have to pay to be part of a golden key society. So I do my best to see applicants who argue, well, I've been invited, but I can't afford to pay as evidence of excellence."**
- *Context: 00:04:25*
- *Significance: Illustrates how explicit rubric items can inadvertently disadvantage excellent candidates from lower socioeconomic backgrounds; shows adjudicator attempts to work around this limitation*

### Quote 4: On the Fundamental Challenge of Weighing Variables
**"Adjudication involves balancing a set of variables that are not always weighed equally, which can sometimes constrain how well I can adjudicate an application that is not clearly excellent across all metrics."**
- *Context: 00:10:33*
- *Significance: Articulates the core tension in the current system where excellence isn't one-dimensional but the rubric forces linear scoring*

### Quote 5: On Excellence in Context—Disadvantage and Local Impact
**"I might review an application of someone from a disadvantaged background and that disadvantaged background would articulate how they would want to make a change in their community...but the grading metrics asks me to evaluate if the project could have national or global significance."**
- *Context: 00:10:33-00:11:57*
- *Significance: Reveals a fundamental misalignment between funding criteria (global significance) and valuable excellence indicators (local impact from disadvantaged backgrounds); highlights edge cases the system doesn't accommodate*

### Quote 6: On Life Experience and Multiple Pathways to Excellence
**"Excellence can be measured whether it is from a lay person's perspective or from an absolute a disciplined leader...there are backgrounds and life paths that are different than my own and it does not mean that a different life path or a life path that appears to be less developmental or less of a straight line towards excellence should be precluded from excellence."**
- *Context: 00:18:45*
- *Significance: Challenges implicit assumptions that linear career paths are markers of excellence; advocates for recognizing diverse routes to achievement*

### Quote 7: On the Bluntness of Scoring
**"Five points isn't always enough to discriminate one against the other."**
- *Context: 00:11:57*
- *Significance: Directly critiques the scoring system's granularity; suggests current scale cannot capture important distinctions between candidates*

### Quote 8: On How AI Could Help with Subject Gaps
**"When I adjudicate I have an earth science BSc which means I should be able to consider topics on a broad range of things. But I can tell you that with a first year, second-year biology I might not appreciate diseases that are specific to a caterpillar species...I could use an AI to better rate what would that mean within a certain degree of significance."**
- *Context: 00:31:08-00:32:33*
- *Significance: Identifies specific, practical AI use case—providing CliffsNotes-style summaries to enable fair evaluation of unfamiliar topics without requiring deep expertise*

### Quote 9: On Desired Change—Flexibility Over Boxes
**"I think that the definition of what they want to fund can be articulated in a manner in which I don't necessarily have