# Interview Analysis: Maureen De Jager
**Date:** 2026_01_22 13_25 SAST
**Source:** OMT Discovery Interview with StrideShift (Maureen De Jager) – 2026_01_22 13_25 SAST – Notes by Gemini.docx
**Transcript word count:** 7004
**Analysis model:** claude-haiku-4-5

# OMT Discovery Interview Analysis: Maureen De Jager

## 1. Interviewee Role & Background

**Role in OMT Review Process:**
- Assessor for creative arts submissions, primarily fine art
- Reviews applications for postgraduate scholarships (Master's and PhD level)
- Makes recommendations on funding decisions

**Duration & Experience:**
- Approximately 3 years assessing OMT applications
- Also sits on a national DHET (Department of Higher Education and Training) sub-panel for review of creative outputs
- Personal experience as OMT-funded PhD candidate (Kingston University, 2015-2019)

**Discipline/Expertise:**
- Fine art practitioner and educator at Rhodes University (since 2002)
- Has also reviewed performance-based and graphic design submissions
- Specializes in practice-based creative research degrees
- Recently served as Deputy Dean in Humanities (involved in recruitment and selection processes)

---

## 2. Current Review Process Description

**Overall Approach: Comparative & Iterative**

Maureen uses a distinctive **comparative assessment method** rather than linear evaluation:

1. **Initial Overview Phase:**
   - Skims through all applications to get "an overall lay of the land" before detailed assessment
   - Reviews portfolios and materials to form first impressions
   - Establishes comparative context across the cohort

2. **Detailed Evaluation Phase:**
   - Applies rubric-based scoring (1-4 scale) with specific criteria
   - Answers rubric questions systematically
   - Tests impressions against explicit criteria

3. **Retrofit/Adjustment Phase:**
   - Frequently returns to adjust final scores if calculated rating doesn't match strength of submission
   - Uses initial impression (often reinforced through detailed review) to override mechanical scoring
   - Described as "retrofit" - tweaking scores to reflect actual quality assessment

**Tools & Systems Used:**
- Rubric with 1-4 scoring scale
- Written application materials (motivation letters, CVs, academic transcripts)
- Portfolio documents (images/work samples)
- Reference letters
- Institutional knowledge/external databases for verifying program quality

**Time Investment:**
- Not explicitly quantified, but indicated as significant
- Plans to spend 3 days with DHET sub-panel to review creative outputs in detail
- Concerned about avoiding "sitting for like 4 hours on each application"

---

## 3. Pain Points & Challenges

**Key Frustrations:**

1. **Interpreting Academic Achievement:**
   - Difficulty when candidates' final grades don't directly relate to proposed study
   - Must sift through partial results or in-progress studies
   - Relies on tacit knowledge of institutional rigor: *"I rely on my knowledge of this sector... a 75% from a particular institution is probably worth more than a 75% from another institution"*
   - Labor-intensive to verify actual program quality

2. **Portfolio Contextualization Gap:**
   - Portfolios often lack annotation or explanation
   - *"If it's just a bunch of images and there's very little contextualization of what was informing the making of the work... it's also a little bit difficult to kind of get a handle on it"*
   - Critical information about artistic practice, methodology, and development is missing

3. **Problematic Rubric Questions:**

   **a) "Significance to South African Society" Question:**
   - Applicants struggle with this; answers often "unpersuasive"
   - Fundamental mismatch: creative practice is exploratory and emergent, not predetermined
   - *"Practice in its nature is exploratory... it's very difficult to know before you've even set out on this journey where you're going to end up... in a lot of cases it's a bit of a thumb suck"*
   - Generates generic, "floaty" responses disconnected from actual practice

   **b) "Personal Motivation" Question:**
   - Misalignment between question framing and assessment criteria
   - Question asks for background/interests (narrative), but rubric seeks evidence of "values and priorities"
   - *"You're looking for evidence of values and priorities... sometimes the personal motivation is framed more as just a narrative... you're being asked to look for something that isn't necessarily in evidence"*
   - Space constraints mean candidates often only cover background, not deeper values

   **c) "Plan to Achieve Vision" Question:**
   - Described as "an odd question"
   - Confusing for both applicants and reviewers
   - Difficult to map when degree programs have specific benchmarks and timelines
   - Conflates generic life planning with specific academic planning

4. **Overseas Study Assessment:**
   - Tricky to verify whether comparable programs truly exist locally
   - *"As a reviewer, one doesn't necessarily know whether there are comparable degree programs... you're relying very much on the applicant saying there is nothing like it rather than your own kind of knowledge"*
   - Limited evidence base for claims of program uniqueness

5. **AI-Generated Content Deception:**
   - Increasing difficulty distinguishing authentic applicant voice from AI-generated text
   - *"With AI you know it's very easy to write a convincing proposal for a research project... and you're sort of wondering how much of that is really this the student being able to articulate"*
   - Creates misalignment between articulation and actual ability
   - Can be difficult to detect even with writing samples, as institutions may miss AI generation

6. **Reference Letter Reliability:**
   - References often unhelpful or biased (e.g., from intimate partners)
   - Only useful when reviewer personally knows referees as strong/credible
   - Generic or unsubstantiated praise common

7. **Inconsistency Detection:**
   - Multiple submissions make it difficult to spot contradictions
   - *"Sometimes you know you don't notice the inconsistencies particularly if you're dealing with a lot of submissions"* (mentioned by other reviewers)

---

## 4. What They Value in Applications

**Critical Success Factors:**

1. **Portfolio & Practical Excellence (Highest Priority):**
   - *"It's actually quite critical if one's looking at a student wanting to go into a creative degree... is their portfolio and that evidence of practical excellence"*
   - Described as "highly detrimental" if absent
   - More reliable indicator than written articulation due to AI risk
   - Preferred with artist annotation explaining context and development

2. **Coherence & Internal Consistency:**
   - *"There needs to be a kind of coherence where the way that the person articulates their work aligns with the work aligns with how they see it developing"*
   - If coherent, automatically persuasive; if not, unconvincing regardless of cohort standing
   - Alignment between:
     - How they describe their work
     - Actual work quality shown in portfolio
     - How they envision development

3. **Academic Transcript (Contextual Assessment):**
   - Looked at with nuance, not mechanically
   - Maureen considers:
     - Performance in relevant subjects (e.g., fine art practice grades)
     - Institutional context and rigor
     - Progress trajectory

4. **Strong Motivation Letter:**
   - Demonstrates understanding of specific program
   - Shows grounded reasoning (not generic)
   - Reveals ability to articulate thinking

5. **Evidence of Realistic Planning:**
   - For overseas study: genuine evidence program is unavailable locally
   - Clear understanding of what they'll build on or develop
   - Specific rather than aspirational

**Red Flags:**

1. **Academic-Portfolio Mismatch:**
   - *"Very clear misalignment between the academic record and the portfolio"*
   - Strong grades but weak portfolio signals doubt about technical/conceptual sophistication
   - May indicate previous support or AI use in academic work

2. **Generic, Ungrounded Responses:**
   - Floating statements about societal benefit ("we all need nice things to look at")
   - Lack of specific engagement with personal practice
   - Absence of disciplinary grounding

3. **Missing Information:**
   - No portfolio (absolutely critical)
   - Incomplete academic records
   - Vague or missing motivation

4. **Weak References:**
   - References from non-credible sources
   - Lack of professional supervisor perspective
   - Generic praise without specifics

**Weighting & Decision Factors:**

Maureen applies **contextual strictness** based on:
- **Level of study:** More forgiving with Master's applicants than PhD candidates
- **Funding type:** Stricter with expensive overseas study due to cost
- **Application stage:** For year 2+ funding, requires evidence of year 1 progress
- **Candidate pool:** Bench marks against internal standard of what constitutes a "persuasive, coherent application"

---

## 5. Views on AI/Technology

**Overall Stance: Cautious Skepticism with Limited Specific Use Cases**

**General Distrust:**
- *"I'm also like probably of the generation that is like a slightly distrustful of AI"*
- Direct negative experience: *"Deputy dean in humanities sat on lot of recruitment and selection processes which relied... AI was used as a kind of screening mechanism and then there were these AI generated summaries of the applicants which kind of didn't help"*

**Core Concern - Interpretative Nature:**
- *"I think it would be immensely helpful, but I don't know how it's possible because so much of this is interpretative"*
- Fundamental problem: creative assessment requires human judgment that can't be systematized
- *"Creative practice is such a tricky thing... there's not a golden standard of how to look at it"*

**Specific Concerns:**

1. **Loss of Professional Judgment:**
   - Risk of over-reliance on automated assessment for inherently interpretative decisions
   - Potential to short-circuit necessary human engagement

2. **Applicant Gaming:**
   - *"One doesn't want a situation where applicants start being quite instrumentalist in how they're answering things in anticipation of an AI process... they start saying things about their values so that the values are there"*
   - Fear that knowing about AI screening will cause inauthentic responses

3. **Hidden AI in Submissions:**
   - Difficulty detecting AI-generated content
   - *"At their institution it wasn't picked up that this is AI generated"*

4. **Poor Experience with AI Summaries:**
   - AI-generated summaries of applicants in recruitment didn't provide useful information

**What She Would Accept - Limited Administrative Tasks:**

1. **Initial Filtering/Screening (SUPPORTED):**
   - *"As an initial filtering or screening, yes, absolutely"*
   - Checking for missing documents
   - Verifying required materials submitted
   - Checking basic compliance with criteria
   - *"If somebody's documents aren't all there... if they haven't answered all the questions, if they don't meet criteria"*

2. **AI-Generated Summaries (CONDITIONAL):**
   - *"AI generated summaries of particular things which would make it kind of like so there's a little summary and then you can read the lengthier narrative"*
   - For lengthy motivation narratives (summary + full text)
   - For reference letter summaries to extract key points

3. **Program Information Research (HELPFUL):**
   - *"Things around students' motivations to study internationally... is there information about the program that could be... information can be made available that would help one to assess the merits of that program"*
   - Compiling information about international programs
   - Helping verify whether comparable local options exist
   - Flagging whether claimed uniqueness is accurate

4. **Inconsistency Flagging (POTENTIALLY USEFUL):**
   - AI identifying contradictions between different application sections
   - Spotting discrepancies reviewers might miss with large volumes

**What She Would NOT Accept:**

- Core assessment/scoring by AI
- Filtering based on subjective quality judgment
- Replacing human evaluation of portfolio or conceptual coherence
- Any system that reduces time spent on serious engagement with applications

**Requirements for Trust:**
- Process must maintain transparency about what AI is doing
- Clear distinction between administrative vs. interpretative tasks
- Demonstration that tool adds value without compromising judgment
- Safeguards against applicant response distortion

---

## 6. Suggestions & Ideas

**Immediate Improvements (Non-AI):**

1. **Redesign Problematic Rubric Questions:**
   - Reframe "Personal Motivation" question to explicitly ask for evidence of values/priorities rather than narrative background
   - Better align question wording with assessment criteria
   - Clarify distinction between generic life planning and specific academic planning

2. **Portfolio Requirements Enhancement:**
   - Make portfolio annotation/contextualization mandatory
   - Request artist statement explaining:
     - Disciplinary focus (sculpture, print-making, etc.)
     - How practice is developing
     - Alignment with interests/concerns to explore
   - *"A little annotation or something where the student speaks to what's in the portfolio"*

3. **Reference Letter Requirements:**
   - Require at least one reference from current or prospective research supervisor
   - Ensures professional perspective rather than personal bias
   - *"At least one of the reference letters should be by a current or prospective supervisor, research supervisor"*

4. **Significance Question Reframing:**
   - Acknowledge exploratory nature of creative practice
   - Ask for grounded vision rather than predetermined societal contribution
   - Make question applicable to Master's as well as PhD candidates

**Technology-Enabled Improvements:**

1. **Leverage AI for Administrative Screening:**
   - Pre-screening for completeness
   - Flagging missing documents
   - Basic compliance checking
   - This would "save time" on routine checks

2. **Summaries of Key Documents:**
   - Generate summaries of lengthy motivation narratives (with full text available)
   - Summarize reference letters to pull key themes
   - Makes information scanning faster without replacing full reading

3. **Program Verification Database:**
   - Compile information about international study programs
   - Help assess claims of program uniqueness/unavailability locally
   - *"Information can be made available that would help one to assess the merits of that program"*
   - Research whether comparable options truly exist

4. **Inconsistency Detection:**
   - Flag contradictions between application sections
   - Highlight when portfolio doesn't align with stated goals
   - Catch typos, date errors, credential mismatches

**Broader Process Suggestions:**

1. **Benchmarking Tool (Internal):**
   - System to help reviewers remember what constitutes a "persuasive, coherent application"
   - Consistency framework without constraining judgment
   - Reference to previous year's benchmark standards

2. **Consider Interim Review Layer:**
   - Inspired by DHET model with peer reviewers + sub-panel review
   - Could reduce individual reviewer bias
   - Maureen noted this is helpful when seeing 2-3 independent assessments before final decision

**Process Changes:**

- Invest time in training reviewers on:
  - How to interpret academic achievements across institutions
  - What to look for in portfolios
  - Red flags in creative work

---

## 7. Key Direct Quotes

### On Portfolio Criticality
> "It's actually quite critical if one's looking at a student wanting to go into a creative degree a practice-based masters or PhD is their portfolio and that evidence of practical excellence... because you can talk things up and especially with AI you know it's very easy to write a convincing proposal for a research project um and then you're sort of wondering how much of that is really this the student being able to articulate and does it match their actual ability, their practical ability and the strength of a portfolio."

**Context:** Explaining why portfolio is the most reliable assessment tool given the rise of AI-generated text

---

### On Creative Practice's Exploratory Nature
> "Practice in its nature is exploratory you know it kind of like opens up towards wards um realizations and and understandings and um you know it's very difficult to know before you've even set out on this journey where you're going to end up. Um and so I think in a lot of cases it's a bit of a thumb suck."

**Context:** Explaining why questions about significance to South African society are problematic—they ask for predetermined outcomes in a fundamentally open-ended discipline

---

### On Rubric-Question Misalignment
> "There's a slight misalignment in that you ask then as the reviewer um to look for evidence of values and priorities and sometimes the personal motivation is framed more as just a narrative... you've only got like a short space of time. And so a lot of candidates don't get much further than their background, schooling, academic history, and general interests, you know, but then you're being asked to look for evidence of their personal values and their priorities... we're looking for something that isn't necessarily in evidence."

**Context:** Identifying how question design creates assessment problems—the rubric asks reviewers to look for something the question structure doesn't elicit from applicants

---

### On Academic-Portfolio Mismatch
> "Somebody can articulate an idea for where they see their research going and it sounds great. It sounds very compelling, you know. Um, and then and then you look at at the caliber of the work that's being produced, you know, and this is also where sometimes there's a very clear misalignment for me between um the academic record and the portfolio... they've articulated um a reasonably strong kind of statement in regards to the significance of what they're intending to do. Um but then when you look at the at the portfolio, doubts creep in as to whether the person has the technical and conceptual sophistication to be able to pull off what they've articulated."

**Context:** Describing what causes mind-changes during evaluation—