# Interview Analysis: Ryan Nefdt
**Date:** 2026_01_21 14_25 SAST
**Source:** OMT Discovery Interview with StrideShift (Ryan Nefdt) – 2026_01_21 14_25 SAST – Notes by Gemini.docx
**Transcript word count:** 6423
**Analysis model:** claude-haiku-4-5

# OMT Application Review Analysis: Ryan Nefdt

## 1. Interviewee Role & Background

**Role in Review Process:**
- Philosophy professor at the University of Cape Town who reviews OMT postgraduate scholarship applications
- Evaluates primarily humanities submissions but also receives applications in related areas

**Duration of Involvement:**
- Not explicitly stated, but implied to be ongoing with multiple evaluation cycles

**Discipline/Expertise Area:**
- Primary: Philosophy and Linguistics
- Secondary: Fine Art, Sociology, Artificial Intelligence
- Describes himself as bridging science and humanities with active involvement in projects on large language models and statistical models
- Has qualifications and broad interests beyond philosophy

---

## 2. Current Review Process Description

**Systematic Approach:**
1. **Initial Read:** Reads submission once independently without predisposing it to rubric categories (when time permits)
2. **Structured Review:** Reviews submission systematically while referencing the OMT rubric simultaneously
3. **Background Analysis:** Examines the background section first, looking for cohesive narrative connecting life story to academic goals
4. **Impact Assessment:** For humanities, specifically evaluates articulated societal impact pathways
5. **Rubric Application:** Uses rubric to extract relevant information and assign numerical values
6. **Comparative Adjustment:** Reviews across the application pool within a cycle and adjusts scores to ensure consistency and fairness

**Tools/Systems:**
- OMT rubric (primary tool)
- OMT portal (where rubric is housed)
- Google searches for discipline-specific terminology/literature (self-directed when needed)

**Time Management:**
- Manages cycles of 20-30 applications at a time
- Uses rubric for efficiency rather than feeling burdened; describes the process as a "joy"
- When time-constrained, reads submission and rubric simultaneously rather than reading separately first

---

## 3. Pain Points & Challenges

**Challenge 1: Rubric Edge Cases for Theoretical Projects**
- Rubric struggles with "highly intricate theoretical projects" where societal impact is difficult to express
- Ryan characterizes these as "puzzle master" work—intellectually valuable but not easily captured by impact-focused rubric categories
- The rubric's structured approach to measuring societal impact doesn't fully capture merit of pure theoretical pursuits (00:08:52)

**Challenge 2: Disciplinary Knowledge Gaps**
- When reviewing outside direct expertise (e.g., ecology), lacks background on discipline-specific terms, literature, and issues
- Must spend time Googling to understand context, which becomes impractical with large application volumes
- Applicants must use precious word count explaining fundamental concepts to non-expert reviewers

**Challenge 3: Recommendation Letter Limitations**
- Views recommendation letters as largely "testimonial" and low-value
- Cannot rely on comparative information (where student ranks among others)
- Difficulty with cultural interpretation—African contexts emphasize interpersonal skills over individualistic academic achievement, creating potential for misreading
- Risk of reading subjective cues that may reflect recommender personality rather than applicant merit
- Absence of guidance on comparative positioning (unlike Oxford/Cambridge systems) reduces utility

**Challenge 4: Holistic Application Assessment Difficulty**
- Online form structure encourages disconnected answers—applicants fill boxes sequentially without seeing application as whole
- Difficult to identify whether applicant approached application holistically or just processed form-filling
- "Level of preparation" is important but hard to quantify for rubric purposes

**Challenge 5: Identifying Inconsistencies in Unfamiliar Domains**
- When reviewing outside direct field, may miss inconsistencies between problem identification and proposed solutions
- Can overlook inappropriate methodological choices if unfamiliar with discipline

---

## 4. What They Value in Applications

**Key Criteria:**

1. **Cohesive Narrative**
   - Connections between life experience, academic development, and current work
   - Personal story should directly inform and explain academic trajectory
   - Example given: Ryan's own multilingual upbringing directly led to linguistics interests (00:06:31)

2. **Articulated Societal Impact (Humanities-specific)**
   - Must demonstrate awareness of how work matters beyond theoretical puzzle-solving
   - Doesn't require perfect fit, but applicant must show consideration of societal goals
   - Recognition that impact pathways differ from sciences (publishing in high-impact journals)

3. **Holistic Preparation**
   - Evidence that applicant thought about application as unified whole, not separate form-filling exercise
   - Application that appears thoughtfully composed rather than rushed/transactional
   - (00:14:26)

4. **Discipline-Specific Intellectual Merit**
   - Recognition of intricate theoretical work even when societal impact is difficult to articulate
   - Values "puzzle master" work—intellectually rigorous pursuits even without obvious social application

**Red Flags:**
- Irrelevant life background that doesn't connect to academic pursuits
- Lack of consideration for societal impact in humanities work
- Disconnected answers suggesting form-filled approach rather than holistic thinking
- Inappropriate tools/methods proposed for identified problems

**What Makes Applications Stand Out:**
- Clarity of narrative arc connecting person to purpose
- Thoughtful engagement with discipline-specific questions
- Evidence of deep preparation and coherent vision

**How They Weigh Factors:**
- Rubric scoring takes priority over gut instinct to neutralize personal bias
- Will adjust scores comparatively within evaluation cycle to maintain consistency
- When on fence, considers level of preparation and holistic approach as tiebreakers
- Does not compare across years/cycles—evaluates each pool on own merit

---

## 5. Views on AI/Technology

**Overall Attitude:**
- "A bit of a lite" (skeptical) about AI in this context (00:23:47)
- Worried about AI summarization specifically
- Does not experience cognitive overload with current process, so primary motivation for AI assistance is unclear

**What He Would NOT Want Automated:**
- Summarization of applications
- Core evaluation decision-making
- Any process that reduces transparency in how merit is assessed

**What Could Be Helpful (With Specific Constraints):**
1. **Discipline-Specific Context Provision** (strongest support)
   - Providing background on specialized terminology, literature, and field-specific issues
   - Explaining where research is situated within its discipline
   - Could reduce burden on applicants to spend word count explaining fundamentals (00:25:53-26:47)
   - "An assistant could bridge a part of that gap for them"

2. **Inconsistency Flagging** (conditional support)
   - Identifying where applications don't hold together holistically
   - Flagging where proposed solutions don't match identified problems
   - Particularly useful in non-specialist domains (00:27:42)
   - "If the tool could identify inconsistencies when you're marking or evaluating a lot of these things, sometimes you miss it"

**Trust & Transparency Requirements:**
- Must understand what AI is doing and why
- Should not replace human judgment
- Should augment rather than automate
- Concerned about objective validity of AI interpretations
- Would need to trust the tool's reliability before relying on it

---

## 6. Suggestions & Ideas

**Rubric Improvements:**

1. **Include Level of Preparation/Holistic Approach**
   - Current rubric doesn't capture whether application was approached thoughtfully as whole
   - Suggested as tiebreaker criterion (00:14:26)
   - Acknowledges this may be "too subjective" but sees value

2. **Reconsider NRF Rating Scoring** (Already implemented)
   - P rating (future leader potential) should score higher than C rating
   - Fewer people achieve P rating; should reflect difficulty (00:12:17)
   - Ryan has already communicated this change to OMT

3. **Reconsider Budget Scoring Removal**
   - Budget section previously helped score something "felt objective"
   - Unclear why it was removed; valued this element

**Process Improvements:**

4. **Comparative Positioning in Recommendation Letters**
   - Recommend adopting Oxford/Cambridge model with comparative questions
   - Ask: "Where would you place the student?" and "How long have you known them?"
   - Would increase cognitive load slightly but provide useful context
   - Could offset issues with purely testimonial letters (00:21:20)

5. **Provide Disciplinary Context for Reviewers**
   - Create accessible background materials on research areas covered in application pool
   - Reduce need for reviewers to Google discipline-specific terms
   - Help reduce burden on applicants to explain fundamentals

**Technology Suggestions:**

6. **Inconsistency Detection Tool**
   - Could flag logical inconsistencies between problem and solution
   - Particularly valuable when reviewer is non-specialist
   - Should work proactively during evaluation, not as post-hoc summarization

7. **NOT Recommended:**
   - Summarization features
   - Automated scoring
   - Black-box AI systems

**Priority for Change:**
- Ryan doesn't experience pressure for change but identifies rubric edge cases and recommendation letter limitations as most significant issues
- Discipline-specific context provision seen as beneficial particularly for applicants in specialized fields
- Improved recommendation letter framework highest priority among actionable changes

---

## 7. Key Direct Quotes

**Quote 1: On Cohesive Narrative (foundational evaluation principle)**
> "I'm looking for if you can tell me a story that connects the different parts... tell me who you are as an academic and in so far as your life has informed that development tell me about your life... So that's one thing. Um I look for very often in humanities..." (00:05:02-06:31)

*Context: Explaining what he examines first in background section; shows how personal narrative is central to his evaluation framework*

---

**Quote 2: On Societal Impact in Humanities (discipline-specific insight)**
> "In the science you often like I'm going to publish in high impact journals and that kind of does the job for you... With humanities it's a little bit different. you have to figure out different conduits to um to getting some societal impact and that's something that OMT I've recognized over the years and they've been explicit that's something they care about." (00:06:31-07:41)

*Context: Explaining unique evaluation criteria for humanities vs. sciences; shows sophisticated understanding of disciplinary differences*

---

**Quote 3: On Rubric Edge Cases and "Puzzle Master" Work (primary limitation)**
> "If you are engaging in a theoretical project that is somewhat lacking in terms of your ability to express the societal impact but that theoretical project is so intricate and interesting... sometimes there are issues that are just not going to be able to be translated that well, but they're very intricate. It's sort of like, maybe this is a personal thing, but I think if you're a puzzle master and you're working on an interesting puzzle, sometimes somebody else is going to realize the value of that puzzle." (00:08:52)

*Context: Identifying where rubric fails to capture merit; articulates tension between structured evaluation and intellectual value recognition*

---

**Quote 4: On Rubric as Bias-Reduction Tool (key evaluation philosophy)**
> "I see the rubric as a measurement that allows comparison and that's why I'm a little reluctant to just form my own view outside of it because it's telling me I can use this... I would on the side of using the rubric because I'm assuming that is a tool to neutralize the sort of like personal bias that might creep in." (00:10:03-11:14)

*Context: Explaining prioritization of rubric over gut instinct; shows deliberate strategy to counteract evaluator bias*

---

**Quote 5: On Recommendation Letters' Limited Value (critique)**
> "I don't put much stock in the recommendation letters... I think if you got somebody to write a letter for you, it's probably going to be fine... But that doesn't give me much information... I think reading between the lines isn't an objective linguistic practice." (00:16:48-19:11)

*Context: Candid assessment of recommendation letters; shows skepticism of subjective interpretation methods*

---

**Quote 6: On Cultural Complexity in Reading Recommendation Letters (unique insight)**
> "In South Africa and in African context a little bit more than Southern Africa, people tend to place emphasis on interpersonal skills as an academic... Those cultural incongruities make me very nervous about reading between lines too often because I feel like I'm using a lot of stereotypes to do it." (00:19:11-20:05)

*Context: Highlights cultural dimensions of evaluation that complicate recommendation interpretation; shows awareness of potential bias in cross-cultural assessment*

---

**Quote 7: On Why Recommendation Letters Should Include Comparative Data (concrete suggestion)**
> "Maybe if there was something where they actually like in certain context where they really are guided on like giving us a comparison point how like I pulled in some recommendations for Oxford and Cambridge recently and they have separate from the recommendation letter they have a bunch of questions like where would you place the student, how long have you known them, what is your level at the thing and that if that information is given to the evaluator that might counteract the other question." (00:21:20-22:36)

*Context: Suggesting concrete model for improvement; shows familiarity with alternative systems*

---

**Quote 8: On When an AI Assistant Could Help (disciplinary context)**
> "I evaluate humanities generally... I don't always know background terms. Um I don't always know particular literatures. I don't always know particular issues. If I get a thing about ecology or whatever, maybe it would be nice... Maybe if there was some sort of assistant who could give you context of the discipline a little bit that might be useful like where this research proposal is situated in its discipline." (00:25:53-26:47)

*Context: Only unsolicited suggestion for AI assistance; framed as helping with discipline-specific knowledge gaps*

---

**Quote 9: On Disciplinary Context as Benefiting Applicants (fairness dimension)**
> "It could help the applicant in a particular way because one of the things I'm looking for from the applicant then because I'm not a polymath who understands everything is for them to be able to translate whatever they're doing to me as a person who might not be an expert. This assistant could bridge a part of that gap for them. So they don't have to do all the work of saying listen I'm working on a particular kind of thing that you might not understand now I have to spend most of my word count which they don't have a lot of." (00:26:47-27:42)

*Context: Frames AI assistance as fairness mechanism; shows concern for how knowledge gaps disadvantage specialist applicants*

---

**Quote 10: On Within-Cycle Comparative Evaluation (methodology)**
> "I don't think I ever think oh last year the crop was... I do see the pool as identifiable with each... I am comparative within [the cycle] and I will go back to other applications when I see something that I think oh wait you should have done this too and I gave you that mark but now it's not fair that I'm giving you that mark if I'm if this is what the standard should be." (00:29:00)

*Context: Explains comparative approach within cycles; shows active calibration of standards across pool for consistency and fairness*

---

## 8. Unique Insights

**Insight 1: "Puzzle Master" Philosophy of Merit**
Ryan articulates a distinctive view that intellectual merit in theoretical work exists independent of immediate societal impact applicability. His repeated metaphor of "puzzle master" work—intricate, valuable intellectual puzzles that may later have uses—captures a tension between utilitarian evaluation frameworks and academic research value systems. This is not just a complaint about the rubric, but a philosophical position about what constitutes merit in humanities scholarship. Few reviewers may articulate this so clearly.

**Insight 2: Cultural Dimensions of Recommendation Letter Interpretation**
While other reviewers may dismiss recommendation letters, Ryan uniquely identifies a *specific cultural explanation*: African contexts (particularly South African) emphasize interpersonal skills in academic recommendations more than Anglophone Western traditions that focus on individual achievement. This makes "reading between the lines" not just subjective but potentially stereotyping. This crosses into equity and cultural competency dimensions that go beyond typical evaluation critiques.

**Insight 3: Application Form Structure Creates Disconnection**
Ryan identifies that OMT's online form structure (fill sequential boxes, close window, move to next question) inherently fragments the applicant's narrative and makes holistic thinking invisible to reviewers. This is a systemic design issue rather than an applicant quality issue. Few reviewers explicitly connect form design to evaluation difficulty. This suggests a technical/UX intervention point.

**Insight 4: Applicant Burden of Translation in Specialist Fields**
Ryan uniquely frames disciplinary context-provision not as reviewer convenience but as *applicant fairness*. Specialists in niche fields must use scarce word count explaining fundamentals to generalist reviewers, disadvantaging them compared to applicants working in popular/accessible fields. This reframes the "context assistance" need as an equity issue. Most reviewers frame similar issues as personal cognitive load; Ryan frames it as fairness to applicants.

**Insight 5: Self-Awareness About Own Bias-Reduction Strategy**
Ryan explicitly articulates using the rubric as a *tool to counteract personal bias* rather than just a compliance checklist. He acknowledges that theoretical topics he finds personally interesting might bias him upward, and uses rubric scoring as an objective brake on that tendency. This sophisticated approach to evaluation integrity is unusual; most reviewers describe rubric use more mechanically.

**Insight 6: Comparative Calibration Within Cycles**
Ryan describes actively adjusting scores after reviewing other applications in the same cycle to maintain fairness—catching instances where