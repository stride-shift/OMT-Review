# OMT Discovery Interviews: Qualitative Analysis Synthesis

## Executive Summary

This synthesis analyzes 15 discovery interviews with OMT scholarship review committee members to understand current review processes and explore potential AI integration opportunities. The analysis reveals a sophisticated, human-centered evaluation system where reviewers employ diverse approaches unified by common values around holistic candidate assessment, authentic motivation, and societal impact potential.

Key findings show reviewers follow similar core workflows—initial scanning, detailed document review, rubric scoring, and comparative ranking—but adapt these to their disciplinary expertise and personal evaluation philosophies. The current 1-4 point rubric system creates systematic pain points, with reviewers finding it restrictive and lacking nuance, particularly for differentiating levels of excellence.

Reviewers demonstrate strong consensus around valuing authentic personal narratives, coherent research proposals, and genuine societal impact potential over purely academic metrics. They actively work to mitigate unconscious bias while acknowledging the inherently subjective nature of excellence assessment across diverse backgrounds and disciplines.

Attitudes toward AI assistance reveal cautious optimism rather than resistance. Reviewers welcome AI support for administrative tasks (document summarization, consistency checking, bibliometric analysis) while insisting on maintaining human judgment for nuanced evaluation and final decisions. Concerns center on AI's limitations in assessing future potential, cultural context, and authentic motivation—areas reviewers consider core to their expertise.

The analysis identifies significant opportunities for AI enhancement in initial screening, data verification, and providing disciplinary context, while preserving the deeply human elements of empathy, intuition, and holistic judgment that reviewers consider essential to identifying transformative scholarship potential.

## Methodology

Fifteen semi-structured discovery interviews were conducted with OMT review committee members between January 12-27, 2026. Interviews explored current review processes, pain points, evaluation criteria, and attitudes toward potential AI assistance. Data was analyzed using thematic analysis to identify patterns across disciplines and reviewer approaches.

## Participant Overview

| Interviewee | Date | Role/Discipline | Key Expertise |
|-------------|------|----------------|---------------|
| Dina Ligaga | 2026-01-12 | VIT Faculty of Humanities | Media & Cultural Studies, Gender Studies |
| Edzai Conilias Zvobwo | 2026-01-13 | Science/STEM reviewer | Science and Mathematics |
| Philippe Burger | 2026-01-13 | Economist | Macroeconomics, Business Management |
| Martin Clark | 2026-01-15 | University of Free State | Geology, Earth Sciences |
| Pieter Pistorius | 2026-01-19 | University of Pretoria | Metallurgical Engineering |
| Mohamed Cassim | 2026-01-20 | Financial Applications | Strategy, Innovation, FinTech |
| Ryan Nefdt | 2026-01-21 | University of Cape Town | Philosophy, Linguistics |
| Maureen De Jager | 2026-01-22 | Rhodes University | Fine Arts, Creative Arts |
| Freedom Gumedze | 2026-01-26 | University of Cape Town | Biostatistics, Applied Health Research |
| Frelet De Villiers | 2026-01-26 | University of Free State | Music, Arts |
| Frasia Oosthuizen | 2026-01-27 | Health Sciences | Pharmaceutical Sciences |

*Note: Interviews with Andrew Macdonald, Cephas Chikanda, and Ndumiso Luthuli did not contain substantive review process content.*

## Thematic Analysis

### Theme 1: Current Review Process & Workflow

Reviewers demonstrate remarkable consistency in their core workflow despite diverse disciplinary backgrounds. The typical process follows four phases: **initial scanning** ("I do an initial 5-minute quick scan" - Pieter Pistorius), **detailed review** ("I read the entire portfolio thoroughly" - Freedom Gumedze), **rubric scoring**, and **comparative ranking**.

However, significant variation exists in implementation approaches. Some reviewers batch applications by degree level (Philippe Burger: "Initial Classification: Sorting applications by degree level"), while others use comparative reading strategies (Maureen De Jager: "Initial overview of all applications, skims through materials to get holistic perspective"). 

Time allocation varies substantially, from Pieter Pistorius's systematic "45-60 minutes per application over 2-3 evenings" to Frasia Oosthuizen's "good few solid hours" per batch approach. This variation reflects both disciplinary differences and personal review philosophies rather than inconsistency in thoroughness.

### Theme 2: Pain Points & Bottlenecks

The most significant structural challenge centers on the **restrictive rubric system**. Dina Ligaga articulates this clearly: "Rubric feels restrictive and 'fuzzy', especially between scores 3-4... Limited scoring range, particularly for Masters applications." Martin Clark echoes this: "Rigid rubric categories that don't accommodate diverse backgrounds... Limited scoring range (1-4, then 5-point final scale)."

**AI-generated content detection** emerges as a growing concern. Philippe Burger warns: "Increasing prevalence of AI-generated essays... Difficulty distinguishing genuine from AI-written motivation essays," while Maureen De Jager notes: "Potential AI-generated application content" compromising authentic assessment.

**Technical interface issues** create workflow inefficiencies. Dina Ligaga specifically mentions: "Multi-window interface is frustrating and time-consuming," though some reviewers like Frasia Oosthuizen find the "split-screen application and rubric" workable.

The **cold start problem** affects consistent evaluation standards. Edzai Conilias Zvobwo identifies: "Cold start problem with first few applications... Difficulty establishing initial benchmark/reference point."

### Theme 3: Evaluation Criteria & Decision-Making

Despite diverse disciplines, reviewers demonstrate strong consensus around core evaluation criteria prioritizing **authentic personal narratives** over pure academic metrics. Martin Clark exemplifies this philosophy: "I always value passion higher than I value cleverness. You can always teach passion but you can't build passion always."

**Coherent research trajectory** represents another universal criterion. Ryan Nefdt seeks "Cohesive narrative connecting life experience to academic goals," while Frasia Oosthuizen values "Holistic alignment across motivation letter, research proposal, academic performance."

**Societal impact potential** consistently influences evaluation decisions. Edzai Conilias Zvobwo shows "Preference for work benefiting 'bottom of the pyramid'," and Mohamed Cassim emphasizes "Potential for creating broader impact."

Reviewers actively acknowledge and manage **subjective judgment**. Frasia Oosthuizen reflects: "I try to be objective, but in something like this which is not based on quantitative values, there is a matter of subjectivity." This transparency about subjectivity represents sophisticated evaluation practice rather than weakness.

**Portfolio assessment** proves crucial in creative disciplines. Maureen De Jager emphasizes: "Portfolio as critical assessment element... Technical and conceptual sophistication," demonstrating how evaluation criteria adapt appropriately to disciplinary requirements.

### Theme 4: Attitudes Toward AI & Technology

Contrary to potential expectations, reviewers show **cautious optimism** rather than resistance toward AI assistance. The key distinction lies between administrative support and evaluative judgment. Mohamed Cassim frames AI as a "cognitive assistant" for "reducing superfluous work," while Freedom Gumedze advocates "human in the loop" approaches.

**Desired AI capabilities** cluster around administrative efficiency:
- **Document verification**: Mohamed Cassim suggests "budget verification, database searches, institutional performance verification"
- **Initial screening**: Martin Clark sees potential for "logic validation, stakeholder perception assessment, data scraping"
- **Bibliometric analysis**: Freedom Gumedze would welcome "research background verification, bibliometric analysis, checklist/summary preparation"

**Trust boundaries** remain clearly defined. Reviewers consistently reject autonomous AI decision-making. Edzai Conilias Zvobwo states firmly: "Strongly advocates 'human in the loop' approach... Rejects fully autonomous systems." Pieter Pistorius concurs: "Eventually the human being must make a final call."

**AI limitations awareness** demonstrates sophisticated understanding. Edzai Conilias Zvobwo explains: "AI as it is can't reason. It's purely statistical, probabilistic," while Mohamed Cassim warns: "AI right now... is built on data that is in the internet... by its very nature AI would go on historic trends when all of us know that the future is not told by history alone."

Some reviewers express **skepticism about creative assessment**. Maureen De Jager argues: "Assessing art requires time and a human element of interpretation, which cannot be easily short-circuited," while Frelet De Villiers believes "AI cannot replicate human judgment" in arts evaluation.

### Theme 5: Desired Improvements & Features

**Rubric enhancement** represents the strongest consensus area. Multiple reviewers request expanded scoring ranges. Dina Ligaga suggests: "Expand rubric scoring range, especially for Masters applications... Allow more granular scoring between excellence levels." Martin Clark wants "More nebulous, flexible definition of excellence."

**Comparative feedback systems** could improve calibration. Freedom Gumedze requests: "Provide cross-reviewer score comparisons... Implement a consensus/moderation process," while Philippe Burger suggests "more robust absolute and relative benchmarking."

**Administrative efficiency improvements** focus on data pre-population. Frasia Oosthuizen suggests "Prefilling objective data (academic performance, funding status)," and Freedom Gumedze wants "bibliometric analysis, checklist/summary preparation."

**Disciplinary context support** could enhance cross-field evaluation. Ryan Nefdt suggests: "If there was some sort of assistant who could give you context of the discipline... that might be useful."

**Authentication mechanisms** address AI-generated content concerns. Philippe Burger proposes "Interviews to verify essay authenticity... Timed, proctored essay writing."

### Theme 6: Holistic Excellence Assessment

A significant emergent theme centers on reviewers' sophisticated approach to **contextual excellence evaluation**. Martin Clark articulates this: "Excellence, true excellence can be measured whether it is from a lay person's perspective or it can be from an absolute disciplined leader... A definition of excellence that was a little bit more sensitive... could shift them right into a realm where they could be funded."

This contextual approach manifests in **bias mitigation strategies**. Edzai Conilias Zvobwo demonstrates "Explicit acknowledgment and active management of personal biases," while Philippe Burger emphasizes being "conscious to not penalize people [for language differences]."

**Institutional context awareness** influences evaluation approaches. Martin Clark notes: "Not every applicant has availability of word processing, spell-check engines," showing sensitivity to structural inequalities in application preparation resources.

## Cross-Cutting Insights

### Consensus Areas vs. Divergence

**Strong consensus** exists around:
- Valuing authentic motivation over pure academic achievement
- Maintaining human judgment in final decisions
- Welcoming AI for administrative tasks
- Need for more nuanced rubric systems
- Importance of societal impact assessment

**Divergence** appears in:
- Specific workflow approaches (batching strategies, time allocation)
- Tolerance for AI assistance levels
- Emphasis on different evaluation criteria weighting
- Approaches to managing subjectivity

### Implicit Assumptions

Reviewers implicitly assume **transformation potential** as a core selection criterion beyond explicit rubric categories. This manifests in seeking candidates who demonstrate potential for significant positive impact despite potentially modest current achievements.

**Cultural competence** assumptions vary significantly, with some reviewers more attuned to structural inequalities affecting application quality than others.

### Institutional Dynamics

The analysis reveals **professionalization tensions** between maintaining scholarly review traditions and embracing technological efficiency. Reviewers value the intellectual engagement of review work while recognizing time constraints of volunteer service.

**Quality control concerns** emerge around ensuring consistent evaluation standards across diverse reviewer backgrounds without eliminating valuable perspective diversity.

## Recommendations

### 1. System Design Priorities

**Enhanced Rubric System**: Implement expanded scoring ranges (1-7 or 1-10) with detailed criteria descriptors for each level. Create discipline-specific rubric variations while maintaining core comparative categories.

**Workflow Optimization**: Develop configurable review interfaces allowing reviewers to choose preferred workflows (batch processing, comparative reading, etc.) while maintaining data consistency.

**Document Management**: Implement intelligent document summarization and cross-reference checking to reduce administrative burden while preserving comprehensive review capability.

### 2. AI Integration Approach

**Tiered AI Implementation**:
- **Phase 1**: Administrative support (document verification, bibliometric analysis, application completeness checking)
- **Phase 2**: Contextual assistance (disciplinary background summaries, institutional performance data)
- **Phase 3**: Advanced analytics (inconsistency flagging, preliminary screening with human override)

**Human-AI Collaboration Model**: Design AI as "cognitive assistant" providing information and analysis while preserving human judgment primacy for all evaluative decisions.

**Transparency Requirements**: Ensure all AI-assisted analyses are clearly labeled and explanable to maintain reviewer trust and enable appropriate skepticism.

### 3. Change Management Considerations

**Gradual Implementation**: Begin with least controversial AI applications (data verification, document summarization) before advancing to more sophisticated assistance.

**Reviewer Choice**: Allow reviewers to opt into different AI assistance levels based on comfort and preference, avoiding mandatory adoption of controversial features.

**Training and Support**: Develop reviewer education programs about AI capabilities and limitations to enable informed utilization decisions.

### 4. Risk Areas to Address

**Authentication Systems**: Implement robust verification methods for application authenticity, potentially including video interviews or timed writing components for high-stakes applications.

**Bias Amplification**: Carefully monitor AI systems for bias amplification, particularly regarding applications from diverse cultural and educational backgrounds.

**Over-Reliance Prevention**: Design systems that encourage rather than replace critical thinking, maintaining reviewer engagement with substantial evaluation decisions.

**Quality Assurance**: Develop metrics and monitoring systems to ensure AI assistance improves rather than degrades review quality and consistency.

## Appendix: Interviewee Reference Table

| Name | Date | Key Role | Top 3 Distinctive Contributions |
|------|------|----------|--------------------------------|
| Dina Ligaga | 2026-01-12 | Humanities/Cultural Studies | 1) Detailed interface usability feedback 2) Rubric restrictiveness analysis 3) Authenticity vs. sincerity distinction |
| Edzai Conilias Zvobwo | 2026-01-13 | STEM Sciences | 1) "Human in the loop" AI philosophy 2) Explicit bias acknowledgment 3) Bottom-of-pyramid impact prioritization |
| Philippe Burger | 2026-01-13 | Economics | 1) AI-generated content detection concerns 2) Interdisciplinary evaluation challenges 3) Authentication mechanism proposals |
| Martin Clark | 2026-01-15 | Geology | 1) Passion vs. cleverness evaluation philosophy 2) Contextual excellence definition 3) Structural inequality awareness |
| Pieter Pistorius | 2026-01-19 | Metallurgical Engineering | 1) Systematic time allocation methodology 2) Document consistency verification 3) South African community commitment emphasis |
| Mohamed Cassim | 2026-01-20 | Financial Applications | 1) "Cognitive assistant" AI framing 2) Probability-of-delivery assessment 3) Cultural/linguistic diversity emphasis |
| Ryan Nefdt | 2026-01-21 | Philosophy | 1) Cohesive narrative evaluation approach 2) Disciplinary context assistance needs 3) Humanities impact assessment insights |
| Maureen De Jager | 2026-01-22 | Fine Arts | 1) Portfolio-based creative assessment 2) Human interpretation requirement for arts 3) Coherence between written and visual work |
| Freedom Gumedze | 2026-01-26 | Biostatistics | 1) Cross-reviewer comparison systems 2) Publication assessment guidelines needs 3) NRF model rubric analysis |
| Frelet De Villiers | 2026-01-26 | Music/Arts | 1) Feedback importance for applicant development 2) AI skepticism for creative evaluation 3) Starting-high scoring adjustment methodology |
| Frasia Oosthuizen | 2026-01-27 | Pharmaceutical Sciences | 1) Holistic candidate "picture" creation 2) Independent judgment preservation 3) Visual learning approach integration |