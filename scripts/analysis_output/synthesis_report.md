# OMT Discovery Interviews: Qualitative Analysis Synthesis

## Executive Summary

This synthesis analyzes 15 discovery interviews with Oppenheimer Memorial Trust (OMT) review committee members conducted between January 12-27, 2026. The interviews reveal a sophisticated review ecosystem where experienced academics balance structured rubric assessment with nuanced professional judgment to evaluate postgraduate scholarship applications across Masters, PhD, Postdoc, and Sabbatical categories.

**Key Findings:**

**Current State:** Reviewers employ diverse but thoughtful approaches, ranging from highly systematic comparative methods to holistic narrative assessment. Most conduct 45-60 minute deep reads per application, taking extensive notes and applying 4-5 point rubric scales. The recently implemented web-based system is generally well-received, representing significant improvement over previous Excel-based processes.

**Critical Pain Points:** The most significant challenges cluster around three areas: (1) **Rubric limitations** - insufficient granularity (particularly distinguishing between scores 3-4), rigid categories that fail to capture disciplinary differences, and unclear scoring thresholds; (2) **Cognitive bottlenecks** - difficulty assessing authentic motivation amid AI-generated content, time-consuming verification of institutional claims and academic records, and challenges evaluating specialized research outside reviewers' expertise; (3) **Process gaps** - lack of reviewer calibration mechanisms, no feedback loops on decision outcomes, and unclear synthesis of multiple reviewer assessments.

**Evaluation Philosophy:** Reviewers prioritize coherence and authenticity over credentials alone. The strongest consensus emerges around valuing: compelling personal narratives showing resilience (not victimhood), realistic and feasible proposals with clear intellectual merit, alignment between applicant background and intended study, and evidence of genuine commitment (particularly through "skin in the game" indicators). Academic records are contextualized rather than mechanically applied, with reviewers consciously accounting for institutional grading differences and socioeconomic background.

**AI Attitudes:** The spectrum ranges from cautiously optimistic to protective skepticism. Most reviewers accept AI assistance for **administrative tasks** (pre-screening completeness, extracting factual data, providing disciplinary context, verifying institutional performance) but firmly reject automation of **judgment tasks** (assessing motivation authenticity, evaluating proposal quality, making final recommendations). A critical concern is AI's fundamental inability to assess "probability of future delivery" - seen as requiring human evaluation of capability, context, and intangibles like passion and resilience. Several reviewers worry about AI-generated application content making authentic assessment increasingly difficult.

**Consensus Opportunities:** Strong agreement exists on needs for: expanded scoring scales (4-point insufficient for nuanced distinction), pre-populated factual data to reduce cognitive load, better disciplinary context provision for specialized fields, and interview/verification mechanisms to combat AI-generated content. There is universal insistence that final judgment authority must remain with human reviewers.

This synthesis provides a foundation for system redesign that preserves reviewer expertise and judgment while strategically deploying AI to reduce superfluous cognitive burden and enhance decision quality.

---

## Methodology

**Research Design:** 15 semi-structured discovery interviews conducted via video conference between January 12-27, 2026. Interviews lasted approximately 30-60 minutes each and explored current review processes, pain points, evaluation criteria, and attitudes toward AI assistance.

**Participants:** Review committee members representing diverse disciplines (sciences, humanities, social sciences, creative arts, business/economics) with 2-5+ years of OMT review experience. Participants included professors, department heads, and practitioners with both academic and industry backgrounds.

**Data Collection:** Interviews were recorded, transcribed using Gemini AI, and individually analyzed. Each transcript was examined for: process descriptions, pain points, evaluation criteria, AI/technology views, and improvement suggestions.

**Analysis Approach:** Thematic analysis across all transcripts identified patterns, variations, contradictions, and consensus areas. Direct quotes were preserved with attribution to support findings. Two transcripts were excluded from synthesis (Andrew Macdonald sessions contained no relevant content; Ndumiso Luthuli interview was not completed in captured recording).

**Limitations:** Sample represents active reviewers willing to participate; may not capture perspectives of reviewers who declined or discontinued service. Self-reported processes may differ from actual practice. Timing during annual review cycle may influence perspectives.

---

## Participant Overview

| Interviewee | Role/Discipline | Years with OMT | Key Expertise |
|-------------|----------------|----------------|---------------|
| **Dina Ligaga** | Media & Cultural Studies, VIT | ~4 years | Humanities, gender studies, interdisciplinary research |
| **Edzai Zvobwo** | Science & Mathematics, Analytics/AI Consultant | Since 2019 (~7 years) | Broad science spectrum, AI implementation, data analytics |
| **Freedom Gumedze** | Statistical Sciences, UCT | 3-4 years | Biostatistics, applied health research, NRF panels |
| **Frelet De Villiers** | Music Department Head, University of Free State | 4-5 years | Arts, music, cross-disciplinary review (also NRF) |
| **Martin Clark** | Geology/Earth Sciences, University of Free State | 3-4 years | Geosciences, laboratory management, international education |
| **Maureen De Jager** | Fine Art, Rhodes University | ~3 years | Creative arts, practice-based research, former Deputy Dean |
| **Mohamed Cassim** | Finance/Development Economics, Africa Climate Ventures | Unknown | Public sector, private sector development, social enterprise |
| **Philippe Burger** | Economics/Macroeconomics | 2-3 years | Business management, economics, quantitative disciplines |
| **Pieter Pistorius** | Metallurgical Engineering, University of Pretoria | ~3 years | Engineering, industry-academia bridge, postgraduate program |
| **Ryan Nefdt** | Philosophy & Linguistics, UCT | Multiple cycles | Philosophy, AI/large language models, science-humanities bridge |
| **Frasia Oosthuizen** | Pharmaceutical Sciences, Registered Pharmacist | Long-standing (exact duration unknown) | Health sciences broadly, pharmacology, undergraduate/postgraduate teaching |

**Note:** Two scheduled interviewees (Andrew Macdonald, Ndumiso Luthuli) did not complete interviews in provided recordings.

---

## Thematic Analysis

### Theme 1: Current Review Process & Workflow

**Common Patterns Across Reviewers:**

All reviewers conduct **deep, time-intensive reviews** of complete applications, typically investing 30-60 minutes per candidate for detailed assessment plus additional time for batch calibration and consistency checking. The dominant workflow structure involves:

1. **Initial familiarization** - ranging from quick scans (Pieter: "5 minutes to get a sense") to complete first reads without rubric reference (Ryan, Maureen)
2. **Document examination** - systematic review of motivation letters, proposals, academic records, CVs, references, and budgets
3. **Rubric application** - scoring on 1-4 or 1-5 point scales across standardized criteria
4. **Comparative calibration** - revisiting scores after reviewing cohort to ensure consistency (Frasia: "if scoring a fifth candidate, may flip back"; Pieter: "consistency check phase")
5. **Written justification** - providing narrative commentary explaining scores and recommendations

> "Every application you read, you need to read with a fresh mind. You need to find a way to value what is being proposed but also value the person who is proposing that." — **Martin Clark, 2026-01-15**

Most reviewers **batch applications by type** (Masters together, PhDs together, etc.) to enable fair comparison within categories. Several maintain **extensive handwritten notes** during review (Dina, Frasia) despite working in digital systems. The recently implemented web-based dual-screen interface receives consistent praise for improvement over previous Excel spreadsheet systems.

**Variations in Approach:**

**Sequencing differences:**
- **Motivation-first readers** (Dina, Frasia, Maureen) prioritize understanding the person before evaluating credentials
- **Academic-first reviewers** (Philippe, Edzai) use academic performance as initial filter before deeper assessment  
- **Holistic-then-structured** (Ryan, Maureen) read applications completely before consulting rubric to avoid premature judgment

**Comparative vs. Independent Assessment:**
- **Comparative method** (Maureen, Frasia): explicitly benchmark within current cohort, adjusting scores for relative positioning
- **Independent method** (Pieter, Martin): attempt to evaluate each application on absolute merit without reference to others
- **Historical benchmarking** (Edzai, Mohamed): draw on accumulated multi-year patterns, though most avoid year-to-year comparison

> "I try to look at that cohort in itself... To benchmark would be difficult I think because the processes are so far apart." — **Pieter Pistorius, 2026-01-19**

**Rubric relationship:**
- **Rubric-as-guide** users (Freedom, Philippe, Ryan) treat rubric as framework but exercise significant interpretive judgment
- **Rubric-as-calibration** users (Ryan, Pieter) explicitly use rubric to counteract personal bias
- **Rubric-retrofit** users (Maureen, Dina) form holistic impressions first, then adjust rubric scores to match intuitive assessment when mechanical scoring conflicts with overall judgment

**Time management:**
- **Single-sitting completers** (Frasia) aim to finish category batches in one day for continuity
- **Multi-session reviewers** (Pieter, Mohamed) spread across multiple evenings with built-in reflection time
- **Deadline strategies**: Most complete well ahead of 2-week deadline; some work closer to cutoff

---

### Theme 2: Pain Points & Bottlenecks

**Most Frequently Mentioned Challenges:**

#### 1. **Rubric Limitations** (Mentioned by 10/11 reviewers)

**Insufficient Scoring Granularity:**

The 4-point scale (1-4) lacks resolution to distinguish between quality levels, particularly at the high end:

> "The bin size is too big. High quality three and a low quality three, they're all bunched up." — **Edzai Zvobwo, 2026-01-13**

> "Sometimes you know five points isn't always enough to discriminate one against the other... [need to distinguish] between 75% excellent and 90% excellent." — **Dina Ligaga, 2026-01-12** / **Freedom Gumedze, 2026-01-26**

**Severity:** HIGH - Creates inability to differentiate between genuinely competitive candidates, potentially affecting funding decisions for borderline cases.

**Disciplinary Misalignment:**

Rubrics built on assumptions inappropriate for some fields:

> "The rubric is built on a master's by dissertation... if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part." — **Freedom Gumedze, 2026-01-26**

> "If you are engaging in a theoretical project that is somewhat lacking in terms of your ability to express the societal impact but that theoretical project is so intricate and interesting... sometimes there are issues that are just not going to be able to be translated that well, but they're very intricate." — **Ryan Nefdt, 2026-01-21**

For creative arts, the societal impact question fundamentally misaligns with exploratory practice:

> "Practice in its nature is exploratory... it's very difficult to know before you've even set out on this journey where you're going to end up... in a lot of cases it's a bit of a thumb suck." — **Maureen De Jager, 2026-01-22**

**Unclear Thresholds:**

Reviewers struggle with what distinguishes adjacent scores:

> "There's always uh in that because you you do a translation of what your impression into a into a score. Um so so there will always be something uh uh um in there that that's a bit subjective... you look for for for that little extra that that that would carry it to the from a three to a four or four to a five." — **Philippe Burger, 2026-01-13**

**Specific Problematic Criteria:**

- **"Extraordinary talent"** described as "too blunt" and lacking scientific definition (Mohamed)
- **Referee report expectations** explicitly require language ("exceptional") that referees rarely use (Frasia, Ryan)
- **Golden Key Society membership** requires financial resources, disadvantaging excellent candidates from lower socioeconomic backgrounds (Martin)
- **NRF ratings** often outdated (5+ years old) and decreasing in relevance across institutions (Freedom)

---

#### 2. **Cognitive Bottlenecks: Assessing Authentic Motivation** (9/11 reviewers)

**The AI Generation Problem:**

Multiple reviewers independently identified increasing difficulty detecting AI-generated motivation letters:

> "With AI these days even the reviewer or the reference referee can also put that through AI and I mean you can't even see if it's a student writing or whatever... you have to believe what you get." — **Frelet De Villiers, 2026-01-26**

> "Especially with AI you know it's very easy to write a convincing proposal for a research project... and you're sort of wondering how much of that is really this the student being able to articulate and does it match their actual ability." — **Maureen De Jager, 2026-01-22**

> "You get students who...all of a sudden improved a lot uh for some of these cases which suggest uh that it might be Chad writing them instead of the candidate... in 2023 2024 still know back then you you could still see that something like a chat GBT generated text was very bland and and generalist... but of course now that that will become more and more difficult." — **Philippe Burger, 2026-01-13**

**Severity:** CRITICAL AND ESCALATING - Reviewers anticipate this will become the primary threat to essay-based assessment. Several noted the problem worsened visibly between 2023-2024 and 2025-2026.

**Distinguishing Genuine Commitment from Last-Resort Applications:**

> "What I quite often struggle with is to find out the motivation. What is really driving this specific individual... I got this sense that he or she was that this was the sort of point of application of the last resort." — **Pieter Pistorius, 2026-01-19**

> "If it's just a bunch of images and there's very little contextualization of what was informing the making of the work... it's also a little bit difficult to kind of get a handle on it." — **Maureen De Jager, 2026-01-22** (on portfolios)

**Impact:** Reviewers feel **least confident** in motivation assessment despite it being weighted heavily in decisions. This creates anxiety about whether funding supports genuine development vs. enabling other agendas.

---

#### 3. **Timeline/Academic Record Verification** (7/11 reviewers)

**Tedious Compilation Work:**

> "Compiling coherent timelines from diverse academic records across different institutions... tracking gaps in education/employment across formats (B.Tech, B.Sc, etc.)... Must manually create diagrams to understand applicant's academic progression. Describes this as 'quite tedious.'" — **Pieter Pistorius, 2026-01-19**

**Institutional Grading Variation:**

> "I rely on my knowledge of this sector... a 75% from a particular institution is probably worth more than a 75% from another institution." — **Maureen De Jager, 2026-01-22**

> "You get students who are really brilliant but you know they did not necessarily have a model ski model C school training. So um their use of English and their and their grammar isn't going to be as sophisticated, but it doesn't mean that these people are not clever." — **Philippe Burger, 2026-01-13**

**Missing Data:**

> "Applicants sometimes don't have final marks available by submission deadline (e.g., just completed exam sessions). Current rubric requires all fields to be completed, which pulls the overall score up or down despite missing information." — **Frasia Oosthuizen, 2026-01-27**

**Severity:** MODERATE - Consumes significant time ("pile of work") but reviewers have developed workarounds. However, fairness concerns exist when candidates penalized for factors beyond their control.

---

#### 4. **Subject Matter Expertise Gaps** (6/11 reviewers)

> "I'm not a water purification expert. So if I could have a water purification expert shadowing and giving their second opinion around that particular one then yes that would actually help because I might be throwing away the next Nobel Prize winner because it doesn't look appealing." — **Edzai Zvobwo, 2026-01-13**

> "When I adjudicate I have an earth science BSc which means I should be able to consider topics on a broad range of things. But I can tell you that with a first year, second-year biology I might not appreciate diseases that are specific to a caterpillar species." — **Martin Clark, 2026-01-15**

> "I evaluate humanities generally... I don't always know background terms. Um I don't always know particular literatures. I don't always know particular issues. If I get a thing about ecology or whatever, maybe it would be nice... Maybe if there was some sort of assistant who could give you context of the discipline a little bit." — **Ryan Nefdt, 2026-01-21**

**Impact:** Risk of **inadvertently dismissing excellent specialized work** due to reviewer unfamiliarity. Creates additional burden on applicants to "translate" specialized work using limited word count, disadvantaging specialists in niche fields.

---

#### 5. **Lack of Reviewer Calibration & Feedback Loops** (6/11 reviewers)

> "I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group... I want to know at the end what they they desire so that maybe we can calibrate that from this side." — **Freedom Gumedze, 2026-01-26**

> "[Doesn't know] whether he's one of 2 or one of 5 reviewers. Prefers not to know to avoid second-guessing his judgment." — **Pieter Pistorius, 2026-01-19**

> "There's nothing that I think one can sort of like maybe change but I would like to know... what they they desire so that maybe we can calibrate... maybe we can give feedback that there's a set of proposals that really didn't have to go through this stage." — **Freedom Gumedze, 2026-01-26**

**The Paradox:** Some reviewers (Freedom, Mohamed) want calibration and outcome feedback to improve future assessments. Others (Pieter, Ryan) prefer independence to avoid groupthink or second-guessing. This tension suggests need for **optional** rather than mandatory calibration.

**Severity:** MODERATE but affects long-term quality - Without feedback, reviewers cannot learn from outcomes or identify systematic drift in standards.

---

**Supporting Frustrations:**

- **Reference letter authenticity** difficult to verify (Frelet, Maureen, Ryan)
- **Budget assessment** time-consuming to verify feasibility (Mohamed, Martin)
- **Inconsistency detection** across application components missed when reviewing high volumes (Freedom, Maureen)
- **"Cold start problem"** - first application lacks reference point, potentially prejudicing early candidates (Edzai)
- **Adjudication fatigue** when reviewing too many applications consecutively (Martin, Dina)
- **Unclear overseas study justification** - difficulty verifying whether comparable programs truly unavailable locally (Freedom, Maureen)

---

### Theme 3: Evaluation Criteria & Decision-Making

**Shared Criteria Across Reviewers:**

Despite disciplinary differences, remarkable consensus emerges on what makes applications fundable:

#### **1. Personal Narrative & Resilience (Nearly Universal Priority)**

Reviewers look for compelling stories showing overcoming adversity, but critically, they distinguish between **resilience** and **victimhood**:

> "When you look at the motivation you can see when they are just um complaining if that makes sense. You you get some people and they will say I come from this very bad background and I don't have this and I don't have that. Um then you get other people who say I come from the background but I did this with what I have already and I want to um achieve this." — **Frelet De Villiers, 2026-01-26**

> "On the personal story where applicants are trying to show their resilience their story where they came from a broken home and against all odds they tended to overcome those obstacles. So that is very human because it has to resonate with you as the adjudicator and I don't think that element that little element in the process can be automated." — **Edzai Zvobwo, 2026-01-13**

> "I'm looking for if you can tell me a story that connects the different parts... tell me who you are as an academic and in so far as your life has informed that development tell me about your life." — **Ryan Nefdt, 2026-01-21**

Personal narrative serves as the **primary authenticity signal** and often functions as tiebreaker between similarly qualified candidates.

---

#### **2. Coherence & Internal Consistency**

The "golden thread" connecting motivation, proposal, background, and references:

> "There needs to be a kind of coherence where the way that the person articulates their work aligns with the work aligns with how they see it developing... If coherent, automatically persuasive; if not, unconvincing regardless of cohort standing." — **Maureen De Jager, 2026-01-22**

> "I try and create a picture of this applicant...everything if the big picture makes sense from the motivation through to the referee reports, I feel more confident that I have an understanding of who this candidate is and that my mark whether good or bad is a better reflection." — **Frasia Oosthuizen, 2026-01-27**

> "The person has chosen a school has got has got sufficient background has good background to actually pursue the work they saying they're going to be pursuing... Does it make sense?" — **Freedom Gumedze, 2026-01-26** / **Pieter Pistorius, 2026-01-19**

This "coherence criterion" is mentioned by 9/11 reviewers and functions as the **primary validation mechanism** - when components align, reviewers gain confidence in their assessment; when they conflict, even strong candidates become questionable.

---

#### **3. Proposal Quality: Originality + Feasibility**

> "I can sort of have a feel as to whether something is innovative or not... Uses expertise from editing peer-reviewed journals to assess innovation. Looks for novel statistical/methodological approaches in their discipline." — **Freedom Gumedze, 2026-01-26**

> "At PhD level, generally stronger... prioritizes originality and excitement in proposed research. Red flag: Generic, unoriginal proposals with nothing exciting about them." — **Dina Ligaga, 2026-01-12**

> "Is there really value in the study if they've done that so well? But you I'm not really looking between a connection between the motivation and the proposal. Um, the openers must get value for their money. So I cannot say oh this is a wonderful project and then nothing can come from it. So is it really viable?" — **Frelet De Villiers, 2026-01-26**

Reviewers assess both **intellectual merit** (is this interesting/novel?) and **practical feasibility** (can this person actually complete this?). The balance shifts by level: Masters proposals allowed more exploratory ambiguity; PhD proposals expected to show clearer methodology.

---

#### **4. "Skin in the Game" / Evidence of Commitment**

> "Strong believer that financial investment by candidate signals commitment. Will comment if candidate hasn't invested own cash (when they have capacity to do so). Recognizes constraints: won't penalize those supporting family members." — **Mohamed Cassim, 2026-01-20**

> "I want to understand if candidate has sourced funding or applied for other bursaries. Interprets this as indicator of how much 'they want this degree.' Views financial commitment as signal of genuine investment in the degree." — **Frasia Oosthuizen, 2026-01-27**

This criterion functions as **behavioral validation** of stated motivation - have they taken concrete action demonstrating commitment?

---

#### **5. Realistic Ambition (Not Grandiosity)**

> "Can you can you spot a bit of ambition or is this just, you know, a run-of-the-mill I'm going to change the world? Uh, has there's been some thought in it? I think that's the other thing you you you know, you can get you can write something very easily to say yes, I want to change this and that, but has there some been some thought in in in in in that motivates you to to to do this?" — **Philippe Burger, 2026-01-13**

> "Someone who's creating a new way to clean jobic water over a way to multiply matrices better... I would go for the job water because I can see the immediate [impact]." — **Edzai Zvobwo, 2026-01-13**

> "I might review an application of someone from a disadvantaged background and that disadvantaged background would articulate how they would want to make a change in their community...but the grading metrics asks me to evaluate if the project could have national or global significance." — **Martin Clark, 2026-01-15**

Reviewers want **stretch goals that are achievable**, not fantasy. Martin's observation reveals a **tension** between valuing local impact from disadvantaged backgrounds vs. rubric demands for global significance.

---

**Differences in Prioritization:**

| Criterion | Emphasized Most By | De-emphasized By | Notes |
|-----------|-------------------|------------------|-------|
| **Academic Record** | Philippe (primary filter), Edzai (disqualifying) | Frelet (least important), Martin (contextual) | Sciences weight higher; humanities contextualize more |
| **Portfolio (Creative Arts)** | Maureen ("most reliable"), Frelet | N/A | Discipline-specific; counterbalances AI-generated text concerns |
| **Social Impact** | Dina (humanities-specific), Ryan, Maureen | Edzai (prefers immediate), Martin (local vs. global tension) | Humanities emphasize more; sciences struggle with pre-determination |
| **Research Standing (Postdocs)** | Freedom (deep assessment), Mohamed (institutional verification) | N/A | Only relevant for senior categories |
| **Budget** | Mohamed (feasibility check), Martin (red flag detector) | Frelet (last priority), Dina (minimal influence) | Investment world perspectives weight more |
| **Ties to South Africa** | Pieter (explicit concern), Mohamed (repatriation value) | Not mentioned by most | May be implicit but not systematically assessed |

---

**Tacit Knowledge & Expert Judgment Patterns:**

Several reviewers describe judgment factors **not captured in rubric**:

> "I've had a situation where I literally held their hand to say, 'Okay, I get what you're saying and I'll give you four because I I I get what you're saying, but you're not saying it, but I get it.'" — **Edzai Zvobwo, 2026-01-13**

This "implicit understanding" of what applicants mean despite imperfect articulation represents **expert pattern recognition** that resists systematization.

**Psychographic Matching:**

> "From the cognitive perspective, it would be fantastic if it said the topic requires an extrovert of this kind with that type of experience and this guy doesn't have it." — **Mohamed Cassim, 2026-01-20**

Mohamed explicitly describes assessing **temperament-topic fit** - whether the person's psychological profile matches project demands. This sophisticated meta-cognitive assessment rarely appears in formal evaluation frameworks.

**Institutional Performance Contextualization:**

> "For candidates with substantial experience (postgrad + 10+ years), examines performance of institutions where they worked, particularly in relation to their stated functions... Won't recommend someone from organization that failed in their stated domain." — **Mohamed Cassim, 2026-01-20**

This requires extensive external research and sector knowledge unavailable to most reviewers.

**Cultural Interpretation Awareness:**

> "In South Africa and in African context a little bit more than Southern Africa, people tend to place emphasis on interpersonal skills as an academic... Those cultural incongruities make me very nervous about reading between lines too often because I feel like I'm using a lot of stereotypes to do it." — **Ryan Nefdt, 2026-01-21**

Ryan's explicit recognition of cultural interpretation dimensions shows sophisticated awareness of how cross-cultural assessment introduces systematic bias risk.

**The "Excellence Relative to Circumstance" Principle:**

> "Excellence can be measured whether it is from a lay person's perspective or from an absolute a disciplined leader...there are backgrounds and life paths that are different than my own and it does not mean that a different life path or a life path that appears to be less developmental or less of a straight line towards excellence should be precluded from excellence." — **Martin Clark, 2026-01-15**

> "Someone that comes from an impoverished neighborhood may be excellent or mediocre. I don't think our idea is to support a mediocre candidate but you need to look at the social context." — **Pieter Pistorius, 2026-01-19**

This represents a **contextual excellence framework** that most reviewers employ but struggle to articulate or systematize.

---

**Decision-Making Heuristics:**

**The "Retrofit" Pattern:**

Multiple reviewers (Maureen, Dina, Mohamed) describe forming holistic impressions first, then **adjusting rubric scores to match** when mechanical scoring conflicts:

> "Frequently returns to adjust final scores if calculated rating doesn't match strength of submission. Uses initial impression (often reinforced through detailed review) to override mechanical scoring. Described as 'retrofit' - tweaking scores to reflect actual quality assessment." — **Maureen De Jager, 2026-01-22**

> "Sometimes adjusts rubric scores to align with overall intuitive assessment... Has to mark up or down scores to match their overall intuitive conviction." — **Dina Ligaga, 2026-01-12**

This reveals rubric functioning as **post-hoc rationalization tool** rather than primary decision mechanism for some reviewers.

**The "Comparative Adjustment" Pattern:**

> "Reviews across the application pool within a cycle and adjusts scores to ensure consistency and fairness... When on fence, considers level of preparation and holistic approach as tiebreakers." — **Ryan Nefdt, 2026-01-21**

> "If scoring a fifth candidate, may flip back to reconsider scores of earlier candidates... Explicitly adjusts previous scores if new candidates reveal that earlier scores were disproportionate." — **Frasia Oosthuizen, 2026-01-27**

This iterative calibration approach produces **relative rather than absolute** assessments within cohorts.

**The "Bias Counteraction" Strategy:**

> "I see the rubric as a measurement that allows comparison and that's why I'm a little reluctant to just form my own view outside of it because it's telling me I can use this... I would on the side of using the rubric because I'm assuming that is a tool to neutralize the sort of like personal bias that might creep in." — **Ryan Nefdt, 2026-01-21**

> "I would follow the rubric. If there's a discrepancy... I'll go and take a look and say why is it lower, and I tend to go back to the individual parameters and look at that again. But I wouldn't sort of push it towards my first intuition." — **Pieter Pistorius, 2026-01-19**

These reviewers use rubric as **counterfactual check** against intuitive bias, representing sophisticated metacognitive practice.

---

### Theme 4: Attitudes Toward AI & Technology

**Spectrum of Attitudes:**

The reviewer community exhibits a **nuanced sophistication** about AI capabilities and limitations that defies simple categorization:

#### **Pragmatic Optimists (Mohamed, Martin, Edzai)**

Accept AI assistance for specific tasks but maintain clear boundaries:

> "I would suggest a situation where there is AI with a human in the loop where AI does the drudge work the mind numbing stuff but leaving room for the human to actually review and almost give a second opinion basically." — **Edzai Zvobwo, 2026-01-13**

> "I love AI... [it] can help in almost any element... Could assist with data scraping and research assistance for unfamiliar fields. Acts as 'CliffsNotes' equivalent." — **Martin Clark, 2026-01-15**

> "Could be helpful if the binary stuff...could answer that in about 30 seconds and the adjudicator can decide... I would love for it to tell me that from this guy's CV I checked on the institutions." — **Mohamed Cassim, 2026-01-20**

**Philosophy:** AI as **cognitive assistant** that handles factual verification and information gathering, freeing humans for judgment.

---

#### **Cautious Skeptics (Pieter, Philippe, Ryan)**

See value but worry about implementation pitfalls:

> "I'm somewhat cynical about AI... It is like measuring something twice. The two measurements will differ. Now you have to explain three things: the first measurement, the second measurement, and the difference... Eventually the human being must make a final call." — **Pieter Pistorius, 2026-01-19**

> "Going forward I think that you're going to have a serious problem relying on essays as a mechanism for distinguishing between applicants... [AI text generation] will become more and more difficult." — **Philippe Burger, 2026-01-13**

> "A bit of a lite [skeptical] about AI in this context. Worried about AI summarization specifically. Does not experience cognitive overload with current process, so primary motivation for AI assistance is unclear." — **Ryan Nefdt, 2026-01-21**

**Philosophy:** Concerned about **double measurement paradox** and AI's tendency to introduce new problems while solving old ones.

---

#### **Protective of Judgment (Frasia, Maureen, Frelet)**

Accept administrative support but firmly reject anything that structures their judgment prematurely:

> "If that person tells me what the red flags are, I already have decided in my mind no this is not going to work. So I'm really not objective... I prefer getting that picture myself and make my own judgment call." — **Frasia Oosthuizen, 2026-01-27**

> "I think it would be immensely helpful, but I don't know how it's possible because so much of this is interpretative... Creative practice is such a tricky thing... there's not a golden standard