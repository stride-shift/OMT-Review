# Evaluation Criteria Deep Dive

## Dina  Ligaga

# OMT Application Evaluation Criteria Analysis

## 1. EXPLICIT CRITERIA MENTIONED

### Primary Evaluation Factors:
- **Applicant's motivation** - emphasized repeatedly as foundational
- **Proposed study** - described as "a big deal" and clearly influential
- **Academic performance and history** - referenced through transcripts and CV
- **Continuity** - narrative thread connecting applicant's background to proposed work

### Verbatim Quotes:
- "I like knowing what it is this person is about" (00:04:41)
- "The proposed study, which I really value" (00:04:41)
- "The proposed study really really you know for me is a big deal" (00:06:03)
- "It's the ideas that that really really make a difference" (00:19:59)

---

## 2. IMPLICIT CRITERIA (Gap Between Rubric and Actual Practice)

### Critical Unwritten Factors:

**Sincerity and Authenticity:**
- "There's a genuiness to some of the some of the applications that I appreciate" (00:08:17)
- "It's not about performing the motivation per se. It's about being as honest as possible" (00:08:17)
- Judgment includes "a perceived sincerity in the applicant's personal motivation" (Summary)

**Socioeconomic Context (Unwritten Rule):**
- "Sometimes I'm influenced by the fact that somebody's really had a really really difficult upbringing and really difficult" (00:32:06)
- "With the motivation there...stems from these kinds of un unwritten unwritten things" (00:32:06)
- Race and class angles explicitly influence reading (00:32:06, Summary)

**Narrative Continuity:**
- "There's a string through that into what it is that they want to do and how it is that then you can weave that kind of narrative" (00:09:30)
- "There's a continuity...that in many ways influences how I then approach that application" (00:09:30)

**Originality and Excitement (PhD level particularly):**
- "I read some brilliant proposals and then I read some that generally are just saying I'm going to do this one thing that doesn't it's it's very generic and there's nothing exciting or original about it and especially at PhD level" (00:10:46)

### What Does NOT Influence Decision:
- Budget: "It's good for me to see the budget...doesn't in any way influence my decisions" (00:19:59)
- Budget influence is minimal: "Often I'm just like, 'Oo, that's a lot.' Or, 'Oh, that's little.' But it doesn't really influence anything" (00:19:59)

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Hierarchy of Importance:

**Tier 1 (Decisive):**
- Motivation quality and sincerity
- Proposed study originality and feasibility
- Narrative continuity/believability

**Tier 2 (Confirmatory):**
- Academic history/transcripts
- Demonstrated past work

**Tier 3 (Peripheral):**
- Budget
- References (noted as helpful but not decisive)

### Confidence Weighting by Stage:

**Highest Confidence (Reading/Analysis Phase):**
- "When I when I start when I open an application and go into um the page that allows me to see everything that this candidate is about...reading about not just who they are, but what they want to do. Um what they've done in the past. That's my area of expertise" (00:19:59)

**Lower Confidence (Rubric Application Phase):**
- "The rubric...boxes me in and then I have to now go back and start thinking okay out of these four things what do I think is most appropriate and then it almost slows me down" (00:21:00)

### Synthesis Process:
- "You're kind of synthesizing it in a way that the rubric doesn't quite capture" (00:14:41)
- "I sometimes mark up or mark down just so that the final tally kind of looks like what it is that I think the person deserves" (00:13:16)

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Green Flags:

**Specificity and Personalization:**
- "Generic motivation immediately puts me at...this person isn't being very serious because what is your story?" (00:08:17)
- Applications showing unique personal narrative

**Clear Intellectual Thread:**
- "If an applicant writes about what influenced them to...I'm very keen on that and I'm keen often. It's interesting because often there's a string through that into what it is that they want to do" (00:09:30)

**Strong, Original Proposals:**
- "I read some brilliant proposals" (00:10:46)
- "Excitement" in the proposed study (00:10:46)
- "Original" and feasible ideas (00:35:58)

**Evidence of Genuine Motivation:**
- "There's a genuiness to some of the some of the applications that I appreciate" (00:08:17)
- Honest reflection on influences and experience

**Demonstrated Trajectory:**
- "Continuity...influences how I then approach that application" (00:09:30)
- Clear connection between past work and proposed direction

### Strong Application Example (Implied):
- Multiple positive indicators across motivation, continuity, and originality
- Clear narrative showing how applicant's background led to proposed work
- Feasible and original research proposal

---

## 5. RED FLAGS AND WARNING SIGNS

### Critical Warning Signs:

**Generic or Formulaic Motivation:**
- "Generic as is possible as they can possibly be...immediately puts me at okay this person isn't being very serious" (00:08:17)
- Boilerplate language suggesting lack of genuine engagement

**Lack of Narrative Continuity:**
- "Sometimes I don't [get continuity]. And I think that that in many ways influences how I then approach that application" (00:09:30)
- Disconnection between applicant's history and proposed direction

**Unoriginal or Generic Proposed Study:**
- "I read some...that generally are just saying I'm going to do this one thing that doesn't it's it's very generic and there's nothing exciting or original about it" (00:10:46)

**Mismatch Between Transcripts and Proposal Quality:**
- "Sometimes I also get some that are...really really good but then the person's history in terms of their transcripts don't quite match up" (00:25:22)
- Concern about capability: "Will they be able to" (00:25:22)

**Insincerity (AI-Generated Content Concerns):**
- "There's a genuiness...it's become a lot fuzzier with AI interventions where you know that somebody...there's a genuiness to some of the some of the applications that I appreciate" (00:08:17)
- Lack of authentic voice

### Process Red Flags:
- No feedback provided on assessments (applicant acceptance concern)
- Lack of oversight that decision is "not too far off" (00:29:08)

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

### Domain Background:
- Reviewer is in **Cultural Studies and Social Sciences**, trained in literature
- Expertise in "work around gender" (00:03:33)

### Discipline-Specific Application:

**PhD Applications:**
- "I find fewer PhD applications and obviously quite strong" (00:11:56)
- Higher expectations for originality: "Especially at PhD level" (00:10:46)
- Easier to evaluate: "With PhDs it's easier because often either the person is just about to finish" (00:24:17)

**Masters Applications:**
- Higher volume makes differentiation harder
- More variable quality: "With masters especially because there's so many of them...sometimes I feel I I I don't know" (00:11:56)
- Less developed proposals more common: "It's it's a proposal and so often it's it's it's it still doesn't have much weight in terms of um thinking process like they haven't thought through you know the entry" (00:10:46)
- **Requested change: "More range would work for me" at Masters level** (00:25:22)

### Subject Matter Applicability:
- No explicit mention of discipline-specific evaluation frameworks
- Criteria appear universally applied across submissions

---

## 7. HOW THEY HANDLE APPLICATIONS OUTSIDE EXPERTISE

**Not Explicitly Addressed** in transcript. However:

- Reviewer's confidence highest when reading/analyzing (their expertise area): "That's my area of expertise" (00:19

---

## Edzai Conilias Zvobwo

# Analysis of Application Evaluation Criteria & Process

## 1. EXPLICIT CRITERIA MENTIONED

### Stated Requirements:
- **Academic qualifications/grades**: "competitive in terms of grades" (00:14:52)
- **Topic quality**: "competitive in terms of the...topic they're doing" (00:14:52)
- **Meeting minimum requirements**: "They have to meet the minimum requirements. That's that's important thing." (00:11:45)
- **Rubric compliance**: Applications must satisfy all rubric dimensions; if they don't qualify, they are discarded: "if they don't qualify Then I discard literally" (00:11:45)
- **Research impact**: "their research uh is going to help as many people as possible" (00:15:56)

### Application Domain:
- **Science applications only**: "I do the science ones. Um I come from a science maths background. So I look at the specifically science and that includes uh across across the science uh spectrum whether it's uh natural science or or or any of the other ones." (00:09:28)

---

## 2. IMPLICIT CRITERIA (What They Actually Look For vs. Rubric Says)

### Applicant Background as Bias:
"So assuming they're applying for a master's uh scholarship. So you're going to look at their high school, their their undergrad performance and already by just looking at that you have a view of the person. Maybe they got too many Fs on their transcript in undergrad or they barely passed metric" (00:10:31)

**Implicit bias:** Past academic struggles bias initial judgment, though reviewer attempts to self-regulate: "so that I don't use that as a weapon against against them" (00:10:31)

### School/University Training Bias:
"certain schools seem to teach how to answer um these applications and then there are certain schools where quality is just bad. they they can't articulate the uh the impact of their studies" (00:04:10)

**Implicit observation:** Applications from certain universities (mentioned: "Limpopo and you know those other universities") are lower quality, suggesting bias based on applicant institution.

### Articulation vs. Actual Understanding:
Reviewer gives scores based on implicit comprehension even when application is poorly written:
"I've had a situation where I literally held their hand to say, 'Okay, I get what you're saying and I'll give you four because I I I get what you're saying, but you're not saying it, but I get it.'" (00:14:52)

**Implicit criterion:** Reviewer fills in gaps and applies subjective judgment about what applicant "really means" rather than what is explicitly stated.

### Personal Impact Resonance:
"I have a personal bias towards uh impact uh impact programs, impact uh studies that that will, you know, will will help the bottom line literally the the I mean the bottom of the pyramid uh people" (00:05:22)

**Implicit weighting:** Research addressing visible, immediate problems (water purification) weighted higher than theoretical advances (matrix multiplication algorithms).

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Primary Weighting: Personal Story/Resilience
**Highest weight:** "the story the story matters...the story is quite is quite um uh as a big awaiting uh because I think it's almost like a social justice element" (00:15:56, 00:18:20)

"So that's yeah, that's the one with the biggest waiting here" (00:18:20)

### Secondary Weighting: Impact Magnitude
"in terms of uh impact the natural bias where uh is almost towards how many people is it going to help you know and how fundamental is the problem" (00:17:07)

**Example showing differential weighting:**
"someone who's creating a new way to clean jobic water you know so I would go for the job water because I can see the immediate you know yeah so yeah so I think someone like that would deserve" vs. "someone is um is is doing um uh maybe in maths is is doing a way to to multiply matrices better" (00:17:07)

### Tertiary: Academic credentials and rubric compliance
Applied systematically but as elimination criteria: "if they don't qualify Then I discard literally" (00:11:45)

### Regression model analogy:
"if we're to do a regression model of of of the contributions of the various elements. the story is quite is quite um uh as a big awaiting" (00:15:56)

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### The Compelling Personal Narrative:
"they have a really uh compelling story. They have all the requirements and their their research uh is going to help as many people as possible" (00:15:56)

### Demonstrated Resilience:
"applicants are trying to show their resilience their story where uh they came from a broken home and uh against all odds they uh they tend to tended to you know overcome those obstacles" (00:13:46)

"So you're sort of saying as a human you're able to identify something about their grit or resilience or um the way in which life circumstances have helped to shape character" (00:13:46)

### Fundamental Problem-Solving with Visible Impact:
"How fundamental is the problem...someone who's creating a new way to clean jobic water" (00:17:07)

### Local/South African Contextual Relevance:
"move the needle in South Africa because uh there's always that uh contextual thing to say like like for South Africans right now where we are or where we want to go what what is it" (00:28:39)

---

## 5. RED FLAGS AND WARNING SIGNS

### Poor Academic Transcript:
"Maybe they got too many Fs on their transcript in undergrad or they barely passed metric, you know, that kind of thing" (00:10:31)

### Inability to Articulate Impact:
"certain schools where quality is just bad. they they can't articulate the uh the impact of their studies. Uh they tend to personalize I I I I it's about them instead of actually uh you know from them to the world" (00:04:10)

### Self-Focused Rather Than Outward-Focused Language:
Applications that focus on personal gain rather than societal benefit are flagged: "it's about them instead of actually uh you know from them to the world" (00:04:10)

### Niche, Non-Fundamental Research:
"some of these researchers they are really good but they are very niche and they are very uh first world oriented as opposed to to solving some of the fundamental problems that we have" (00:05:22)

### Failure to Meet Minimum Requirements:
This is absolute disqualification: "if they don't qualify Then I discard literally...They have to meet the minimum requirements" (00:11:45)

### Saturation/Trending Topic Participation Without Real Novelty:
"in a year maybe uh there's a hot topic like AI and you know people are just throwing in the the term AI you know that kind of thing" (00:28:39)

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

### Science-Specific Evaluation:
Reviewer specializes in science across spectrum: "I do the science ones...specifically science and that includes uh across across the science uh spectrum whether it's uh natural science or or or any of the other ones" (00:09:28)

### Subject Matter Expertise Limitations:
Reviewer acknowledges gaps in specific technical areas:
"I'm not a water purification expert. So if I could have a water purification expert shadowing and giving their second opinion uh around that particular one then yes that would uh that would add value because h I might be throwing away the next Nobel Prize winner because it it's not it doesn't look appealing" (00:26:06)

**Concern:** Reviewer may inadvertently penalize applications in areas where they lack expertise.

### Cross-Disciplinary Comparison Problem:
"you're comparing someone who's doing nanotechnology to someone who's doing animal husbandry uh the scope is quite is quite wide" (00:21:45)

---

## 7. HOW THEY HANDLE APPLICATIONS OUTSIDE THEIR EXPERTISE

### Self-Awareness of Limitations:
"I'm not a water purification expert" (00:26:06)

### Proposed Solution: Subject Matter Expert Shadowing:
"if I could have a water purification expert shadowing and giving their second opinion uh around that particular one then yes that would uh that would add value" (00:26:06)

**Specificity requirement:** "a subject matter expert in the particular research not in a broad area" (00:26:06)

### Current Risk:
"I might be

---

## Freedom Gumedze

# Analysis of OMT Application Evaluation Criteria
## Interview with Freedom Gumedze, UC Statistical Sciences Professor

---

## 1. EXPLICIT CRITERIA MENTIONED

### Core Evaluation Categories (from rubric):
- **Social impact** - mentioned repeatedly as emphasized throughout rubric
- **Academy contribution** - "what's there for the academy"
- **Applicant benefit** - "what's there for the applicants themselves"
- **Innovation** - whether something is "innovative or not"
- **Research quality** - assessing "the quality of research"
- **Publications** - track record of published work
- **Academic background** - educational credentials and preparation
- **Institutional fit** - whether chosen institution aligns with applicant's work
- **Budget** - reviewed as part of portfolio

### Discipline-Specific Criteria:
**By Application Level:**

*Masters:*
- "You're not looking at publications. If there are publications that's good, but you're not looking for publication"
- Academic background check
- Alignment with overseas institution

*Postdocs/PhD:*
- Publications expected
- Research impact metrics
- NRF rating status

*Sabbaticals:*
- "Higher impact of their work"
- Publications emphasized
- Research standing of individual
- Whether going to "particular sort of like institution overseas"

---

## 2. IMPLICIT CRITERIA (What They Actually Look For vs. Rubric Says)

### The Reading Strategy (Real Process):
**Quote:** "I'll read a portfolio. Obviously other things like you're looking at references, you're looking at whether there's a budget or not and things like that, but one kind of starts off looking at the core sort of content as to what the proposal what the proposal is about."

**Quote:** "I'll first read the the portfolio as I would read in a portfolio and then I go back to what the OMT looks like."

**Implicit Finding:** Reviewers read intuitively first, then check against rubric retroactively, rather than using rubric as initial guide.

### Benchmarking Against Memory:
**Quote:** "If you're trying to sort of like to to to to benchmark things, you try and look at is this is this the best sort of like proposal I've seen compared to previous one or something like that."

**Quote:** "if I've sat on the committee for five years I have the full five year record of of of how candidates have been graded. So something similar I would apply something similar here just to sort of try and benchmark"

**Implicit Finding:** Reviewers use informal historical comparison rather than absolute standards, creating year-to-year inconsistency.

### What Matters Most (Priority Order):
1. Core proposal content and feasibility
2. Candidate's background fit for work
3. References/support letters
4. Budget reasonableness
5. Social impact articulation

**Quote:** "budget is the last thing actually that I that I that I look at even though it's part of the it's part of the as part of the portfolio"

### Quality Assessment Without Guidelines:
**Quote:** "It's only if especially for postto and the sabatical it's only if you you are familiar with assessing the quality of research otherwise you can just look at the numbers and say h index of 30 this person is a very good when in fact what they are publishing is just it's just not up to standard."

**Implicit Criteria:** Relies on reviewer's tacit expertise; metrics can be misleading without domain knowledge.

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Research Quality/Publications: HIGHEST WEIGHT (but problematic)
**Quote:** "I'm I'm I've been saying about comparing with what I do in in other spaces, but I think I I do go back to look at what fourstar means in terms of what OMT puts down. One star I think that's kind of I shouldn't say it's no brainer, but again it's it's something is one star and something is four star to me."

**Quote:** "if you want to be thorough and in fact the the candidates are not told what to include they they obviously they could put in Google Scholar they could put in Scopas at my university something called biblometrics where you actually look at what the impact of the scholar is"

**Problem:** Inconsistent assessment method:
- Google Scholar vs. Scopus vs. Biblometrics not specified
- H-index alone insufficient indicator
- NRF ratings can be outdated

### Social Impact: EMPHASIZED BUT UNDERSPECIFIED
**Quote:** "you have got this social responsive uh social sort of like impact that you're looking for in fact everywhere you are looking for this social impact which I think it's a good thing but if you read the rubric in most instances it's it's kind of leans towards NRF sort of like way of assessing proposals"

**Quote:** "the sustainability goals goals you you want to see when where they they are contributing that's not asked of the of the candidates they can just put a follow on any way that they want"

**Finding:** Social impact expected but candidates get no guidance on how to demonstrate it.

### Institutional Alignment: MODERATE WEIGHT
**Quote:** "looking at they want to go overseas. Is that a good school or not? Is it aligned with what they are currently doing?"

### Budget: LOWEST WEIGHT
Reviewed for reasonableness but not emphasized in scoring.

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Positive Signals:
1. **Clear articulation of proposal** - "a sort of a a research project that are going to pursue"
2. **Strong fit between candidate background and proposed work** - "the person has chosen a school has got has got sufficient background has good background to actually pursue the work they saying they're going to be pursuing"
3. **Demonstrated research quality** - requires reviewer's tacit knowledge of field quality standards
4. **Innovation relative to field** - "I get a sense as to if I feel that something is innovative or not"
5. **Clear social benefit articulation** - "what's there for the for the for the applicants themselves"
6. **Alignment between social activities and career direction** - "social activities aligned with what they are sort of like proposing or where they are going with their career"

### Strong Indicator of Capability:
- **Relevant publication track record** (for postdoc/sabbatical level)
- **Acceptance to good overseas institution** - "In most cases you find that they have already been accepted at a school"

---

## 5. RED FLAGS AND WARNING SIGNS

### Critical Red Flags:

1. **Missing or poorly articulated information**
**Quote:** "there were applicants who even in the scoring obviously there will be one star candidate but there's some information that's probably missing or not properly articulated in the portfolio"

2. **Unclear educational background**
**Quote:** "something that's not properly articulated in the portfolio is educational background maybe that will be in the CV but what school are they going to"

3. **Misalignment between proposed work and institution/career direction**
**Quote:** "Is it aligned with what they are currently doing?"

4. **Outdated or inflated credentials**
**Quote:** "it could be that someone has got an NF rating but it's 5 years old or something like that. So it doesn't tell me about what they are sort of like at currently"

5. **Publications in low-quality journals masked by metrics**
**Quote:** "It's going to pick up something from MDPI or something. It's still going to be in Google Scholar...if you just have a number and you have a high Google Scholar sort of like H index and maybe you go with that but if you want to be thorough...maybe they are not they not at that at at at at that rating"

6. **Vague social impact claims**
No specific quote, but implied: When sustainability goals "can just put a follow on any way that they want" - suggests vague claims are a problem

7. **Sabbatical proposals without institutional anchoring**
**Quote:** "when it comes to sabaticals people will find that I want to go on sabatical but it's not necessarily going to a particular sort of like institution overseas or something like that. they just putting together a a sort of a a research project that are going to pursue in that six or one year period"

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

### Statistics/Data Science/Biostatistics Focus:

**Masters-level distinction:**
**Quote:** "The master's programs are changing. The rubric is built on a master's by dissertation. So the masters were seen at least in my discipline whether locally or overseas I mean if you're thinking about chemistry and other disciplines but if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part"

**Implication:** Rubric doesn't account for coursework-heavy master's programs in some disciplines.

**What matters in stats/biostatistics:**
- Application of "innovative statistical methods"

---

## Frelet De Villiers

# OMT Application Review Analysis: Frelet De Villiers

## 1. EXPLICIT CRITERIA MENTIONED

**Academic Record:**
- "The academic record is not that important to me because there are circumstances"
- "academic record for me...is the least important for me"
- Notes discipline-specific variations (e.g., music theory grades differ across institutions)

**Motivation/Personal Statement:**
- Assesses whether applicants are "complaining" vs. demonstrating agency
- Differentiates between "people just blaming their circumstances and others who really wants to be someone else"
- Looks for genuine motivation distinct from circumstance-blaming

**Research Proposal Quality:**
- "the proposal is very important because the openers must get value for their money"
- Assesses feasibility and realism of proposed outputs
- Evaluates internal consistency: "the golden thread inside the proposal is very important"
- Checks alignment between title, aims, methodology, and research questions
- Assesses value/significance of the study

**Reference Letters:**
- "the reference letters of the supervisors tell me a lot"
- Generic or short letters are major red flags: "If you get a letter which is very um generic and like only one or two paragraphs, I don't get a very nice feel of the student"
- Infers effort from depth: "If you really want this person to get something you will really put everything in that"

**Budget Realism:**
- Flags inflated costs: "if they really inflate the prices like that that is a red flag for me"
- Provides specific examples: "a ticket to um you know uh Europe is definitely not 35,000 rand"
- Checks necessity of travel/in-person work vs. online alternatives

---

## 2. IMPLICIT CRITERIA (What They Actually Look For vs. Rubric)

**Emotional/Contextual Understanding:**
- "I'm in the arts. So I think I'm I'm more um, prone to to be emotional perhaps about this and not as clearcut and um, you know, black and white perhaps as as other viewers will be"
- "I like to see what the circumstances are and their own um of course um motivations"

**Realistic Goal-Setting:**
- Applicants who promise unrealistic outputs signal disconnection from reality
- Example: "they say they are going to have four articles and three conferences and two books. I mean it's just not possible. So when you you look at the proposal, you can also see this person is really so far removed from reality"

**Writing Style Analysis:**
- Uses writing consistency to detect ghostwriting: "I'm looking at the this writing style if that makes sense"
- Checks if motivation letter and proposal are written by same person

**Intrinsic Value Assessment:**
- Beyond generic research value statements
- Looks for tangible, community-based, or discipline-specific value
- Examples of meaningful vs. generic value: "If they say the value will be to have a community program that will uplift um 20 community members...is there really value?"

**Respect for Funder's Resources:**
- "you you try to once again um let the opus not waste their money on on people but really invest in people who who can really go somewhere"

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

**Hierarchy of Importance:**
1. **Proposal quality** (highest weight) - internal consistency, feasibility, value
2. **Motivation letter** - reveals character and realistic self-assessment
3. **Reference letters** - depth indicates supervisor endorsement strength
4. **Academic record** (lowest weight)

**Comparative Process:**
- Reviews candidates in batches, comparing across the pool
- Not benchmarking against previous years: "I will not compare with previous years"
- Relative ranking within current cohort: "it's relative to this group usually"
- Uses gut identification of top candidates, then validates against scores

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

- **Demonstrates agency despite adversity** rather than listing hardships
- **Realistic, well-designed proposal** with internal coherence
- **Detailed, substantive reference letters** showing deep supervisor engagement
- **Reasonable budget with justified costs**
- **Clear line of reasoning** through title → aims → methodology → outcomes
- **Tangible, measurable value proposition** (not generic "advance knowledge")
- **Writing consistency** suggesting applicant wrote their own materials
- **Proportionate ambition** - achievable outputs within funding scope

---

## 5. RED FLAGS AND WARNING SIGNS

**Major Red Flags:**

1. **Generic/short reference letters**
   - "If you get a letter which is very um generic and like only one or two paragraphs, I don't get a very nice feel of the student"

2. **Inflated budget items**
   - "if they really inflate the prices like that that is a red flag for me"
   - Travel costs that don't align with market rates

3. **Unnecessary travel/in-person requirements**
   - "sometimes people will apply for funding and then they can just do it here. So or they can have online meetings with with um people and then they say they have to go to Europe to do this. So uh no you can just have online meeting"
   - Suggests: "these people just want to have a nice time going to and and there's not really value in why do they need to be there"

4. **Unrealistic output promises**
   - Multiple publications + conferences + books within funding period
   - Indicates detachment from reality

5. **Circumstance-blaming without demonstrated agency**
   - Heavy focus on obstacles without showing what they accomplished despite them

6. **Misalignment between motivation and proposal**
   - When motivation letter and proposal suggest different authors or motivations

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

**Music Department Background:**
- Acknowledges institutional variation in grading standards: "music theory is not the same as UC has so now as oh all our students usually get 80 and now I get someone that gets 60"
- Uses disciplinary experience to contextualize academic records

**Arts vs. STEM Orientation:**
- Explicitly identifies as more "emotional" and contextual than STEM reviewers
- Prioritizes qualitative factors (motivation, circumstances, proposal narrative quality)

**Cross-Disciplinary Review (NRF):**
- Reviews outside music/arts domains
- Doesn't require deep subject expertise for structural evaluation
- "the type of proposal are the same across all fields. you must have certain stuff"
- Consults domain experts when uncertain: "if I'm not sure about something, I will go and ask an expert in a certain field"

---

## 7. HOW THEY HANDLE APPLICATIONS OUTSIDE THEIR EXPERTISE

**Approach:**
- Focuses on **proposal structure and logic** rather than technical details
- "I will not be able to say okay all the sources that I use are excellent or whatever but the the type of proposal are the same across all fields"

**Expert Consultation:**
- "I will go and ask an expert in a certain field. I will not trust an assistant"
- Does not delegate evaluation to assistants due to subjective judgment requirements

**Confidence Boundaries:**
- Acknowledges limitations but doesn't use this as disqualifier
- Applies consistent structural/logical evaluation across disciplines

---

## 8. VERBATIM QUOTES - ORGANIZED BY THEME

### On Academic Records
- "The academic record is not that important to me because there are circumstances"
- "academic record for me...is the least important for me"
- "there are circumstances um I know from our own students they just have the most horrible here and just nothing works out for them"

### On Motivation Letters
- "when you look at the motivation you can see when they are just umum complaining if that makes sense"
- "You you get some people and they will say I come from this very bad background and I don't have this and I don't have that. Um then you get other people who say I come from the background but I did this with what I have already"
- "you really try to differentiate between people just um blaming their circumstances and others who really wants to to be someone else"
- "the motivation letter tells me a lot about the person itself"

### On Reference Letters
- "the reference letters of the supervisors tell me a lot. If you get a letter which is very um generic and like only one or two paragraphs, I don't get a very nice feel of the student"
- "If you really want this person to get something you will really put everything in that"
- "you can just believe what you get and you also have to believe the facts that you get"
- "if if they lie to me it's on their black book"

### On Proposals
- "the proposal is very important because the openers must get value for their money"
- "Is it really viable? Is it really something that can work?"
- "they say

---

## Martin Clark

# Analysis of OMT Application Evaluation Criteria
## Martin Clark Interview - January 15, 2026

---

## 1. EXPLICIT CRITERIA MENTIONED

### Rubric-Based Criteria
- **Golden Key Society membership** - "one metric for excellence of a candidate is being part of the golden key society"
- **Academic grades** - assessed as part of evaluation
- **Project significance** - expected to demonstrate "national or global significance"
- **Budget feasibility** - "I always look at what's being proposed as the budget"

### Formal Scoring Structure
- 1-4 point rubric scale on multiple categories
- Final 5-point assessment scale

### Institutional/Disciplinary Calibration
- Applications parceled by level: Master's, PhD, postdoctoral, sabbatical
- "I try and calibrate uh what my expectation of excellence is at those levels"

---

## 2. IMPLICIT CRITERIA (What They Actually Look For vs. Rubric Says)

### Pattern Recognition & "Tacit Knowledge"
**Verbatim:** "I bring expert judgment, or 'tacet knowledge,' to the process, which involves recognizing patterns, such as a positive outlook for change and aspiration, as key metrics for generating future leaders"

### Character and Passion Assessment
**Verbatim:** "I always value passion higher than I I value cleverness. Uh you can always you can always teach passion but you can't build passion always."

**Verbatim:** "there are projects and there are candidates I get where um I see a candidate who is excellent um but the grades or the life choices or their narrative don't always articulate that"

### Looking for Future Leadership Potential
**Verbatim:** "I look for a little bit of um aspiration as well. And uh that that's a key metric for the OMT to to really be generating the funding the next world leaders. Um or the people who will lead in respective categories."

### Positive Outlook for Change
**Verbatim:** "one thing would be uh a positive outlook for for change. Um maybe that's maybe that's uh local community change, maybe that's global change."

### Reading Between the Lines - Character Inferences
**Verbatim:** "you need multiple dots on a graph to see a curve. Um and sometimes I have more dots and sometimes I have less dots. Um, so to what degree can I accurately um um see the dots that were missing or are missing?"

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Explicit Weighting Structure Exists But Isn't Transparent
**Verbatim:** "there's a clear waiting structure in the adjudication that um that again speaks to what the foundation is looking for"

**Verbatim:** "the various elements of the rubric are also weighted in a way that they would like to weight it and and um that is not always clear to me but um honest honestly again as the adjudicator it doesn't necessarily need to be clear to me it needs to be clear to them"

### Balancing Variables (Not Equal Weight)
**Verbatim:** "adjudication involves balancing a set of variables that are not always weighted equally, which can sometimes constrain how well they can adjudicate an application that is not clearly excellent across all metrics"

**Verbatim:** "I do my best to weigh what the project is, what the capability of the applicant is against what could be considered minor elements in what is submitted by that applicant"

### Candidate Circumstance vs. Absolute Standards
**Verbatim:** "I might consider excellence among what is possible with the time. um the time this candidate has had"

**Verbatim:** "does that mean that um the the the applicant with 20 years of industry experience who's in their 50s is more um appropriate to achieve excellence than someone who is who is striving for it at 21? Those are those are the types of things I have to weigh uh in my adjudication"

### Project Feasibility Flags Issues
**Verbatim:** "if it's not feasible uh even if the it's it's a cure for cancer whatever it may be then um I need to flag that in my adjudication"

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Clear Excellence Across Multiple Dimensions
**Verbatim:** "I'll read an application and I can sit back and I can say it's excellent uh from a variety of different ways and then it becomes quite easy"

### Demonstrated Passion & Aspiration
**Verbatim:** "I I try and look at those elements... I always value passion higher than I I value cleverness"

### Evidence of Impact Potential (Even at Local Level)
**Verbatim:** "I might review an application of someone from a disadvantaged background and that disadvantaged background would articulate how they would want to make a change in their community and this is this is the the passionate thing"

### Evidence of Tenacity and Character
**Verbatim:** "circumstances could perhaps be understood if for instance um I'm looking at tragedy in one's life or or or elements that show tenacity um and budding excellence"

### Appropriate Calibration to Career Stage
**Verbatim:** "When it would come to a a a very let let's call it a master's program um I I assessed... And those boxes become a lot more fuzzy as I look at something like a master's degree where um they've only had a finite amount of time"

### Well-Articulated Positioning (Even for Niche Topics)
Evidence that applicant understands how their work fits into broader significance frameworks - shown through deep diving on butterfly subspecies example

---

## 5. RED FLAGS AND WARNING SIGNS

### Surface-Level Errors (Discipline-Dependent)
**Verbatim:** "if I if I spelling mistakes, especially in a CV or something like that, um, I personally feel like that should be judged rather harshly"

**CAVEAT:** **Verbatim:** "Um but not everyone is has availability of um uh word processing, spellch check engines and not everyone has had that has developed an appreciation of that"

### Lack of Resource Access Indicators
**Verbatim:** "not everyone has had that has developed an appreciation of that I guess that that type of pedantic judgment. So does that preclude a good applicant or not?"

### "Clear Caveats" in Application
**Verbatim:** "When I have an application that has clear uh caveats in the application, that's where uh there's there's quite a lot of humming and hawing going on"

### Misaligned Budget (Too High or Too Low Without Justification)
**Verbatim:** "sometimes there are excessive budgets that don't speak to what that project is and sometimes there are extremely modest budgets and um that's another element of feasibility of what is being proposed"

### Project Scope Doesn't Match Applicant's Articulated Understanding
**Verbatim:** "grades and the way that the candidate actually expresses themselves and expresses the project do not align between institutions"

### Lack of Clarity on Project Significance
**Verbatim:** "I need to come to account what does that subspecie species of butterfly mean on various context scales"

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

### Expertise-Dependent Confidence
**Verbatim:** "Where do you feel the most confident in your decisions and where do you feel the least confident... it's always more difficult in um uh projects that are towards the periphery of my expertise. Um it's very easy when it's within my my wheelhouse"

### Disciplinary Variation in Rubric Application
**Verbatim:** "I find that the categories provided are rather rigid. So um sometimes the rating I feel could be um more accurately made if the boxes were were more nebulous or the boxes could actually change based off of discipline or category"

### Level-Based Expectations Differ by Discipline
**Verbatim:** "I expect someone who's applying for a posttock or sabbatical to to really know where it fits on on those scales. And those boxes become a lot more fuzzy as I look at something like a master's degree"

### Subject Matter Expert Gap Areas
**Verbatim:** "I I have um an earth science BSC which means I I should be able to consider topics on a broad range of things. But I can tell you that um with a first year, secondyear biology um I might not appreciate um diseases that um are specific to a caterpillar species for instance"

---

## 7. HOW THEY HANDLE APPLICATIONS OUTSIDE THEIR EXPERTISE

### Multi-Angle Adjudication Approach
**Verbatim:** "I do my best to um adjudicate from multiple different angles"

### Deep Diving/Research Process
**Verbatim:** "It might be something very niche it might be looking at a subspecies of butterfly for instance and an

---

## Maureen De Jager

# Analysis of OMT Application Assessment Criteria

## 1. EXPLICIT CRITERIA MENTIONED

**Academic Performance:**
- Academic transcript required
- Academic achievement in relation to proposed study
- Distinction or high marks in relevant practice-based subjects preferred
- Final results may not be directly relevant; requires interpretation

**Creative Work:**
- Portfolio (described as "critical" and "highly detrimental" if absent)
- Evidence of practical excellence
- Technical and conceptual sophistication of work produced

**Application Materials:**
- CV
- Motivation letter/statement
- References (minimum one from current or prospective research supervisor recommended)
- Writing sample (mentioned as institutional practice)

**Program Fit:**
- Alignment between articulated goals and actual ability
- Coherence across application materials
- Motivation for overseas study (must demonstrate unavailable locally)
- Relevance to South African context/society

---

## 2. IMPLICIT CRITERIA (Rubric vs. Actual Practice)

**What rubric says vs. what they actually assess:**

| Rubric Expectation | Actual Assessment |
|---|---|
| "Personal motivation" with background/interests | Looks for evidence of *values and priorities* not explicitly stated |
| "Plan to achieve vision" | Recognizes this is "odd" and "confusing"; prefers evidence of coherence and how candidate would "build on, develop, extend" |
| Significance to South African society | Acknowledges this is difficult because creative practice is exploratory; answers often "unpersuasive" and "floaty" |
| References as standard indicators | Largely disregards unless referees are "strong, credible" and known; suspects bias from friends/partners |

**Key quote on misalignment:**
> "there's a slight misalignment in that you ask then as the reviewer um to look for evidence of values and priorities and sometimes the personal the the personal motivation is framed more as just a narrative... we're looking for something that isn't necessarily in evidence."

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

**Hierarchy of importance:**

1. **Portfolio (Highest weight)**
   - Can override strong academic records
   - "Often my mind is changed by the portfolio"
   - Can reveal misalignment between articulation and actual ability

2. **Coherence across application**
   - "there needs to be a kind of coherence where um the way that the person articulates their work aligns with the work aligns with how they see it developing"
   - "if there's a coherence then it's automatically persuasive"

3. **Academic record with contextual interpretation**
   - Must interpret institutional rigor: "a 75% from a particular institution is probably worth more than a 75% from another institution"
   - Only relevant when connected to proposed study

4. **Program fit and motivation**
   - Higher scrutiny for overseas applications (cost factor)
   - Must demonstrate program unavailable locally

5. **References (Lower weight)**
   - "I don't pay them too much heed" unless referees are known to be credible
   - Concern about bias and conflict of interest

**Comparative weighting by level:**
> "I tend to be more forgiving for Master's applicants than PhD candidates"

**Cost-based weighting:**
> "I tend to be more strict if somebody's going to be doing an overseas-based study and it's going to be costing OMT a lot of money"

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

**Strong indicators:**

1. **Demonstrated technical and conceptual sophistication in portfolio**
   - Work quality matches articulated proposal strength
   - No gap between "on paper" credentials and actual creative output

2. **Clear, grounded vision**
   - Not "generic and floaty"
   - Significance grounded in actual practice rather than abstract benefit claims
   - Example of vagueness: *"I'm going to make art and it's going to benefit society because we all need nice things to look at"*

3. **Coherence throughout application**
   - How they articulate work aligns with the work itself
   - Aligns with how they see it developing
   - All elements "add up"

4. **Portfolio with contextualization**
   - Includes annotation explaining choices
   - Shows understanding of own practice direction
   - Demonstrates clarity about artistic discipline/approach
   > "annotation or something where the student speaks to what's in the portfolio... contextualization um of what was informing the making of the work"

5. **Strong institutional track record**
   - High marks in relevant subjects (distinction in practical subject)
   - Evidence of progression in Year 1 if applying for Year 2 funding

6. **Specific, evidenced justification for overseas study**
   - Can articulate what makes the program unique
   - Not relying on applicant claim alone; some evidence needed

7. **Long-term planning aligned with study**
   - For overseas PhD: demonstrated plan to implement learning (e.g., developing program upon return)

---

## 5. RED FLAGS AND WARNING SIGNS

**Critical concerns:**

1. **Missing portfolio**
   - "highly detrimental if there isn't one"
   - Cannot assess practical ability without it

2. **Misalignment between articulation and ability**
   - Strong proposal but weak portfolio
   - High academic record with technically/conceptually weak work
   > "somebody can articulate an idea for where they see their research going and it sounds great... and then you look at at the caliber of the work that's being produced... there's a very clear misalignment"

3. **Generic, ungrounded significance statements**
   - Vague claims about societal benefit
   - Not anchored in specific practice concerns
   - Described as "floaty responses... not really sort of anchored in"

4. **Lack of coherence**
   - Elements don't "add up"
   - Articulation doesn't match work doesn't match stated direction
   - Not persuasive "no matter where it falls in terms of the overall cohort"

5. **Evidence of AI-generated content**
   - Suspicious if writing is polished beyond student's apparent level
   - "very easy to write a convincing proposal... and then you're wondering how much of that is really this the student being able to articulate"

6. **Biased or questionable references**
   - Reference from intimate partner flagged as concerning
   - References from friends seen as unreliable
   - Preference for at least one reference from current/prospective supervisor

7. **Unsubstantiated claims about overseas program uniqueness**
   - Applicant claims "nothing like it locally" without evidence
   - Reviewer must verify independently

8. **Incomplete applications**
   - Missing required documents
   - Unanswered questions
   - Not meeting stated criteria

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

**Fine Arts/Creative Practice specific:**

1. **Portfolio evidence is essential and non-negotiable**
   - Cannot assess practice-based degrees without it
   - Different from academic disciplines

2. **Understanding of artistic discipline/practice type**
   - Assessor asks: "are they a sculptor are they a print maker do they work in a kind of cross-disciplinary way"
   - Needs clarity on practice direction

3. **Technical skill evaluation**
   - "technical and conceptual sophistication to be able to pull off what they've articulated"
   - Visible in portfolio work quality

4. **Exploratory nature of creative practice acknowledged**
   - Applicants struggle with "significance" questions because practice is exploratory
   - Answers inherently tentative and difficult to predict
   > "creative practice is exploratory you know it kind of like opens up towards wards um realizations and and understandings... it's very difficult to know before you've even set out on this journey where you're going to end up"

5. **Evidence of practice development trajectory**
   - How would they "build on, develop, extend" their practice
   - Continuation planning important for Masters/PhD

6. **Written component expectations**
   - Even practice-heavy programs require written work
   - Assessor checks if applicant can "write persuasively in an academic manner"

---

## 7. HOW THEY HANDLE APPLICATIONS OUTSIDE EXPERTISE

**Within their expertise:**
- Focus primarily fine art but comfortable with performance-based and graphic design submissions
- Has not been "pushed to sort of assess something that I just have no sight line on"

**External knowledge leverage:**
- Sits on national DHET panel for creative outputs review (provides broader sightline)
- Uses institutional knowledge from own university's assessment practices
- Familiar with sector rigor across institutions

**Unknown/adjacent areas:**
- When unfamiliar with specific programs: "you're relying very much on um the applicant saying there is nothing like it rather than you know what your own kind of knowledge"
- Suggests need for

---

## Mohamed Cassim

# OMT Application Evaluation Criteria Analysis
## Interview with Mohamed Cassim

---

## 1. EXPLICIT CRITERIA MENTIONED

### Academic/Administrative Criteria
- **Academic achievement**: "Academic achievement is exactly what they're asking for on the stars of candidate and that's straightforward. I mean that's kind of administrative if you think about it."
- **Intended study**: Assessed for significance across fields (double-scored on rubric)
- **Budget/Financial items**: Specific factual questions about funding amount, other funding sources, personal investment, project timeline
- **Research excellence**: Evaluated based on candidate quality

### Behavioral/Commitment Criteria
- **"Skin in the game"**: "I'm a firm believer that if you don't have skin in the game or a really good reason to get the job done, right, no matter what the merit, you're halfway out the door."
- **Personal motivation**: Assessed from "how you've built yourself, how you've approached what you say your interests are, what you've done to advance your interests"
- **Probability of delivery**: "It's also about the probability of that being delivered... ultimately it's not quite simply about the merit of the intent cause everybody can write a brilliant piece of intent."

### Experience/Context Criteria
- **Depth of achievement**: "I'm able to reference fairly quickly the depth of a submission... I can tell you fairly quickly if someone's blustering or if they've actually achieved something."
- **Institutional performance of employer**: "If a person has significant or substantial experience... I would use the performance of the institutions where this person was at as part of my considerations"
- **Field/sector exposure**: "What sectors were these people exposed to? What are the potential of these sectors?"
- **Leadership experience relevance**: "Particularly in their function... if they were in some kind of position of leadership"

---

## 2. IMPLICIT CRITERIA (Practice vs. Rubric)

### What Mohamed Actually Looks For (Beyond Stated Rubric)
- **Authenticity of motivation letter**: "I think a letter you can get a good friend to write it. Hell, you can get AI to write it for you and you can tweak a few words... I don't think the letter is the key to that discussion."
  - Instead assesses through: "How you've built yourself, how you've approached what you say your interests are... how you would perform if you had an opportunity, that sort of stuff you can take out through the rest of the paper"

- **Reference letter quality and genuineness**: 
  - Red flag: "Sometimes you get a guy who gets three letters from people around him. You can see he's written it and he's tweaked a few words and gotten someone to sign it."
  - Positive indicator: "Really heartfelt proper letters from people who were their mentors or their um seniors or their professors... if it feels genuine and it's real and if the three are from different angles aiming at a person's character or their capability makes a real difference"

- **Capacity to manage multiple obligations**: "I would write on a separate comment box just check on the capacity of this person to be able to deliver on the academic obligations... they may have too much that they're busy doing and it may affect their lives"

- **Logic and thinking quality**: "I like taking a look at how they think about and how they construct their logic"

- **Intentionality vs. bridge funding**: Identifies candidates using funding as placeholder: "Sometimes you can see a guy just collecting funds very quickly so that he can spend a couple years filling up time in order to get somewhere later. It's a bridge. It's not really for the thesis and it's not for the content. Um, and those I push aside fairly quickly."

- **Honesty about achievements**: Cross-references CV against actual institutional performance: "I would love for it to tell me that from this guy's CV I checked on the institutions... I brought back the following financial statements and stuff for those companies... to check if they're being honest about their achievements"

- **National context factors**: "One of my big bug bears is that as a country we, you know, our throughput on NSF... for every five students we pay for, we get one degree roughly... I factor into my process."

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Two-Step Methodology
1. **Own merit assessment first**: "If it doesn't stand up on its own merit then I wouldn't recommend it anyway"
2. **Relative calibration second**: "Sometimes it's hard to choose between one or the other when you're calibrating. And so I use the relativity is just to calibrate"

### Weighting Hierarchy (Inferred from Emphasis)

**Highest Priority:**
- **Probability of delivery** (trumps merit of intent): "It's also about the probability of that being delivered... OMT is running a portfolio... if this particular piece of study holds that much merit then I do want to make sure that the probability of being delivered is high"
- **Skin in the game / commitment**: "No matter what the merit, you're halfway out the door" [without this]

**High Priority:**
- **Institutional/sector context**: Where candidate has worked, how those institutions performed
- **Quality of reference letters**: "Those I take to heart"
- **Demonstrated logic and thinking quality**

**Medium Priority:**
- **Intended study significance**: Important but requires "broad spectrum experience" to assess
- **Personal motivation**: Assessed indirectly through actions and history

**Lower Priority (but noted):**
- **Personal motivation letter alone**: "Letter is not the key"
- **Academic achievement** itself: Described as "administrative"

### Cross-Factor Considerations
- Mohamed adjusts weighting based on capacity: "If that means I need to support this human being in other ways, then I got to do that if I want this job to be done"
- Example from investment thinking: "Technically you're spreading the full value... in a way that'll get you to the other side where it delivers the returns that it should"

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Evidence of Depth & Achievement
- "Substantial experience" with 10+ years at quality institutions
- Clear pattern of advancing stated interests through actions
- Demonstrated logic and thinking quality in how problems are approached

### Authenticity & Commitment Indicators
- **Personal financial investment**: "Has he put some of his own cash in?"
- **Authentic reference letters**: "Really heartfelt proper letters from people who were their mentors"
- **History aligns with stated interests**: Pattern of actions supporting the articulated motivation
- **Appropriate scope**: Understanding realistic timeline and effort required

### Exceptional Cases Mentioned
- One candidate was so strong Mohamed "picked up the phone" to Tracy: "You've left value on the table. That guy, you're not going to see a profile like that in a hurry"
- Description: "Incredible candidate" with a profile "you're not going to see... in a hurry"

### Demonstrated Strategic Thinking
- Clear understanding of future impact: "What would this guy like to do for his next round that'll make a difference in this world"
- Sector-specific insight: Understanding where intellectual property and economic value can be created
- Realistic assessment of constraints and enabling factors

### Cross-References Verify Claims
- Institutional performance aligns with CV claims
- Reference letters support and extend CV narrative
- Financial situation allows genuine focus on thesis work

---

## 5. RED FLAGS AND WARNING SIGNS

### Critical Red Flags (Disqualifying)
- **No skin in the game without justification**: "If you don't have skin in the game or a really good reason to get the job done, right, no matter what the merit, you're halfway out the door"
  - Exception: Recognized if "you can see a guy... and they'll say look I support three people in my family and I do this and I do that"

- **Using funding as bridge time**: "You can see a guy just collecting funds very quickly so that he can spend a couple years filling up time in order to get somewhere later... those I push aside fairly quickly"

- **Dishonesty about achievements**: Cross-checking CV against institutional performance reveals false claims
  - Indicator: "Reference letter. They say things that don't quite stack up to what the performance looks like"

- **Capacity overload**: Acknowledges family/work obligations but applies anyway: "They may have too much that they're busy doing and it may affect their lives"

### Medium-Level Concerns
- **Artificial reference letters**: "You can see he's written it and he's tweaked a few words and gotten someone to sign it"
  - Though acknowledged as sometimes unavoidable: "Sometimes people are busy. I've had some really senior people say, 'Write the letter and I'll decide'"

- **Generic or poorly-written motivation letter**: Suggests candidate didn't invest effort or used AI
  - "You can get a good friend to write it. Hell, you can get AI to write it for you and you can tw

---

## Philippe Burger

# Analysis of OMT Application Evaluation Criteria
## Philippe Burger - Adjudication Interview

---

## 1. EXPLICIT CRITERIA MENTIONED

### Primary Screening Factors:
- **Academic Record**: "the first thing I look at is the academic record to to to see how they how they they fare" (00:03:30)
- **Motivation Essay**: Core evaluative document
- **Field Alignment**: Match between intended field of study and existing qualifications
- **Degree Level Appropriateness**: Different standards for Masters, PhD, and Scholar applicants

### Specific Essay Assessment Markers:
- **Focus**: "do they have a focus is it is it clear what they want to do" (00:08:08)
- **Realism**: "how realistic is it that they want to do?" (00:08:08)
- **Clarity**: General clarity of goals and intentions
- **Ambition**: "can you spot a bit of ambition or is this just, you know, a run-of-the-mill I'm going to change the world?" (00:13:53)
- **Thoughtfulness**: "has there's been some thought in it?" (00:13:53)

### Field-Specific Requirements:
- **Quantitative Competency**: For quantitatively demanding fields - "If you've only done a first or second year stats module, you're not going to succeed in this" (00:27:19)
- **Disciplinary Foundation**: Particularly for interdisciplinary applications - "they might not have the disciplinary foundation uh to to to to tackle that particular topic" (00:05:51)
- **Literature Mastery**: Varies by level - PhD candidates should already "be master of the literature" (00:10:33)

---

## 2. IMPLICIT CRITERIA (Rubric vs. Actual Practice)

### The Subjectivity Gap:
- **Stated Process**: Numerical scoring rubric (scale of 1-5)
- **Actual Practice**: Heavy reliance on gut feeling and iterative adjustment
  
> "assigning a score is always somewhat subjective, as it involves translating an impression into a numerical score" (00:09:27)

> "there's always uh in that because you you do a translation of what your impression into a into a score. Um so so there will always be something uh uh um in there that that's a bit subjective" (00:09:27)

### Unstated But Active Criteria:

**Authorship Authenticity** (increasingly important):
- English improvement patterns suggesting AI usage: "you the English all of a sudden improved a lot uh for some of these cases which suggest uh that it might be Chad writing them instead of the candidate" (00:03:30)
- Personal voice and authenticity: Preference for applicants showing "a bit of flare, the bit of drive, the bit of focus, reflecting that you've understood the issue" (00:16:45)

**Socioeconomic Background Considerations**:
- NOT penalizing for language sophistication: "you don't you're conscious to not penalize people for that" (00:20:59)
- Awareness of educational inequity: Students "did not necessarily have a model ski model C school training. So um their use of language might not be what a kid from from a private school uh would have" (00:20:59)
- Recognition of diverse preparation: "I think particularly particularly in a country like this uh you know you you you have have students coming from very diverse communities" (00:22:21)

**Personal Narrative Importance**:
- Background explanation through essays: "I think that that that is very that that's very important. Um so you need to understand where people come from... that's also why the personal essay is is in a in some sense is important" (00:22:21)

**Institutional Fitness Assessment**:
- Absolute vs. relative benchmarking: "we send somebody say yeah you're number one in our pack of 20 but then they will still fall short of of of of what what is need needed to make a success of their qualification" (00:25:54)

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Hierarchical Decision Tree:

**Stage 1 - Hard Filter (Highest Weight):**
- Academic record as initial gatekeeper: "the competition is stiff and it's really just the top few who make it" (00:03:30 - 00:04:38)
- Insufficient marks eliminate applicants pre-adjudication: "I mean, but the marks are are not the marks won't let them qualify" (00:19:20)

**Stage 2 - Alignment Assessment (High Weight):**
- Field-qualification fit for eliminating mismatched candidates
- Prerequisite competency check (especially quantitative): "if you are a student who wants to go now study something that that is uh let's say uh uh mathematically quantitatively but but but you know South African universities are not that strong in uh in in preparing students for that" (00:19:20)

**Stage 3 - Essay/Motivation (Heavy subjective weight, determines final ranking):**
- The "biggest issue" in adjudication: "when when it is more about the self-escription essay or or later on when you look at what what the motivation for what they want to study um there's a bit more to that uh but but when it's this motivation essay I think that is where the the biggest issue comes in and that that is very subjective" (00:08:08)
- Differentiator between borderline candidates: Used to move from "3 to a 4 or four to a five" (00:12:43)

**Stage 4 - Value-Add Assessment (For edge cases):**
- "value added that that that you bring" (00:12:43)
- Specificity of goals vs. generic aspirations
- Evidence of genuine thought vs. boilerplate statements

### Level-Specific Weighting:

**Masters applicants**: "you you need some basic skills demonstrate ated uh is is a bit more basic a bit more foundational" (00:10:33)

**PhD applicants**: "for a PhD you would assume that the person is already master of the literature you are in position of a master's degree uh so you you sort of know the field" (00:10:33)

**Scholars**: "if it's a scholar then it really needs to be good just to get an average" (00:10:33) - highest standard

### Iterative Weighting Adjustment:
Scores are not fixed but adjusted as more applications are reviewed:
> "you adjust your ranking as as you go... let's first see where where the others fall out uh or pitch. Um and and and then as you start working through it, you you start getting an idea who is higher and who's lower" (00:06:59)

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Essay-Level Excellence:
- **Demonstrable Ambition**: Beyond generic statements
  > "can you spot a bit of ambition or is this just, you know, a run-of-the-mill I'm going to change the world? Uh, has there's been some thought in it?" (00:13:53)

- **Evidence of Deep Thought**: 
  > "has there some been some thought in in in in in that motivates you to to to do this?" (00:13:53)

- **Specificity of Purpose**: 
  > "is it I just want to go do a masters in I don't know public administration or is it something specific that you want to go go find out" (00:12:43)

- **Clear Focus and Realism**:
  > "do they have a focus is it is it clear what they want to do um or is it a bit fairy. Um, how realistic is it that they want to do?" (00:08:08)

- **Distinctive Voice**: 
  > "the bit of flare, the bit of drive, the bit of focus, reflecting that you've understood the issue" (00:16:45)

### Authentic Authorship Signal:
- Personality reflected in writing
- Specificity indicating genuine understanding
- Inability to be replicated by current AI: "these things that I highlighted, you know, the the bit of ambition and so on, uh, the bit of flare, the bit of drive, the bit of focus, reflecting that you've understood the issue, you could still pick that up from from from your better essays" (00:16:45)

### Academic-to-Goal Alignment:
- Clear disciplinary foundation for intended research
- Appropriate prerequisite knowledge for quantitative demands
- Logical progression from previous studies

### Exceptional Academic Performance:
- Baseline requirement, but excellence here necessary for consideration
- Especially important for Scholar level

---

## Pieter Pistorius

# Analysis of OMT Application Evaluation Criteria and Process

## 1. EXPLICIT CRITERIA MENTIONED

### Primary Evaluation Criteria:
- **High integrity and high caliber individuals**: "OMT plays great store on sort of well clearly on high integrity, high caliber applications" (00:06:39)
- **Community contribution**: "it will go back into some form of a community. It could be an academic community or the industrial community or what whatever the case may be" (00:06:39)
- **Career trajectory alignment**: "how this career fits together the career as at at that point and how this application would would move that specific applicant's career into a specific direction" (00:06:39)
- **South African retention/ties**: "how deep are the ties to South Africa... are you funding them to immigrate or you funding them to come back and in some way you know enrich enrich the the South Africa" (00:06:39)
- **Consistency and discrepancies**: "to look at discrepancies and consistencies in a sense" (00:05:17)
- **Application coherence**: "does it make sense?" (00:05:17)

### Rubric-Based Assessment:
- Four or five-point scoring system
- Multiple evaluation dimensions (mentioned "five or six different ways in which you evaluate it") (00:08:19)
- Rubric is "quite useful" (00:14:37)

---

## 2. IMPLICIT CRITERIA (What They Actually Look For vs. Rubric)

### True Motivation Detection (Greatest Challenge):
- **Stated struggle**: "my greatest struggle in judgment is determining the true motivation of the applicant, such as whether an application is a last resort" (00:12:04)
- **Direct quote**: "What is really driving this this specific individual?" (00:13:38)
- **Specific example**: "somebody that applied for a support for a sabbatical leave... one I got this sense that he or she um was that this was the sort of point of application of the last resort" (00:12:04-00:13:38)
- **Implicit test**: Industrial background influences this—"it is somewhat cynical I think but you know it is maybe it's with my time in industry but if somebody asks you something you should figure out why they do that" (00:13:38)

### Social Context and Personal Background:
- "I consider that essentially I consider that as part of the dimension" (00:09:26)
- **Example given**: Student whose father's medical emergency led to medical engineering pursuit (00:10:23)
- **Nuanced view**: "somebody that comes from a impoverished neighborhood or or context may be an excellent candidate or mediocre you know so... you need to I think you need to look at the social context" (00:10:23)

### Superficial Consistency Checks:
- Verification of postgraduate department existence and relevance (00:05:17)
- Timeline coherence across academic records
- Gap identification in career progression

### Memorability and Halo Effect Management:
- Recognizes applications are "very memorialable" (00:08:19)
- Actively avoids halo effect: "not to finalize it until I've gone through everybody" (00:08:19)
- Forces extreme scoring to avoid middle-ground bias: "not to give everybody three out of five" (00:14:37)

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Process Flow (Weighted by Time Investment):
1. **Initial scan**: 5 minutes per batch to "see who they are" (00:04:06)
2. **Deep document review**: 45 minutes to 1 hour per applicant (00:04:06)
   - Reading all documents in one sitting
   - Checking discrepancies and consistencies
   - Assessing broad strokes of application
3. **Overall consistency check**: ~20 minutes after reviewing all applicants (00:08:19)
   - Looking for outliers
   - Identifying any halo effect issues

### Calibration Strategy:
- **First pass**: Score extremely to avoid center bias (00:14:37)
- **Second pass**: Review for outliers and inconsistencies (00:08:19)
- **Final resolution**: Compare rubric scores to intuition and investigate discrepancies
  - "If there is a discrepancy between their initial intuition and the rubric score, Pieter Pistorius follows the rubric" (00:19:13)

### Confidence Hierarchy (Explicitly Stated):
**Most Confident**: Academic career assessment
- **Quote**: "the the one is probably in terms of academic career you know because that's the environment in which I work. So I know what the distinction is... I know what it takes to complete a PhD in two and a half years" (00:12:04)

**Least Confident**: True motivation assessment
- **Quote**: "the least confident is probably this consideration of what is the true motivation... I try to figure it out and sometimes I work it out sometimes I don't" (00:12:04)

### Rubric vs. Intuition Resolution:
- **Follows rubric**: "I would uh follow the rubric" when discrepancies arise (00:19:13)
- **Critical re-evaluation**: "I would go back to the sort of individual parameters and and look at that again" (00:19:13)
- **Corrects first impressions**: "this individual looked very impressive, but if you look at the details, maybe it wasn't, you know, first first impression maybe may be incorrect" (00:19:13)

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Academic Excellence Signals:
- Completing PhD in accelerated timeframe (2.5 years mentioned as benchmark)
- Clear progression through academic ranks
- Coherent research trajectory

### Authentic Motivation:
- Personal experiences driving career direction (medical engineering example from father's medical emergency)
- Evidence of deep commitment to field rather than opportunism
- Seeking funding as confirmation of direction, not as "last resort"

### South African Engagement:
- **Explicit priority**: "very probably very old-fashioned so somewhat naive argument but I think it is important" (00:06:39)
- Deep ties to country suggesting return commitment
- Plans to contribute to South African community/industry

### Social Context + High Caliber (Combined):
- Overcoming disadvantaged backgrounds without mediocrity
- Evidence of resilience and capability despite constraints
- Clear mentoring/teaching others in community

### Memorability:
- Applications that "stick in your mind" (00:08:19)
- Evaluator recalls them 3-4 years later
- Standing out as exceptional within cohort

---

## 5. RED FLAGS AND WARNING SIGNS

### Primary Red Flag:
**Immigration without return plans**:
- "are you funding them to immigrate or you funding them to come back and in some way you know enrich enrich the the South Africa" (00:06:39)
- Lack of South African ties treated as disqualifying concern

### Secondary Red Flags:
**"Last resort" application status**:
- Quote: "somebody heard about OMT" as motivation (00:13:38)
- Applying for sabbatical when other options unclear
- Weak fit with OMT mission

**Internal inconsistencies**:
- Timeline gaps without explanation
- Postgraduate program claims that don't check out on institution websites
- Career narrative that doesn't cohere

**Superficial vetting failures**:
- "I sense that these applications have been vetted and scrubbed quite carefully before they came to me" suggests poorly vetted applications stand out negatively
- Major inconsistencies (though he reports finding none) (00:05:17)

**Unexamined privilege/lack of context awareness**:
- Applicants from privileged backgrounds without acknowledgment of advantage
- No reflection on social position or contribution to community

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

### Academic Background (Primary Evaluator Expertise):
- **Pieter Pistorius**: Metallurgical Engineering department, University of Pretoria
- **Strength area**: "I know what the distinction is... what it takes to complete a PhD in two and a half years" (00:12:04)

### How This Shapes Evaluation:
- Comfortable evaluating academic career progression
- Can recognize quality of PhD programs and completion speed
- Can assess research trajectory coherence
- **Example application**: Medical engineering student pursuing honors
- **Cross-disciplinary judgment**: Uses metallurgical engineering knowledge as baseline for academic rigor assessment

### Acknowledged Limits:
- Less confident in non-academic trajectories
- Cannot assess industrial career progression with same confidence
- "Least confident" in motivation assessment, which spans all disciplines

---

## 7. HOW

---

## Ryan Nefdt

# OMT Application Evaluation Criteria Analysis
## Ryan Nefdt Interview

---

## 1. EXPLICIT CRITERIA MENTIONED

### Primary Evaluation Framework
- **Cohesive narrative connecting life to academics**: "I'm looking for if you can tell me a story that connects the different parts so don't tell me something about your life that is irrelevant to your studies" (00:05:02)

- **Societal impact articulation** (especially humanities): "in humanities it's a little bit different. you have to figure out different conduits to um to getting some societal impact" (00:06:31)

- **Thematic relevance and integration**: "I see the question as much more connected like tell me who you are as an academic and and in so far as your life has informed that development tell me about your life" (00:05:02)

- **NRF Rating scores**: Comparative evaluation using NRF ratings (P, C, B ratings) as numerical inputs (00:12:17)

- **Rubric compliance**: Systematic evaluation against OMT's rubric scoring system (00:03:54, 00:07:41)

---

## 2. IMPLICIT CRITERIA (Actual Practice vs. Rubric)

### Holistic Preparation and Intentionality
- **Application demonstrates holistic thinking**: "what tips me very often is if it looks like somebody has actually not just filled this in as an online form but thought about the application itself holistically and approached it like that" (00:15:38)

- **Level of preparation**: "just the level of preparation the application has undergone" (00:15:38)

- **Distinction between box-filling vs. thoughtful submission**: "sometimes we write especially online applications we we fill the boxes in like I'm processing the information and then we we don't like so when I'm doing a proposal or something like that I like to take it out and put it in a different document and then sort of read it as a whole" (00:15:38)

### Intellectual Sophistication (Beyond Rubric Measurement)
- **Merit of theoretical complexity**: "if you are engaging in a theoretical project that is somewhat lacking in terms of your ability to express the societal impact but that theoretical project is so intricate and interesting...I I kind of want to give it more merit and that often isn't represented in the rubric" (00:08:52)

- **"Puzzle master" work**: "if you're a puzzle master and you're working on an interesting puzzle, sometimes somebody else is going to realize the value of that puzzle, but it's important that you are able to to decode it and try to approach it" (00:08:52)

### Absence of Negative Signals (Rather than Positive Affirmation)
- **Recommendation letters as screening tool only**: Used to flag concerning information, not to identify strengths: "I tend not to put much stock in in the rec unless there's something that's been flagged that is concerning which is rare" (00:16:48)

---

## 3. HOW THEY WEIGH DIFFERENT FACTORS

### Hierarchy of Information Sources
1. **Rubric scoring system** (primary decision tool): "I see the rubric as a measurement that allows comparison...I would on the side of using the rubric...I actually use it as a tool to like you know uh a comparative tool and and I give it a little bit um more priority than my own initial gut instinct feelings very often" (00:10:03)

2. **Gut instinct/personal assessment** (secondary, checked by rubric): "I think somebody's fantastic, then this has happened to me before, but the rubric doesn't really differentiate them from somebody else. I I would on the side of using the rubric" (00:10:03)

3. **Recommendation letters** (minimal weight): Essentially ignored unless containing red flags (00:16:48)

4. **Comparative assessment within cycle** (tie-breaker): Used when rubric scores are similar (00:29:00)

### Disciplinary Weighting
- **Humanities:** Societal impact articulation weighted heavily (00:06:31)
- **Sciences:** Publishing in high-impact journals = impact (less interpretation needed) (00:06:31)
- **Theoretical work:** Must still address "why it matters" but won't be penalized if story isn't perfect fit: "I don't judge them if that story isn't a perfect fit, but I I do need to see that they've they've considered this" (00:07:41)

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Direct Integration of Life and Academic Work
**Example given by reviewer**: "whenever I applied to OM when I have in the past it's been that I grew up in a multi-uh cultural and multilinguistic environment and that shift between codes always fascinated me. So I went into linguistics and I tried to study that phenomenon. So that's a life story that directly speaks to the the um the reason for a particular academic thing um academic facet or something like that" (00:06:31)

### Demonstrated Intentionality in Application Preparation
- Evidence that applicant drafted application separately from online form structure
- Application shows integrated thinking across sections rather than disconnected responses
- "somebody has actually not just filled this in as an online form but thought about the application itself holistically and approached it like that" (00:15:38)

### Sophisticated Engagement with Difficult Concepts
- Ability to articulate complex theoretical work in accessible terms
- Demonstrating how intricate "puzzle master" work connects to broader significance: "it's sort of like, maybe this is a personal thing, but I think if you if you're a puzzle master and you're working on an interesting puzzle, sometimes somebody else is going to realize the value of that puzzle, but it's important that you are able to to decode it" (00:08:52)

### Evidence of Sources and Discussion
- Application shows evidence of having engaged with relevant literature/discussion: "give some background, some indication that you've sourced um discussion" (00:16:48)

---

## 5. RED FLAGS AND WARNING SIGNS

### Inconsistencies and Logical Gaps
- **Methodological mismatches**: "when the same point as previously when it's not your direct field. Um maybe somebody's identifying a problem that the tools that they're claiming to use to solve that problem are not the appropriate tools" (00:27:42)

- **Incoherent narratives**: Applications that mention life experiences disconnected from academic trajectory: "don't tell me something about your life that is irrelevant to your studies" (00:05:02)

### Negative Signals in Recommendation Letters
- **Absence of substantive praise** (mild concern): "the students always on time and they're always friendly and whatever and you're supposed to read from that. They're not very smart" (00:19:11)

- **Vague or formulaic language**: Though reviewer tries not to over-interpret: "I try not to put too much stock in like overly lordary uh letters, but at the same time I try not to read too much in very formulaic ones that uh that might just be a reflection of the personality of the recommener" (00:21:20)

- **Recommender lacks credibility or standing**: "where the the person recommending isn't really that much above in in terms of academic level...I feel like now I have to really read into this like is this a friend is this somebody who doesn't have the ability to judge from that perspective" (00:21:20)

### Concerning Substantive Flags
- Something "odd or strange" in recommendation letters: "now if I'm being honest I just look at them to make sure that they kind of say the usual things and aren't coming out with something odd or strange" (00:22:36)

### Application Approach Red Flags
- **Surface-level engagement**: Simply filling boxes rather than thinking holistically about the application narrative
- **Lack of self-awareness about impact**: Highly theoretical work that shows no consideration of societal relevance whatsoever

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

### Humanities (Fine Art, Sociology, Linguistics, Philosophy)
- **Must articulate societal impact**: "With humanities it's a little bit different. you have to figure out different conduits to um to getting some societal impact" (00:06:31)

- **Cannot rely on publication metrics**: "in the science you often like I'm going to publish in high impact journals and that kind of does the job for you...With humanities it's a little bit different" (00:06:31)

- **Consideration of impact is required even if execution is imperfect**: "unfortunately for the very highly theoretical sort of pursuits they have to be thinking about why it matters. And I don't judge them if that story isn't a perfect fit, but I I do need to see that they've they've considered this" (00:07:41)

### Sciences

---

## Frasia Oosthuizen

# OMT Discovery Interview Analysis: Frasia Oosthuizen

## 1. EXPLICIT CRITERIA MENTIONED

### Application Components Evaluated:
- **Motivation letter**: "The fact that they write the motivation first me always gives them a sense of who this candidate is."
- **Academic history/prior performance**: Marks and academic standing
- **Research proposal**: "the full proposal and then the references at the end"
- **Referee reports**: "the referee reports which is very important"
- **Funding status**: "if the candidate has sourced funding"
- **Budget**: "I can't say do away with the budget because that also adds"

### Categorization by Application Type:
- Masters applications
- PhD applications
- Postdoc applications
- Sabbatical applications

### Disciplinary Scope:
- Health sciences broadly (not limited to pharmacy)
- "I specifically look at health sciences. So it's it's not pharmacy. It can be across the board. Anything in health sciences."

---

## 2. IMPLICIT CRITERIA (What They Actually Look For vs. Rubric Says)

### Alignment/Coherence Assessment:
**Most Important Finding - "Alignment":**
- "if everything if the big picture makes sense from the motivation through to the referee reports, I feel more confident that I have an understanding of who this candidate is"
- Looks for consistency between what candidate claims in motivation and what the proposal actually shows: "someone can spin a lot of very nice things in their motivation. But when you read the proposal, it's not aligned at all to what they want to achieve as an individual"
- Example of misalignment: "the candidate wants to, you know, cure cancer, but the project is so basic, there's a total disalignment"

### Judgment Beyond Rubric:
- Personal coherence of candidate's narrative across documents
- Whether candidate truly wants the degree (commitment assessment)
- Consistency of academic ability claims
- Visual presentation aids memory and judgment: "I'm just a visual learner. So for me that helps because I can recall who the candidates were better because of the photos. It's a stupid thing but it helps me."

### Referee Report Interpretation:
- Explicitly stated rubric requirement: referees must say candidate is "exceptional" to score a 4
- **Actual practice**: "I have um ignored it but as I said ignored it consistently in a group that if I felt the referees mentioned this or referred to that that is similar to a four"
- Acknowledges rubric language is too rigid: "not something we don't referee them as exceptional generally"

---

## 3. HOW DIFFERENT FACTORS ARE WEIGHTED

### Primary Weight - Alignment/Coherence:
Frasia's most emphasized factor. Disalignment leads to lower scores regardless of individual components.

### Sequence of Consideration:
1. **First**: Motivation letter (establishes candidate identity)
2. **Second**: Proposal (check alignment with motivation)
3. **Third**: Prior academic performance (context)
4. **Fourth**: Referee reports (validation, but cautiously)

### Deliberate Caution with Referee Reports:
- "I am cautious not to let it sway me too much"
- Reason: "where I found the motivation the prior performance in terms of marks as well as the proposal not overwhelmingly positive and then getting overwhelmingly positive um referee reports"
- Protective mechanism: Reads motivation and proposal first to form independent judgment before reading referees

### Visual Information:
- Photo inclusion aids recall and judgment quality (implicit weighting)

### Funding Status:
- Important for understanding candidate commitment ("how much they want this degree")
- But interpretation weight is minimal: "anyone can take they passed with distinction...That's just a tick. You don't need to interpret anything"

---

## 4. WHAT MAKES AN APPLICATION STAND OUT POSITIVELY

### Coherent Narrative:
- Everything aligns: motivation → proposal → academic record → referee reports all tell same story
- "If everything if the big picture makes sense from the motivation through to the referee reports, I feel more confident that I have an understanding of who this candidate is"

### Authentic Ambition:
- Candidate's stated goals match project scope and sophistication level
- Shows genuine commitment: "I also want to know how much they want this degree"

### Clear Academic Foundation:
- Strong prior academic performance (distinctions/merit)
- Demonstrates ability to execute proposed work

### Positive External Validation:
- Referee reports that substantiate internal document narrative (when aligned)
- Note: Not blindly trusted, must corroborate other evidence

### Young Achievers:
- Frasia's language reveals positive bias toward exceptional young talent: "it's amazing to see what some young people are achieving in their lives"
- "I say no to a lot of things but I enjoy doing this"

---

## 5. RED FLAGS AND WARNING SIGNS

### Primary Red Flag - Misalignment/Incoherence:
**"those inconsistencies red flags as I'm and that's just me personally when I read this I try and create a picture of this applicant"**

Examples given:
- Motivation claims one thing; proposal shows something very different
- Stated ambitions (e.g., "cure cancer") don't match project reality (basic research with narrow scope)
- Academic record doesn't support claimed capabilities

### Secondary Red Flags - Referee Inconsistency:
- Overwhelmingly positive referee reports paired with weak motivation/proposal/academic record
- Suggests referee may not be carefully evaluating or may be biased

### Missing or Incomplete Information:
- Timing issue: "candidates might just have completed an exam session or whatever. I have come across that um a few times"
- Missing marks at deadline (affects scoring)

### Lack of Genuine Commitment:
- Insufficient evidence of funding pursuit or personal investment
- "I also want to know how much they want this degree"

---

## 6. DISCIPLINE-SPECIFIC CRITERIA

### For Health Sciences Broadly (Not Just Pharmacy):
- Applicability of research to health outcomes
- Methodological rigor appropriate to health disciplines
- Budget relevance to health sciences research

### Interdisciplinary Challenge:
- Reviewer (pharmacist) evaluates across health sciences disciplines
- Acknowledges some gaps require additional understanding: "I know definitely. Yeah" (when asked about submissions outside her expertise)

### No Explicit Sub-discipline Weighting Mentioned:
- Treats Masters/PhD/Postdoc/Sabbatical as separate evaluation groups, but no health science sub-specialties mentioned

---

## 7. HOW THEY HANDLE APPLICATIONS OUTSIDE THEIR EXPERTISE

### Strategy: Contextual Research:
- "I know definitely. Yeah" (encounters submissions outside her expertise)
- Implies she looks things up or researches: "would it be helpful if you sometimes received some of that context that might not have been present"

### Proposed Solution - Layman's Abstract:
- **Exact quote**: "maybe, you know, like a layman's abstract that can accompany it...It's not dumbed down. It's just written in a language that is more um easy to understand for everyone that's not necessarily a biologist or a bioineticist or something."
- Purpose: Aids understanding of specialized proposals
- Secondary benefit: Assesses writing skills across levels

### Confidence in Broad Health Sciences Review:
- "I have not find that that my background in pharmacy limits me. I think as you grow in academia, you are requested and asked to do a lot of things that's not necessarily pharmarmacology or or pharmacy, but they they take it that you have the experience."
- Trusts her general academic judgment applies across health sciences

---

## 8. VERBATIM QUOTES - ORGANIZED BY THEME

### ON OVERALL APPROACH & CONSISTENCY:

- "I try and do the masters as a group and at least to do it in a day. So I have a sort of a recall as I go through the process and I do sometimes flip back."

- "if I do that group I think okay I gave this one a four but really then that one shouldn't have been a four. It should have been a three."

- "I do it group by group because I try and be objective but I don't think in something like this which is not based on quantitative values you can always just there is a matter of subjectivity."

- "I try to do at least my group that in that group they are the same for me. So if I said someone should definitely get it and someone should ne not get it, I at least know in my candidates why I chose that one and not that one."

- "I don't think you you can expect that but as I said I don't believe that is a flaw in the system."

### ON THE RUBRIC:

- "I do find the rubric as a very useful guide. I do believe that the way the rubric is structured that every evaluator might interpret it in their own way."

- "I find it very easy

---

