# Rubric and Scoring Deep Dive

## Dina  Ligaga

# RUBRIC, SCORING SYSTEM, AND EVALUATION MECHANICS ANALYSIS

## 1. HOW THEY USE THE RUBRIC

**Primary Use:**
- Tick-box scoring system with narrative justification section
- Scores range from 1-4 across multiple dimensions
- Different rubrics for PhD vs. Masters vs. Postdoc applications
- Used AFTER initial reading and note-taking, not before

**Application Process:**
- Dina reads application first, takes extensive notes
- Then applies rubric scores to tick-boxes
- Then writes narrative explanation
- Final scores inform recommendation decision (absolutely recommend vs. hesitant)

**Quote:**
> "Um and so there's a there's a there's something that happens to that process for me where you know I have my notes and I know what I want to say and then there's the rubric and it feels almost like oops oh okay now I have to go back and tick the boxes" (00:17:44)

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**"Fuzziness" Between Score 3 and 4:**
- Unclear distinction between the two middle scores
- Main source of scoring uncertainty
- Creates ambiguity about what constitutes "excellence"

**Quotes:**
> "so I find sometimes that the rubric can be a bit so I know one and I know four, but in between can be a bit fuzzy" (00:11:56)

> "Um so I find sometimes that the rubric can be a bit so I know one and I know four, but in between can be a bit fuzzy. So I I know I know when I give somebody um I don't I probably have given just maybe one personal one, but that was quite early on. I'm I'm usually more generous than that. Um but I tend to to to use the number three quite a lot." (00:13:16)

> "So three and four sometimes tend to be a little bit fuzzy." (00:14:41)

**Lack of Gradations/Range:**
- Only 4 scoring levels insufficient for nuanced distinction
- Particularly problematic at higher end (difference between 75% excellence and 90% excellence)
- Statistical ranges too wide (4 could be anything 75-100%)

**Quotes:**
> "But it's actually the top 25% um range, you know, it's sort of from 75% through to 100. It's not just 100%. And so you maybe don't have enough gradations in your scoring thing" (00:18:40)

> "Um and so sometimes excellence can be for me a 75% excellent and sometimes it can be has never quite been. But I mean you know just for sake of example um sometimes excellence can be for me a 90% and for me those are very two different types of excellence then I can't always get it." (00:23:04)

**Rubric Feels "Blunt" and "Boxing":**
- Perception that rubric oversimplifies complex judgments
- Feels constraining rather than clarifying

**Quotes:**
> "but it's actually the top 25% um range, you know, it's sort of from 75% through to 100. It's not just 100%. And so you maybe don't have enough gradations in your scoring thing that undermines your confidence in at the rubric phase...you know what But four, four is obviously excellent, but it's actually the top 25% um range" (00:18:40)

> "the rubric for me then then takes it kind of boxes me in and then I have to now go back and start thinking okay out of these four things what do I think is most appropriate and then it almost slows me down" (00:21:00)

**Inconsistent Difficulty Across Application Types:**
- Postdoc rubric "most difficult" - "really like sometimes it's just one and two"
- Easier to score on some dimensions (personal motivation) than others (academic performance)

**Quote:**
> "Uh I see for instance that uh the the one for the PhDs is slightly different from the one for the MAS and you know postocs as well. You know they're all different and the postto is the most difficult actually cuz it's really like sometimes it's just one and two." (00:24:17)

> "Um so I don't know I just find sometimes that three and four can be a little bit yeah uh fuzzy. Um, and so it always ends up being a judgment call. And I must say once or twice I found myself having to go back and either mark up or mark down just so that the final the final tally kind of looks like what it is that I think the person deserves." (00:13:16)

---

## 3. WORKAROUNDS THEY'VE DEVELOPED

**Retroactive Score Adjustment:**
- Reads application fully and forms holistic judgment
- Applies initial rubric scores
- Then "marks up or down" to align with overall conviction

**Quotes:**
> "And I must say once or twice I found myself having to go back and either mark up or mark down just so that the final the final tally kind of looks like what it is that I think the person deserves. So, and also just to make it easier for me to then say at the end that you know this person either absolutely deserves this award or could be considered you know the really at the right at the end where you you make that kind of call." (00:13:16)

> "So then I have to then go back and say okay no mm it's not a 90% excellent it's a 70% or 75%. So then I have to go back and and see what it is that I may have awarded more than I should have." (00:23:04)

**Overusing Middle Score (3):**
- Uses score of 3 frequently as default/safe option
- Avoids committing to 4 unless certain

**Quote:**
> "Um but I tend to to to use the number three quite a lot. Um partly because um I I take that to give somebody all the all the points the four you know to to to take on four also means that the person is excellent um or the application is excellent" (00:13:16)

**Extended Narrative Justifications:**
- Writes detailed notes during reading
- Adds explanatory text at end to justify scoring decisions
- Intends to help other decision-makers understand reasoning

**Quotes:**
> "Um and I don't know if that's fair or not. Um but um in the end, and that's part of why I have the little section where I try to explain my thinking um about why the person has gotten what they did." (00:14:41)

> "Um when I feel strongly that somebody does um needs to be recommended I will write it there and I will I will spend some time to just explain why just so the other person can also you know it makes somebody else's work easier." (00:27:56)

> "I like writing so I do a lot of notes but after a while I'm just like you know I have to do all of this in I don't know it it takes a bit of time" (00:16:06)

**Relying on End-of-Process Clarity:**
- Makes final recommendation decision at very end after all scoring
- Uses that final conviction to validate earlier scores

**Quote:**
> "By the time I get to the end, I have a more or less good sense of what's going on." (00:21:00)

> "I know immediately who is just a good person to give an award to. Um but then going through the rubric it doesn't always come it doesn't always u tally and so I have to then go back" (00:23:04)

---

## 4. SCORING STRATEGIES

**Starting with Holistic Intuition:**
- Begins with reading and forming overall impression
- Scores AFTER forming conviction, not before
- Uses rubric as validation/translation tool rather than discovery tool

**Quote:**
> "And so that suggests, and you can correct me if I'm wrong, that as you're looking at all of the facts internally, you're kind of synthesizing it in a way that the rubric doesn't quite capture. And then sometimes you're adjusting your reaction to the rubric to align with your more um intuitive sense." [Dina: "Yes. Thank you. Yes."] (00:14:41)

**Conservative Scoring (Retrofitting Strategy):**
- Starts with lower scores and adjusts UP to match conviction
- Not starting high and adjusting down

**Quote:**
> "Um but I tend to to to use the number three quite a lot. Um partly because um I I take that to give somebody all the all the points the four you know to to to take on four also means that the person is excellent" (00:13:16)

**Synthesizing Multiple Dimensions:**
- Forms intuitive sense combining: motivation, proposed study, past performance, continuity of narrative
- Then maps this synthesis onto

---

## Edzai Conilias Zvobwo

# Rubric Analysis - OMT Discovery Interview with Edzai Conilias Zvobwo

## 1. HOW THEY USE THE RUBRIC

**Primary Use:**
- Applied to science applications across the entire science spectrum (natural science, etc.)
- Used as an elimination/filtering tool for minimum requirements
- Barbara Dale-Jones characterizes it as "helping you at a certain level of judgment" and functioning as "a kind of elimination process or at least a categorizing process"

**Integration with Judgment:**
- Rubric serves as a guardrail but not the sole decision-making tool
- Reviewer acknowledges looking at academic transcripts (high school, undergrad performance) before applying rubric criteria
- Rubric application happens after initial assessment of candidate credentials

**Verbatim quotes:**
- "because the rubric tells you that they have to have this this if they don't qualify Then I discard literally. Yeah...They have to meet the minimum requirements. That's that's important thing."
- "So the rubric is helping you at a certain level of judgment. It's although on the one hand you're saying AI could take away the grunt work of that you're nevertheless going through a kind of a elimination process or at least a categorizing process"

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**Cold Start Problem (No Frame of Reference):**
- First application is "always the most difficult one because there's no frame of reference. There's no standard set. So literally you are flying in blind."
- "That could actually prejudice the person who who's judged first"
- Only after "one or two then at least there's a pattern a rhythm that you can see"

**Scoring Scale Bin Size (0-4 Scale) - PRIMARY COMPLAINT:**
- "the bin the bin is too big. So, so you see at three, quality three and a low quality three, they're all bunched up in there. So, yes, yes, the bin system is too is too uh too few beans here."
- Barbara Dale-Jones confirms: "Is it too blunt?" and Edzai agrees: "Yes"
- Creates inability to differentiate between high-quality (90-100%) and merely acceptable (75%) applicants both scored as "4"

**Fuzzy Scoring Over Time:**
- "the numerical number right you'll find after some time because this is a mind-numbing process I mean going through those sometime now you is this a four is this a three you know it becomes fuzzy along the way"
- Scores "become just numbers afterwards after some time"

**Final Assessment/Summation Difficulty:**
- "that final assessment where you have to write to say okay I don't know somehow it's so difficult to I struggle with that yeah the final assessment where you're writing and just finding the the sum summation of everything"
- "it's quite easy but that final assessment where you have to write to say okay I don't know somehow it's so difficult"
- Attributes this to "a human memory thing" - difficulty synthesizing all the step-by-step evaluation into coherent summary

**No Institutional Memory Between Years:**
- "if you ask me who I adjudicated last year I don't remember so the the the the contiguity of of of intera is is is almost zero for me"
- "I don't remember what I did last year. Yeah...So I don't know if it's a if it's a it's a deliberate uh design element or maybe it's just a case of uh there's no knowledge base"

**Verbatim quotes on complaints:**
- "It's mind-numbing"
- "goes back to the to the guy who's doing uh nanotechnology and the one who's doing water for Joelbe uh water for Joelbe affects me directly"
- "the scope is quite is quite wide"

---

## 3. WORKAROUNDS THEY'VE DEVELOPED

**Revision Strategy for Cold Start Problem:**
- "you start off maybe you do three, two, three, and then you realize that okay, maybe the first three I started with were were low quality and I gave them I put them on five and then I get really good ones, then I have to go back and revise downwards"
- Uses the system's flexibility to adjust scores after establishing frame of reference
- Goes back to "re-re the other" applications with new calibration

**Hand-Holding/Implicit Understanding:**
- "sometimes they can't actually articulate. So, so I've had a situation where I literally held their hand to say, 'Okay, I get what you're saying and I'll give you four because I I I get what you're saying, but you're not saying it, but I get it.'"
- Gives higher scores (4) even if articulation is poor if reviewer implicitly understands the concept

**Self-Regulation Against Known Biases:**
- "I'm aware of them and I try and self-regulate along the way"
- Acknowledges personal bias toward impact programs for "bottom of the pyramid" but attempts to correct
- Also tries to regulate against negative first impressions from transcript grades: "so maybe they were going through something but for this particular masters they actually doing a an impactful thing so that is bias that uh I try and regulate myself against"

**Bayesian Approach to Scoring:**
- Explicitly uses Bayesian logic: "So it's Beijian. Uh you change your beliefs as you as you gather more evidence basically"
- Updates scores as pattern emerges from multiple applications

**Thematic Grouping Strategy (Proposed Workaround):**
- Suggests applications "annually if there's a theme where it's almost closely neat uh around around a subject where it's thematic for me that would help in terms of objectivity"
- "But now if you had 10 submissions that are dealing with water and finding the best solution around water then it's easier to judge to compare because uh it's basically the same thing"

**Verbatim workaround quotes:**
- "I have to go back and revise downwards"
- "the system is flexible enough to be able to to correct for that based it's almost uh I don't know if you know Beijian statistics. Yeah. So it's Beijian. Uh you change your beliefs as you as you gather more evidence basically"

---

## 4. SCORING STRATEGIES (STARTING HIGH, RETROFITTING, ETC.)

**Retrofitting/Calibration Correction:**
- Starts with initial scores then revises downward: "you start off maybe you do three, two, three, and then you realize that okay, maybe the first three I started with were were low quality and I gave them I put them on five and then I get really good ones, then I have to go back and revise downwards"
- This is explicitly acknowledged as necessary due to lack of initial benchmarking

**Implicit Understanding Override:**
- Gives higher score when applicant's underlying concept is sound despite poor articulation: "I'll give you four because I I I get what you're saying, but you're not saying it, but I get it"

**Story-Weighted Scoring:**
- "the story is quite is quite um uh as a big awaiting uh because I think it's almost like a social justice element"
- When between scores or on fence: "if if if it's a case of there's there's nothing amazing or or bad, it's just neutral kind of thing. Yeah. Then the story comes into the frame"
- Confirmed: "for me the story matters...So that's yeah, that's the one with the biggest waiting here"

**Impact-Based Tie-Breaking:**
- When objectively similar: "someone is um is is doing um uh maybe in maths is is doing a way to to multiply matrices better, right? in as much as it has quite uh profound secondary rotiary uh effects uh but someone who's creating a new way to clean jobic water you know so I would go for the job water because I can see the immediate"
- This reveals hierarchical scoring: measurable immediate impact > abstract potential impact

**Comparative Scoring Within Category:**
- "if you had 10 submissions that are dealing with water and finding the best solution around water then it's easier to judge to compare because uh it's basically the same thing"
- Suggests scores are more standardized when comparing like-to-like

**Verbatim scoring strategy quotes:**
- "I literally held their hand to say, 'Okay, I get what you're saying and I'll give you four because I I I get what you're saying, but you're not saying it, but I get it'"
- "if if if it's a case of there's there's nothing amazing or or bad, it's just neutral kind of thing. Yeah. Then the story comes into the frame to say, 'Okay, what is this person's story?'"
- "I would go for the job water because I can see the immediate"

---

## 5. HOW THEY HANDLE EDGE

---

## Freedom Gumedze

# Analysis of Rubric, Scoring, and Evaluation Mechanics

## 1. HOW THEY USE THE RUBRIC

**Process:**
- Reads the portfolio first, then refers back to the rubric criteria
- Uses the rubric as a comparative framework against known standards (1-star to 4-star ratings)
- Applies domain expertise to assess innovation before consulting rubric

**Verbatim quotes:**
- "I'll first read the the portfolio as I would read in a portfolio and then I go back to what the OMT looks like."
- "I do use the rubric. I always I mean I'm I'm I've been saying about comparing with what I do in in other spaces, but I think I I do go back to look at what fourstar means in terms of what OMT puts down."
- "Am I meeting most of the things that are listed in that fourstar? Am I really convinced that I'm meeting what is in that fourstar?"

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

### Publication Assessment
- **Problem:** No guidance on which sources to use for evaluating publications
- **Verbatim:** "publications by the way that's loose because you're not told how you should do this publication. Should you be looking at Google Scholar? should be scopas."
- **Impact:** "you really have to apply your mind as to are this good publications are this because Google square will just give you everything that's coming through but it will not tell you whether this is published in a very good journal."

### Sustainability Goals Reporting
- **Problem:** Candidates not asked to report in standardized way
- **Verbatim:** "you want to see when where they they are contributing that's not asked of the of the candidates they can just put a follow on any way that they want"

### Over-reliance on NRF Rating
- **Problem:** Rubric leans heavily on NRF metrics that are outdated and unreliable
- **Verbatim:** "the standing of the of this particular of the particular individual coming back to NRF again the there's a somewhere in the rubric kind of sabatical that are NRF rated"
- **Problem specificity:** "if you can look at their work now maybe they are not they not at that at at at that rating" (ratings can be 5 years old)

### Masters Program Structure Misalignment
- **Problem:** Rubric treats all masters equally, but discipline-specific differences exist
- **Verbatim:** "The rubric is built on a master's by dissertation. So the masters were seen...if you're think about statistics data science those disciplines the masters really is course work and the dissertation is is just a small part"
- **Recommendation:** "So that so I would like that to be separated. I'm not sure how I would evaluate it, but I would like you to be separated."

### General Framework Complaint
- **Verbatim:** "it's built on a NRF kind of the only thing that has been added is that you have got this social responsive uh social sort of like impact...if you read the rubric in most instances it's it's kind of leans towards NRF sort of like way of assessing proposals."

---

## 3. WORKAROUNDS THEY'VE DEVELOPED

### Benchmarking Across Multiple Years
- **Verbatim:** "if I've sat on the committee for five years I have the full five year record of of of how candidates have been graded. So something similar I would apply something similar here just to sort of try and benchmark"
- **Note:** Admits this creates bias: "So I that does happen. I don't think it's completely independent"

### Using External Domain Knowledge
- **Verbatim:** "Obviously other things like you're looking at references, you're looking at whether there's a budget or not and things like that, but one kind of starts off looking at the core sort of content"
- Uses journal editor experience as proxy for innovation assessment: "I also sit on several sort of uh I would say high impact statistical journals...I have a sense of if something is innovative or not."

### Manual Research Digging
- **Verbatim:** "I would like to say the research because there's information that needs to be dug out there that I have to dig...you're not going to put in metrics in there...if that was requested was not requested from the from the applicants then I would like someone assistant maybe to dig up that information"

---

## 4. SCORING STRATEGIES

### Comparative Benchmarking (Not Starting High/Low)
- Does NOT use "starting high/low" approach
- Uses historical comparison: "if I've done this three or four years then I'm looking across those four years"
- **Note on current-year pooling:** "I don't think I do that with the OMT. I don't look at the current pool that in this pool this is the this is the best even more or less a recommendation that we must do that"

### One-Star vs. Four-Star Clarity
- **Verbatim:** "One star I think that's kind of I shouldn't say it's no brainer, but again it's it's something is one star and something is four star to me."
- Suggests one-star and four-star are intuitive, middle ratings are ambiguous

### No Retrofitting Mentioned
- No evidence of retrofitting scores after initial assessment

---

## 5. HOW THEY HANDLE EDGE CASES IN SCORING

### Missing Information in Portfolios
- **Verbatim:** "there were applicants who even in the scoring obviously there will be one star candidate but there's some information that's probably missing or not properly articulated in the portfolio"
- **Examples:** "something that's not properly articulated in the portfolio is educational background maybe that will be in the CV but what school are they going to if they are interested in social things are those um uh social activities aligned"

### Outside Area of Expertise
- **Response:** "Fortunately in the past ones no...I've felt confident what a ga sign is something that I can confidently assess"
- Claims this hasn't been an issue but suggests others might face it

### Incomplete Budget Information
- **Verbatim:** "budget is the last thing actually that I that I that I look at even though it's part of the it's part of the as part of the portfolio"
- Suggests budget is secondary when other info is missing

---

## 6. SUGGESTIONS FOR RUBRIC IMPROVEMENT

### Separate Master's by Dissertation from Master's by Coursework
- **Verbatim:** "I would like that to be separated...you can't be evaluating the masters by dissertation the same way as you would evaluate the masters by coursework."

### Provide Publication Assessment Guidance
- Implied recommendation: Specify whether to use Google Scholar, Scopus, or other metrics

### Standardize Sustainability Goals Reporting
- **Verbatim:** "sustainability goals goals you you want to see when where they they are contributing that's not asked of the of the candidates they can just put a follow on any way that they want so I find a research then especially when it comes to maybe postto and the and the sabatical that you you really have to apply yourself"

### Implement Reviewer Calibration/Consensus Process
- **Verbatim:** "I would like to know of the of the reviews that we have given do they take a sort of like combination of scores from the group because I'm assuming that I'm I look at the statistics uh sort of uh uh projects or rather proposals but maybe there's another statistician that looks at at that but I want to know at the end what they they desire so that maybe we can calibrate that from this side"
- **Model from NRF:** "we actually share scores and then there kind of like moderators where there's kind of a a decision right at the end where I don't want to say one gets forced to to to agree but some moderation where you kind of have to come to a consensus"

### Provide Feedback Loop on Outcomes
- **Verbatim:** "I would like to know...do they take a sort of like combination of scores from the group...maybe we can calibrate that from this side and because maybe they have to give feedback that there's a set of proposals that really didn't have to go through this stage."
- **Additional ask:** "it might also be helpful to know the outcomes which which people got funded and and so on."

### Consider Scientific Committee Review Model
- **Verbatim:** "There's something called a scientific committee that actually sits and deliberates on the scoring...if you're thinking about funding proposals of millions then there's a scientific committee that sits around and liberates on that scoring"

### Move Away from NRF Rating Dependency
- **Implicit:** Suggests rubric should weight peer reviews and current work quality rather than NRF ratings: "if someone doesn't have an rating, then I'm going to rely on the referee's reports. I don't necessarily have to rely on because it could be that someone has got an NF rating but

---

## Frelet De Villiers

# Rubric & Scoring Analysis: StrideShift Interview

## 1. HOW THEY USE THE RUBRIC

Frelet uses the rubric as a **framework for consistency** rather than a strict constraint:

- Initial scan of all applicants to understand the cohort, then detailed review of individual candidates one-by-one
- Scores assigned during first detailed read-through, but with **iterative refinement** after reviewing the full batch
- Compares candidates within the current batch to establish relative rankings
- Does NOT benchmark against previous years' cohorts

**Key quote:** "the rubric is the only way to to have the same level of what do you see as good in comparison to what I see as good. So no that is really it has to be there."

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**Primary complaint: Restrictiveness and misalignment with actual judgment**

- "Sometimes the rubrics are very um restrictive and not really um incompulsing. You know, what you are seeing in front of you. So, some of the the things that you're are judging does not really fit into the rubric."

**Lack of clarity on scoring thresholds:**
- No explicit percentages provided for score levels (0-4 scale)
- Interviewer notes: "I'm not sure they are [connected to numbers]"
- Reviewer had to develop own internal benchmarks

**Lack of specificity/detail:**
- Interviewer acknowledges: "I'm so sorry. I really can't remember what the rubric has"
- Suggests rubric design may not be memorable or salient

---

## 3. WORKAROUNDS THEY'VE DEVELOPED

**Starting High Strategy:**
- "I always start on a four. So, if I read through something, the candidate has a four. As I go further, I will I will then go, no, this is actually a three. Okay, this is actually a two."

**Within-Batch Calibration:**
- Reads everything first for general sense, then conducts detailed reviews
- "when I get to the next one, I also um then go through the questions, but then you kind of compare it with what you have"
- "I will not finally submit my my review first and then I will I will see what I have and and then compare what is a merit with this one in comparison to the previous one"

**Comparative Ranking Method:**
- "I have my three top candidates without looking at the marks and then when I look at the marks and I see okay those three are the top then then I'm very grateful for that"
- Uses intuitive ranking to validate scores: "Sometimes there's like just a margin and some of them are on the same then then I have to decide which one is is actually the better one"

**Ignoring Rubric Where It Doesn't Fit:**
- "if something is not um is if they say something and it's not in the rubric I I also add value to that"

**Speed Management:**
- "I I will not do like them at a date because I really need to be focused and and just be fresh in in my thoughts regarding that"
- Intentionally limits batch size per session

---

## 4. SCORING STRATEGIES

**Starting High, Then Deducting (Anchoring Strategy):**
- Default assumption is a 4/4, then scores downward as weaknesses emerge

**Iterative Revision Post-Review:**
- "Yeah. Yeah, definitely" and "definitely" when asked if scores change after reviewing entire batch
- Implicit loop: score → review all → recalibrate → revise

**No Explicit Percentage Anchors:**
- "I don't I I don't really say a four is like 94.6% and higher"
- Operates on intuitive/holistic judgment rather than formulaic thresholds
- "in my minds I I have a certain score of what is a four, what is a three"

**Non-Comparative (Year-to-Year):**
- "I can't remember what I've done the previous year"
- "I will not compare with previous years"
- Scoring is relative to **current cohort only**

---

## 5. HOW THEY HANDLE EDGE CASES

**Inconsistent Writing Style/Ghostwritten Proposals:**
- "you look at the writing and see perhaps, you know, the proposal was done by someone else"
- Does flag inconsistencies but **takes documents at face value**: "there's no way to to see who wrote this um and especially with AI these days... you can't even know see if it's a it's a student writing"
- Philosophy: "If if they lie to me it's on their black book. So I I can just review what I have in front of me"

**Inflated Budgets/Unrealistic Claims:**
- "I have this preposterous or or what is it like exuberant amounts and then I know a a ticket to um you know uh Europe is definitely not 35,000 rand"
- **Red flag strategy:** "if if they really inflate the prices like that that is a red flag for me. And then when I see that I will be more cautious when I look at the value and the proposal"

**Unfeasible Project Scope:**
- "sometimes these people they say they are going to have four articles and three conferences and two books. I mean it's just not possible"
- Uses judgment to assess realism of proposals

**Generic/Weak Reference Letters:**
- "If you get a a letter which is very um generic and like only one or two paragraphs, I don't get a very nice feel of the student"
- Takes as indicator of lack of supervisor commitment but doesn't have mechanism to score this systematically

**Close Score Margins:**
- "two or three marks can can make a big difference" when candidates have nearly identical scores
- Forces explicit re-comparison to break ties

---

## 6. SUGGESTIONS FOR RUBRIC IMPROVEMENT

**Implicit/Stated Suggestions:**

1. **Greater flexibility for discipline-specific values:**
   - "what is valuable for me in a study is not valuable for you in a study. So it all depends on your own um reference your own framework"
   - Current rubric may be too generic across disciplines

2. **Clearer scoring definitions:**
   - Interviewer probes whether 0-4 scale has clear thresholds; reviewer reveals she created own mental model
   - Suggests rubric should define what 3 vs. 4 means explicitly

3. **Less restrictive framework:**
   - "Sometimes the rubrics are very um restrictive and not really um incompulsing"
   - Allow assessors to capture value that falls outside predefined criteria

4. **Interviewer suggests additional feedback:**
   - Reviewer already provides detailed written feedback
   - Mentions she considered making rubric suggestions before but "can't really remember if it was for them or for the NRF"

**Note:** Reviewer deferred detailed feedback:
- "If you sent me the rubric, I can give you email feedback on that. But unfortunately, I really can't remember the different."

---

## 7. ALL RELEVANT VERBATIM QUOTES

### ON RUBRIC USE & STRUCTURE

- "the rubric is the only way to to have the same level of what do you see as good in comparison to what I see as good. So no that is really it has to be there. It has to be there. Sometimes it's restrictive if I can say that."

- "the rubric is a is a is a nice framework of of your what you have to do but um like I say I don't if something is not um is if they say something and it's not in the rubric I I also add value to that"

### ON RESTRICTIVENESS

- "Sometimes the rubrics are very um restrictive and not really um incompulsing. You know, what you are seeing in front of you. So, some of the the things that you're are judging does not really fit into the rubric."

### ON SCORING STRATEGY - STARTING HIGH

- "I always start on a four. So, if I read through something, the candidate has a four. As I go further, I will I will then go, no, this is actually a three. Okay, this is actually a two."

### ON ITERATIVE REVISION

- "I'm a UNISA examiner. So I'm so used to when you hear something you have to what is your your first idea um without thinking too much. So what is my my first impression is like this is this is the mark I want to give. Um so but when I said I I read through everything it's just scanning. I I'm not reading all this all the applicants everything from them. No I I just get an idea. But yeah, when I I now really go through one applicant, I immediately have my mock and my my ideas and then later on I will make it beautify my sentences and and things like that"

- "afterwards say for example I have 10 applicants in my

---

## Martin Clark

# Rubric & Scoring Analysis: Martin Clark Interview

## 1. HOW THEY USE THE RUBRIC

**Process:**
- Parcels applications by level (Master's, PhD, sabbatical) to calibrate expectations of excellence accordingly
- Uses specific rubric categories to guide evaluation
- Applies a 1-4 point scale across rubric categories, then a final 5-point scale assessment
- Reads 5-10 applications per sitting to avoid "adjudication fatigue"
- Limits focus to specific categories when evaluating edge cases
- Adds comments to explain reasoning when unable to give desired grade

**Key Quote:**
> "Um so um as I'm sure you're aware um you first engage with the OMT and then you you agree to the uh to receive the publications then you get essentially a link to all the publications on the OMT uh website. Um at that point uh generally what I do is I parcel down what uh applications I'm going to be dealing with and I and I separate it by level." (00:01:59)

> "So um the way I would do this generally is um uh once I know how many masters, how many PhDs, how many postocs, uh I see how much time I have and then I parcel my time because the other thing you don't want to do is you don't want adjudication fatigue. Uh every application you read, you need to read with a fresh mind." (00:03:16)

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**Rigidity & Inflexibility:**
- Categories are "rather rigid" and don't accommodate discipline variation
- Boxes should be "more nebulous" or "change based off of discipline or category"
- Limited ability to account for circumstantial excellence

**Golden Key Society Example (Problematic Metric):**
- Being part of Golden Key Society is listed as excellence metric
- Requires paid membership, disadvantaging excellent candidates who cannot afford it
- Applicants may not explicitly state they were invited but couldn't afford membership
- System caveat: doesn't capture actual excellence, only financial access

**Scoring System Bluntness:**
- 1-4 point rubric is too blunt to make necessary distinctions
- 5-point final scale insufficient: "five points isn't always enough to to discriminate one against the other"
- Large gaps exist within point ranges (e.g., difference between 75% and 100% both score as "4")

**Institutional Benchmarking Problem:**
- Cannot adequately compare grades across institutions (e.g., C at UCT vs. B at Nelson Mandela University)
- Grades and project expression don't always align between institutions
- Creates unfair comparison of candidates from different institutional contexts

**Inability to Weight Variables Appropriately:**
- Variables are "not always weighed equally"
- Weighting structure sometimes becomes "a constraint in how well I can adjudicate an application"
- Cannot properly evaluate applications not "clearly excellent across all metrics"

**Key Quotes:**

> "Um the rubric itself uh gets back to that measure of excellence. Um and I think because that was the way that uh the OMT has designed it. Um the questions are very specific. Now um without um criticizing um I find that the categories provided are rather rigid." (00:04:25)

> "So um I do my best to see applicants who argue, well, I've been invited, but I can't afford to pay as evidence of excellence. But an applicant might not always put that in their application if they aren't explicitly in a golden key society. So, you understand this is perhaps a caveat in the system." (00:05:45)

> "So if a project is proposed that from that person's perspective is their world should that project be valued less than someone who has been exposed to a much broader understanding of the world um and then can can pitch a project that is um argued in such a way. So that is the situation where I need to try and look at all elements of how one classifies excellence and um the the rigidity of some of the the categories uh because it all boils down to if I remember correctly a five point score across a few categories. Um five points isn't always enough to to discriminate one against the other." (00:11:57)

> "Um and there I have had applications from certain institutions where the grades and the way that the candidate actually expresses themselves and expresses the project do not align between institutions." (00:22:41)

> "And they're and they're not always weighed equally. Um and that's also the thing is um I there's a clear waiting structure in the adjudication that um that again speaks to what the foundation is looking for. Um and depending on what is what is shown I I might I I might sometimes feel that's a constraint in how well I can adjudicate an application that again is not um clearly clearly excellent across all metrics." (00:10:33)

---

## 3. WORKAROUNDS THEY'VE DEVELOPED

**Comment/Caveat Strategy:**
- Adds comments and caveats when rubric doesn't capture full picture
- Uses comments to provide direction to decision committee for potential adjustment
- Documents reasoning for higher/lower scores in edge cases

**Multi-level Deep Diving:**
- Explores context for niche topics outside expertise (e.g., subspecies of butterfly)
- Reviews applications multiple times, limiting focus to specific categories each time
- Escalates uncertainty to decision committee rather than forcing rubric fit

**Circumstance-Based Excellence Assessment:**
- Evaluates "excellence among what is possible with the time" candidate has had
- Compares candidates relative to their circumstances rather than absolute metrics
- Example: 50-year-old with 20 years industry experience vs. 21-year-old

**Budget Feasibility Check:**
- Informally examines proposed budgets (not a formal component)
- Flags "excessively high or extremely modest budgets"
- Uses budget as feasibility proxy for project viability

**Character/Passion Recognition:**
- Looks for passion, positive outlook for change, aspiration
- Values passion "higher than I I value cleverness"
- Recognizes pattern indicators not in rubric

**Multiple-Angle Adjudication:**
- Consciously adjudicates "from multiple different angles"
- Attempts to account for personal bias by considering diverse life paths
- Writes to be "understood by the widest possible audience" rather than to sound learned

**Key Quotes:**

> "And if I don't have a box for it, I will put a caveat uh or a comment for the for the decision committee to review." (00:14:51)

> "So I end up going through it several times and um because I'm given very specific categories, I will limit what I'm looking at when I'm uh evaluating a said category." (00:13:24)

> "Um another element that I might consider is I might consider excellence among what is possible with the time. um the time this candidate has had." (00:14:51)

> "The the final thing that I I do look at and um part of I mean I I run a lab. I I manage students. I I have uh I have to do a whole bunch of elements of my projects is I while it's not a component of my decision, I always look at what's being proposed as the budget." (00:16:13)

> "I I always value passion higher than I I value cleverness. Uh you can always you can always teach passion but you can't build passion always." (00:08:11)

> "Um I do my best to um adjudicate from multiple different angles um because excellence uh true I think true excellence um can be measured whether it is from um a lay person's perspective or it can be from um an absolute a disciplined leader." (00:18:45)

---

## 4. SCORING STRATEGIES

**Conforming to Instructions with Commentary:**
- Explicitly conforms to OMT's 3 vs. 4 criteria as instructed
- Uses comment opportunity when perceiving "inability to give the grade I desire"
- Provides direction to decision committee for potential adjustment

**Humming and Hawing Between Adjacent Scores:**
- Sometimes changes mind between adjacent scores (e.g., "hum and haw between a three and a four")
- Explains reasoning for higher or lower score through comments
- Not major changes, but refinements within narrow range

**Circumstance-Weighted Scoring:**
- Adjusts expectations based on candidate circumstances
- Does not apply uniform excellence standard across all candidates
- Compares candidates to their own possible maximum rather than absolute standard

**No Explicit Starting High/Retrofitting Mentioned:**
- No discussion of starting with high scores and reducing
- No discussion of starting low and building up
- Approach appears to be: read carefully, apply judgment, comment when rubric inadequate

**Key Quotes:**

> "Um so there's very explicit instructions given what would get someone to a four and what would get

---

## Maureen De Jager

# Rubric, Scoring System, and Evaluation Mechanics Analysis

## 1. HOW THEY USE THE RUBRIC

**Overall Approach:**
- Uses rubric as a benchmarking tool after forming initial impressions
- Employs comparative assessment across multiple submissions
- Skims all applications first to establish "overall lay of the land" before detailed scoring
- Does detailed rubric marking on specific questions only after forming overview impression

**Process Flow:**
- Skim through all applications/portfolios
- Get overview perspective
- Go in and do specific questions and specific ratings
- Sometimes adjusts final score afterward

**Scoring System:**
- Uses 1-4 point scale on rubric questions
- Finds this "helpful as a starting point"

**Verbatim quotes:**
- "I think it is helpful because I think it gives you something as a reviewer um to to kind of benchmark against. You know, you you're looking for particular things. You're um testing uh your your impression against particular criteria." (00:06:27)
- "I think the criteria overall are useful and I think it's useful to have um the questions and even to sort of score them 1 2 3 and four. I find that kind of helpful as a starting point." (00:06:27)

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

### A. Academic Achievement Questions

**Complaint:** Difficulty interpreting grades when not directly relevant to proposed study

- "it's not always straightforward though especially when something is in progress. One doesn't have final results or the final results are not necessarily related directly and specifically to what the candidate is wanting to study." (00:07:37)

- "So one has to do a little bit of um sifting and interpretation there" (00:07:37)

- Relies on external knowledge of institutional rigor: "I rely on my knowledge of this sector um to be able to say that a 75% and I'm glad I'm not going to be this not going to be attributed directly to me but you know a 75% from a particular institution is is probably worth more than a 75% from another institution." (00:07:37-00:08:43)

### B. "Significance of Intended Study" Question

**Complaint:** Applicants struggle with this; answers often unconvincing and misaligned with exploratory nature of creative practice

- "I think that applicants struggle with this at least in my in my understanding. um because the questions are asked around the the how their studies will contribute to the South African Academy to South African society basically and to community" (00:10:13-00:11:31)

- "I think it's I mean I would struggle with those kinds of questions partly because um practice in its nature is exploratory you know it kind of like opens up towards wards um realizations and and understandings and um you know it's very difficult to know before you've even set out on this journey where you're going to end up." (00:11:31)

- "Um and so I think in a lot of cases it's a bit of a thumb suck. I don't you know like I don't feel that the answers provided there are necessarily that persuasive." (00:11:31)

- "I see a lot of those sort of quite generic and like floaty responses, you know, they're not really sort of anchored in" (00:13:57)

**Additional context:** More difficult for Master's applicants than PhD applicants to answer self-reflexively, as students aren't necessarily encouraged to think about societal benefit at that level.

### C. "Personal Motivation" Question

**Complaint:** Mismatch between what the question asks candidates to write and what reviewers are supposed to look for

- "there's a a slight misalignment in that you ask then as the reviewer um to look for evidence of values and priorities and sometimes the personal the the personal motivation is framed more as just a narrative" (00:13:57-00:15:18)

- "It should include the applicant's background, schooling, academic history, general interests, and personal philosophy. But then you've only got like a short space of time. And so a lot of candidates don't get much further than their background, schooling, academic history, and general interests, you know, but then you're being asked to look for evidence of their personal values and their priorities, you know." (00:15:18)

- "So, I kind of feel like it would be helpful for the question to be slightly reframed or for the criteria to be slightly reframed to allow for a better fit because we're looking for something that isn't necessarily in evidence. And I'm like sometimes, well, I don't know because the person hasn't mentioned their values or priorities. They've been telling me about their background, you know." (00:15:18)

### D. "Plan to Achieve a Vision" Question

**Complaint:** Confusing and potentially infeasible; lacks clarity on what specific benchmarks are expected

- "there's also the personal motivation. It asks for um a plan to achieve a vision which is also a difficult thing I think you know there's the very specific benchmarks in specific degree programs you know you got to have your proposal in at this stage and then so on um and it's I'm not sure that it's necessarily feasible to kind of like map out a plan you know it's a bit of an odd question to me in a sense." (00:15:18-00:16:43)

- "Um and then it also asks things around you know like motivation very generic life plans no substance strong vision plans for the future. So I don't know there's no like plans like long-term short-term it's a bit confusing I think it's confusing possibly for applicants and for reviewers." (00:16:43)

### E. "Motivation for Overseas Study" Question

**Complaint:** Difficult to verify claims; reviewers must rely on applicant assertions rather than evidence

- "I think that it's a it's an important question, but it's also a difficult one because as a reviewer, it's you one doesn't necessarily know whether there are comparable degree programs or things. So you're relying very much on um the applicant saying there is nothing like it rather than you know what your own kind of knowledge or what sort of evidence is provided." (00:18:12)

- "You know how is it evidenced? We don't know." (00:18:12)

### F. General Issue: Some Questions More Helpful to Candidates Than Reviewers (or vice versa)

- "I think some of the questions are more helpful than others and they might that they're probably more helpful they might be helpful to the candidate but not the reviewer or they might be helpful to the reviewer but not necessarily to the candidate." (00:06:27)

---

## 3. WORKAROUNDS THEY'VE DEVELOPED

### A. Retrofitting Scores

Uses final holistic impression to override calculated rubric score

- "sometimes I actually find that when I get to the end, I then go back and tweak because it comes up with a, you know, a final rating which doesn't sort of feel appropriate to the strength of the submission, you know. So, I do kind of retrofit um things a little bit uh to try and get a sense of okay, this was the best one that I saw. It might not tick all the boxes in the way that will lead to a particular kind of rating overall, but it was certainly the strongest for whatever reason." (00:05:08)

- "I do kind of retrofit um things a little bit" (00:05:08)

**Verbatim from summary:** "forming an initial impression, detailed rubric marking, and potentially retrofitting the score based on the overall assessment"

### B. Comparative Assessment Across Cohort

Skims all submissions before evaluating individual ones to establish benchmarks

- "I like to um look at them comparatively in a strange kind of way. And so, you know, usually there will be a few submissions rather than just one." (00:04:06)

- "it's kind of difficult to just sort of start with one and say whether it's a four or a three or you know so so sometimes for me it's helpful to get a a kind of overview sense. So, I'll sometimes just skim through all of the applications to see what's there and to get an a kind of overall lay of the land. And then when I look at each um application specifically, I'll I'll do the same again." (00:04:06)

- "So, instead of just sort of going step by step, I'll I'll skim through everything. I'll look at the portfolio. I'll, you know, um, and try and get almost like an an overview perspective, um, before I then go in and and and do the specific questions and the specific ratings." (00:05:08)

### C. Relying on External Knowledge and Experience

Uses institutional knowledge, sector experience, and personal funding history to interpret qualifications and applications

- "I rely on my knowledge of this sector" (00:07:37)

- Uses

---

## Mohamed Cassim

# Analysis of OMT Rubric, Scoring System, and Evaluation Mechanics

## 1. HOW THEY USE THE RUBRIC

**Two-Step Assessment Process:**
- **Merit-based:** "if it doesn't stand up on its own merit then I wouldn't recommend it anyway"
- **Relativity-based calibration:** Used when comparing similar submissions to differentiate between candidates

**Rubric Adherence with Subjective Integration:**
- "the rubric I try and standardize as far as you know because you know rubrics are there for a reason and and you do want to make sure that you're not um you're not biased in the way you you you plug in um but there are parts which are subjective"

**Reading Process:**
- "For my sons, I read it all. I read it all. Um, so my process is kind of longish... I spend a couple nights I'm a night owl, by the way. So I spend a couple nights while the family is sleeping just reading all the stuff"

**Integration with Additional Evidence:**
- References institutional performance: "if they're talking postgrad mast's PhD with 10 years plus of experience then I would use um the performance of the institutions where this person was at as as part of my considerations"
- Analyzes logic construction: "I like taking a look at how they think about and how they construct their logic"
- Identifies funding-seeking patterns: "sometimes you can see a guy just just collecting funds very quickly so that he can spend a couple years filling up time in order to get somewhere later... It's not really for the thesis and it's not for the content. Um, and and those I push aside fairly quickly"

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**"Extraordinary Talent" Category is Too Blunt:**
- "extraordinary talent and something special I feel leaves a little bit too much room for adjudicators to to think about we all have it from our own experience set I don't even know how you decide what talent is and and I feel like honestly I feel like writing a definition against which they make it more scientific"
- "I think it's a little bit blunt"
- Suggests need for specificity: "Are we talking about the probability that if this guy wrote some piece over a period of time and was funded, he'd do something amazing and the world would see it? Or talking about the probability of him taking a high high value um topic and delivering it to the extent that the world can build on it. Both of those represent something, but they're very different, like awfully different."

**Bias Toward Specialization "Unavailable in SA":**
- "I've always wondered why um a fourstar candidate is looking for a specialization unavailable in essay. And my only my only push back on that is while that may be valuable that we're bringing stuff unavailable in essay, there are also parts about essay that we can deliver because essay is kind of the Olympic champ at a particular subject matter or something else"
- "I feel like this question is not is not correctly done. I think it's it's biased in in possibly a way that that hasn't there's an opportunity to to improve on it"
- Example of overlooked expertise: "We are great at mining as an example as a country. I don't know that there's much that can be taught about taught to us about our extractive industries and I think we can spread a lot of that knowledge across the world. Um and we can repatriate some of that intellectual property out back to SA. We can create technology around it"

**"Intended Study" Requires Broad Experience:**
- "it relies on the adjudicator to on the experience of the adjudicator to say well in this field or that field um the significance of of the intended study is X which is quite a big is is one of the big pillars of this discussion and that does require some kind of broadsp spectrum experience across across time"

**"Personal Motivation" is Subjective:**
- "personal motivation again is another fairly subjective thing"
- Notes vulnerability to gaming: "they write a letter and and they show their experience and what they've done and how they do it... I think a letter you can get a good friend to write it. Hell, you can get AI to write it for you and you can tweak a few words"

---

## 3. WORKAROUNDS DEVELOPED

**Using Reference Letters as Authenticity Indicators:**
- "the other thing that I find, you know, sometimes useful and sometimes not, but I use as an indicator, as an inference is the reference letters"
- Identifies ghost-written letters: "Sometimes you get a guy who who gets three letters from people around him. You can see he's written it and he's tweaked a few words and gotten someone to sign it"
- Values genuine letters: "if if that's the the case, if if it's a a if if it's a a letter letter that that look look like like the the candidate candidate drafted drafted for for someone, someone, it it better better be be someone someone severely severely senior senior who who couldn't couldn't draft draft it it themselves"
- "you get these really heartfelt proper letters from from people who were their mentors or or their um seniors or or their professors or whatever else academic instructors. those I take to heart, you know, because because you can actually if it if it feels genuine and it's real and and if the three are from different angles aiming at at a person's character or or their capability makes a real difference in talking about their motivation"

**Evidence Synthesis Beyond Written Application:**
- Assesses holistic profile: "how you've built yourself, how you've approached what you say your interests are, what you've done to advance your interests, how how you would perform if you had an opportunity, that sort of stuff you can take out through the rest of the paper and add the letter on top"

**Cross-Referencing Achievement Claims:**
- Plans to verify institutional performance against CV claims (discussed as potential AI task): "I brought back the following financial statements and stuff for those companies to be able to see how they've done and they did badly... I've checked his reference letter. They say things that don't quite stack up to what the performance looks like and nor does the CV"

**Building in Contextual Factors:**
- Factors low national throughput rates: "One of my big bug bears is that as a country we, you know, our throughput on NSF, I don't know if you guys have seen it, but for every five students we pay for, we get one degree roughly... I factor into my process"

---

## 4. SCORING STRATEGIES

**"Skin in the Game" Assessment (Personal Investment Indicator):**
- "I'm a firm believer that if you don't have skin in the game or a really good reason to get the job done, right, no matter what the merit, you're halfway out the door"
- Applies conditional judgment: "I would be fairly pointed about saying, well, this guy hasn't tried any of his own cash or hasn't"
- Makes accommodations for financial constraints: "if a person and they'll say look I I support three people in my family and I do this and I do that and I do that. So then then certainly I wouldn't touch them on the financial stuff"

**Probability of Delivery Framework (Investment Portfolio Approach):**
- "ultimately it's not quite simply about the merit of of the intent cuz everybody can write a brilliant piece of intent. It's also about the probability of that being delivered. And I come from the investment world where that's a brutal discussion. You know, either they either it is probable or it's not when you're running a portfolio and OMT is running a portfolio"
- Uses multifaceted consideration model: "these multifaceted considerations do take place in my view whether we're talking about students or investments or whatever else it is"

**Capacity Assessment for Competing Obligations:**
- When personal circumstances are complex: "I would write on on a separate comment box just check on the capacity of this person to be able to deliver on the academic obligations that they're asking for money for because they may have too much that they're busy doing and it may it may affect their their lives"
- Recommends targeted support: "if you want to do that then give them extra money so that the dependents are okay and they can actually focus on this thing"

**Differentiation Strategy for Similar Candidates:**
- "if let's say there are three that look similar or the same um in the same field, then then I would want a reason then then I would want to figure out what the good answer is to split them"
- Extracts differentiating factors: "what what sectors was were these people exposed to? What are the potential of these sectors? What would this guy like to do for his next round that'll make a difference in this world compared to the others?"
- Explicit note approach: "I would just quite simply put that down as a note saying this one is interested in the following and I think X"

**Avoiding Mechanical Scoring:**
- "I wouldn't score others less. I would just quite simply put that down as a note saying this one is interested in the following an

---

## Philippe Burger

# Analysis of Rubric, Scoring System, and Evaluation Mechanics

## 1. HOW THEY USE THE RUBRIC

**Iterative Ranking Process:**
- Starts with provisional rankings that are adjusted as more candidates are reviewed
- "Usually you have like a provisional ranking as you go through you know so because you need to need to see you need to get to the sixth candidate and then rethink where you put number one and two." (00:05:51)
- "So so it it's a you know you adjust your ranking as as you go." (00:06:59)
- "It's a bit of an iterative process" (00:06:59)

**Three-Point Baseline System:**
- Uses 3 out of 5 as the average baseline score
- "I usually try to if if it's a score out of five to say, 'Okay, let's make three the average and and uh and and and see whether somebody pitches above that.'" (00:09:27)

**Level-Specific Scoring Demands:**
- Different standards applied to Masters, PhD, and Scholar applicants
- "You're going to be a bit more demanding of a scholar then you than you're of a PhD" (00:10:33)
- "For a masters somebody who wants to do a mast's you know it's you you need some basic skills demonstrate ated uh is is a bit more basic a bit more foundational for a PhD you would assume that the person is already master of the literature" (00:10:33)
- "For a scholar then it really needs to be good just to get an average" (00:10:33)

**Initial Academic Filter:**
- Academic record is the first screening criterion
- "The first thing I look at is the academic record to to to see how they how they they fare." (00:03:30)
- "The first filter is usually the academic one. Um because the competition is stiff and it's really just the top few who make it." (00:04:38)

**Field Alignment Assessment:**
- Reviews whether intended field of study aligns with existing qualifications
- "You look at the so they have an intended field uh that they that they indicate what they want to study and I think it's uh it's usually important to see whether that actually lines up with what they want to achieve but also with their their qualifications." (00:03:30)

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**Subjectivity in Essay Scoring (Primary Complaint):**
- "That little essay that they that they that they write uh I think there uh you know you you the English all of a sudden improved a lot uh for some of these cases which suggest uh that it might be Chad writing them instead of the candidate." (00:03:30)
- "When it is more about the self-escription essay or or later on when you look at what what the motivation for what they want to study um there's a bit more to that uh but but when it's this motivation essay I think that is where the the biggest issue comes in and that that is very subjective" (00:08:08)

**Translation of Impression to Score is Inherently Subjective:**
- "There's there's always uh in that because you you do a translation of what your impression into a into a score. Um so so there will always be something uh uh um in there that that's a bit subjective." (00:09:27)

**Essay Quality is Unreliable Discriminator:**
- "The essay is you know it's like grading essays. uh particularly if it's an essay not based on um you know a scientific essay where you look at whether they site sufficient sources and and uh do they report the literature etc. when when it is more about the self-escription essay" (00:08:08)

**AI-Generated Essays Becoming Undetectable:**
- "Let's say in 2023 2024 still know back then you you could still see that something like a chat GBT generated text was very bland and and generalist. Um, and then these things that I highlighted, you know, the the bit of ambition and so on, uh, the bit of flare, the bit of drive, the bit of focus, reflecting that you've understood the issue, you could still pick that up from from from your better essays. Um, but of course now that that will become more and more difficult because you can ask JGBT or Gemini or any of these things to uh to generate you a piece of text that that that reflects these things." (00:16:45)

**Challenge of Distinguishing Applicants:**
- "I think the going forward I think that you're going to have a serious problem relying on essays uh to as a as a uh mechanism for distinguishing between uh applicants" (00:16:45)

## 3. WORKAROUNDS THEY'VE DEVELOPED

**Reading Multiple Applications Before Scoring:**
- "I try to to really if if I have like 20 or 25 I like to read like 10 11 12 of them and and then think about the scoring you know so just so that I can just get an idea of of how they stack up uh relatively." (00:24:56)

**Recalling Context from Previous Years:**
- "By the time you are at number five everything of last year's come back to you in terms of you know where do I click this or that." (00:24:56)
- "You start thinking about oh this might compare like this to to last year." (00:24:56)

**Interview Process to Verify AI-Generated Essays:**
- "We then need to need needed to do is to actually have an interview with these students uh because then it is you and the interview panel. It's not you and Chad GPT. Um and and then that way you sort of can compare uh what was said in the essay with what what what comes out in an interview. um if there's a big discrepancy there then then then that is an indicator" (00:15:14)

**Using Turn-It-In AI Detection:**
- "We put these essays through turn it in's AI detector not you can't prove it but let's you know and they all come back or not all about 80% of come back with with with that uh that there's a high level." (00:15:14)

**Identifying Bland vs. Quality Essays:**
- Looking for markers of genuine ambition: "The bit of ambition and so on, uh, the bit of flare, the bit of drive, the bit of focus, reflecting that you've understood the issue" (00:16:45)
- "You look for for for that little extra that that that would carry it to the from a three to a four or four to a five." (00:12:43)

## 4. SCORING STRATEGIES

**Starting High Then Adjusting Downward:**
- "Sometimes you think okay this is a mediocre I'm not going to you know let's first see where where the others fall out uh or pitch." (00:06:59)

**Bottom-Up Building Approach:**
- "You really look for for are there markers in there that would allow you to give this person a four or or a five." (00:09:27)

**Intuitive Adjustment Post-Marking:**
- "So you read through everything, you get a kind of intuitive score. You've marked everything and given it a number and you see this number doesn't align with my feeling. And then you [adjust]" (00:10:33)
- "Because remember you do this every year from scratch." (00:10:33)

**Comparative Stacking Against Cohort:**
- "I need to read through and then you also need to look at the at where what it pitches you know is it a masters is it a PhD is it a is it a scholar" (00:10:33)

**Looking for "Extra Something" in Borderline Cases:**
- "If it is a borderline um you know you look for for for that little extra that that that would carry it to the from a three to a four or four to a five." (00:12:43)

## 5. HOW THEY HANDLE EDGE CASES IN SCORING

**Borderline Case Decision Process:**
- "As I said then then you look at let's say the essay or the motivation for what they want to go study and you look um and and and if it is a borderline um you know you look for for for that little extra that that that would carry it to the from a three to a four or four to a five." (00:12:43)

**Assessing Value-Add:**
- "What's the value added that that that you bring um particularly also when you look at you know the intended field of study is it I just want to go do a masters in I don't know public administration or is it something specific that you want to go go find out" (00:12:43

---

## Pieter Pistorius

# Analysis of Rubric, Scoring System, and Evaluation Mechanics

## 1. HOW THEY USE THE RUBRIC

**Process:**
- Uses rubric as a structured evaluation framework after completing detailed application review
- Applies rubric scoring after reading all documents (45 mins to 1 hour per applicant)
- Does NOT finalize scores until reviewing entire cohort
- Conducts final consistency check across all applicants (~20 minutes) before submitting
- Uses rubric as critical re-evaluation tool when discrepancies arise

**Key Quote:**
> "And what I tend to do then is to say that and not to finalize it until I've gone through everybody. And then I'll just have a quick check to see if it's consistent." (00:08:19)

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**No major complaints identified.** Instead, Pistorius expresses satisfaction:

> "I find the rubric quite useful." (00:14:37)

> "I find the rubric um quite useful what I've done once or twice is to read because there's a there's a there's a a sort of back a letter setting out the background and I I I I went through that letter not in the last version but in the previous cohort that I evaluated and looked at the the intention in the letter and how the rubric is structured and it looked okay." (00:14:37)

> "So I think you know my own experience of the rubric is that it is reasonably wellought out. It's not overly complicated. So my my own evaluation would be that it has been put together with some care and it does help help me in making." (00:15:52)

**Re: Scoring scale granularity:**
> "I don't think it look I my view was not that you know I was because I think it you scored it out of a fivepoint range again I haven't looked at the the the system recently um my experience is that and this is in in in a sense also in in in in in sort of a work environment. If you sort out a problem, sometimes it's useful to give a group some structure in which to score aspects of a problem or something like that is that you can there sometimes a rubric one, two or three or maybe I've heard that people should say it should be an even number... but you know a rather core system, I think, is is is not a disadvantage." (00:16:50-00:18:00)

## 3. WORKAROUNDS THEY'VE DEVELOPED

**Avoidance of middle scores ("forcing extremes"):**
> "And you know what I try to do and I'm not I haven't checked my own performance in that sense is to essentially on the first round to be rather extreme you know not to give everybody three out of five for whatever the scoring system is" (00:14:37)

**Halo effect mitigation:**
> "There could be a little bit of a halo effect. you know, somebody maybe working in a topic that's close to my heart or well the application is not that strong. So once you know I've I've I've gone through everyone I I just again maybe spend 20 minutes or so looking looking at the overall rating and um decide whether there are any outliers from my point of view." (00:08:19)

**Manual timeline creation for CV/record analysis:**
> "I think that that would help help quite a bit. That's and it's a completely different level but one of my uh responsibilities here at the university is I teach a specific uh post-graduate program a honors program by teaching and we get a large number of applicants and um you know is to one of my jobs is to go through the applicants to decide whether they are admitted or not... quite often it is somewhat tedious you know because everyone's academic record comes in a different format from a different institution you know B techs and B and BC's and all sorts of things... I think that that can be quite tedious that's gone sometimes when I draw out a little diagram say okay this individual was in metric this year, you know." (00:31:03-00:32:00)

**Deliberate avoidance of comparative benchmarking:**
> "I I try to look at that cohort in itself, you know, is to is to benchmark um that would be difficult I think because the processes are so far apart and of course um you know I may get a different mix you know so I I on purpose I don't try to benchmark." (00:23:22)

> "Yeah. Yeah. Yeah. I think they asked me because they value my opinion not to calibrate not to support what the eventual um decision was. If that makes if that makes sense." (00:24:48)

## 4. SCORING STRATEGIES

**"Extreme then calibrate" approach:**
> "So almost there's the interesting thing about your process. It sounds like you have a sniff test where you're just getting like a a whiff of each one across the whole batch... And then you say that you do a pass but you don't finalize it until you've done everything. But then you have another self-imposed constraint where you almost force yourself not to score in the middle but to pick an extreme and then through successive approximations in a later pass you'll bring it you'll calibrate it considering the hole." (00:15:52-00:16:50)

**Pistorius confirms:**
> "Yeah. That is that's fair. That sort of forcing myself to be rather extreme that is in the sort of middle part you know we spent an hour roughly speaking an hour on application." (00:16:50)

## 5. HOW THEY HANDLE EDGE CASES IN SCORING

**Two-tier edge case approach:**
> "It's usually and again this is looking at the cohort is you know if I end up with two candidates according to my own score obviously that are very close to one another I would try to think who do I think is the most deserving that would be the one edge case and on the other side you know there's there's an there's an option where you say but you know this this shouldn't be supported so I would look at the top and the bottom." (00:25:41)

> "Usually the um as I say the you know clearly the applicants have been there's been some homework done on that. So usually it's more an issue of the two top people but I may force myself to say if I have to pick a winner which one would it be?" (00:26:45)

**Note: No tiebreaker methodology per se - only forced choice:**
> "Um you know that is not that you know that would be a sort of Yeah. Yeah. Yeah. I think they asked me because they value my opinion not to calibrate not to support what the eventual um decision was." (00:24:48)

**Discrepancy handling protocol:**
> "I would uh follow the rubric. You know what I would do if there's a discrepancy uh or if say for example somebody looks looks pretty good on the first impression that quick impression and you know I score him or her and see that it is maybe you know lower then I'll go and take a look and say why is it lower you know and um you know I tend you know so I would go back to the sort of individual parameters and and look at that again." (00:19:13)

## 6. SUGGESTIONS FOR RUBRIC IMPROVEMENT

**No direct rubric modifications suggested.** Process improvements offered instead:

**Timeline/deadline management (self):**
> "I would you know um okay it's probably more towards myself you know it does help and I've I on purpose I try to do that it does help not to walk to work towards a deadline but you know that is not a reflection on OMT... what I try to do and that's again this sort a quick scan is to start earlier rather than later and not to bump up against the deadline. You know, I find that and you know is is typical with most intellectual processes or or mind processes that it you know it helps to sleep on it... if they ask me to do it again, I'll probably try on purpose to do to complete at least the second phase, the sort of detailed phase well ahead of the deadline." (00:20:35-00:22:14)

**AI for tedious tasks (CV/timeline compilation):**
> "I think that that would help help quite a bit... this sort of screening out I think could could help significantly." (00:32:00)

> "It could probably help a lot in that sort of initial initialskilling." (00:31:03)

**On AI's role (with caveats):**
> "It probably wouldn't do any harm, you know... it may make sense, you know. I don't I don't I I don't know. It would be interesting to run that experiment to see you know after the whether it would whether it would really change things you know or make it easier... eventually the human being must make a final call." (00:28:59-00:30:02)

## 7. VERBATIM QUOTES - COMPREHENSIVE INDEX

### Rubric Utility & Design

---

## Ryan Nefdt

# OMT Rubric & Scoring Analysis - Ryan Nefdt Interview

## 1. HOW THEY USE THE RUBRIC

**Primary Use:**
- Systematic review tool when managing large application batches (20-30 submissions)
- Reads submissions "in tandem with uh looking at the rubric and and like kind of structuring it" (00:03:54)
- Uses rubric to identify "what OMT wants me to extract from that" (00:05:02)
- Tool for standardization and comparison across applications

**Comparative Function:**
- Does NOT compare across evaluation cycles: "I don't think I ever think oh last year the crop was" (00:29:00)
- HIGHLY comparative within single cycle: "I am quite comparative in and I'll give you a specific example of how I'm comparative. I will do things give an evaluation and sometimes I will go back to other applications when I see something that I think oh wait you should have done this too and I gave you that mark but now it's not fair" (00:29:00)
- Adjusts scores retroactively for consistency: "I do go back and forth to try to to um to like smooth that function" (00:29:00)

**Priority in Decision-Making:**
- Treats rubric as primary decision mechanism: "I see the rubric as a measurement that allows comparison and that's why I'm a little reluctant to just form my own view outside of it" (00:10:03)
- Uses rubric to override gut instinct: "I would say I actually use it as a tool to like you know uh a comparative tool and and I give it a little bit um more priority than my own initial gut instinct feelings very often" (00:10:03)
- Rationale: neutralizing personal bias - "it's telling me I can use this. So, say I think somebody's fantastic, then this has happened to me before, but the rubric doesn't really differentiate them from somebody else...I would on the side of using the rubric because I'm assuming that is a tool to to neutralize the sort of like personal bias" (00:10:03)

---

## 2. SPECIFIC COMPLAINTS ABOUT THE RUBRIC

**Edge Cases with Theoretical Work:**
- "there are times when when it's it's just not a perfect fit and it it's at the edge cases. So for instance, personally for me I feel like if you are engaging in a theoretical project that is somewhat lacking in terms of your ability to express the societal impact but that theoretical project is so intricate and interesting and you know I I kind of want to give it more merit and that often isn't represented in the rubric things" (00:08:52)

**"Puzzle Master" Problem:**
- "maybe this is a personal thing, but I think if you if you're a puzzle master and you're working on an interesting puzzle, sometimes somebody else is going to realize the value of that puzzle, but it's important that you are able to to decode it and try to approach it...I don't I think sometimes in the humanities um uh the sort of the way we asked to evaluate these things that level is not really captured" (00:08:52)

**Societal Impact Framework Too Rigid for Humanities:**
- "with humanities it's a little bit different. you have to figure out different conduits to um to getting some societal impact...If their study is aligned with certain kinds of societal goals so they unfortunately for the very highly theoretical sort of pursuits they have to be thinking about why it matters" (00:07:41)
- Note: He doesn't penalize, but the rubric doesn't capture theoretical merit well

**Previously Complained About NRF Rating Scoring:**
- "I did change one thing but it was a very uh technical point. So at some point we had um they had are you NRF rated and the number that you got assigned for P rating was less than if you had a C rating...a P is harder to get in some cases than an A because there are fewer people who have P's and it involves like whether you're going to be a future leader. So, you know, I I think this should actually be scored higher than a C or even a B" (00:12:17)

**Removed Elements He Preferred:**
- "I I believe they don't ask us to evaluate the the budget anymore, right? that used to be an element as well like how much has this person sourced outside things and that was a I liked that question because it just gave a scoring to something that was kind of felt objective" (00:13:25)

---

## 3. WORKAROUNDS THEY'VE DEVELOPED

**Reading Strategy for Large Batches:**
- Tries to read submission once without rubric categories "just at least once by itself without like uh predisposing it to the the categories of of relevance" (00:03:54)
- Then applies rubric in second pass for systematic evaluation

**Retroactive Score Adjustment:**
- Reviews other applications after initial scoring to ensure fairness: "I will do things give an evaluation and sometimes I will go back to other applications when I see something that I think oh wait you should have done this too and I gave you that mark but now it's not fair that I'm giving you that mark if I'm if this is what the standard should be" (00:29:00)
- Self-described practice: "I do go back and forth to try to to um to like smooth that function" (00:29:00)

**Communication with OMT:**
- Has sent feedback directly to OMT about rubric problems: "I have in the past sent an a message to OMT and said I I don't think the scoring works perfectly for this question. You're missing this dimension" (00:12:17)
- This led to actual change in NRF rating scoring

**Holistic Application Review:**
- Takes applications out of online form structure: "when I'm doing a proposal or something like that I like to take it out and put it in a different document and then sort of read it as a whole" (00:15:38)
- Recognizes OMT's structure is fragmented: "OMT's structure is very much like you can do it separately. Write that question close that window write the next one. And so I get very disconnected answers sometimes" (00:15:38)

**Discipline-Specific Research:**
- When unfamiliar with field: "what I will do often is I will go out and Google it myself or try to find out a little bit more if I can" (00:25:53)

**Recommendation Letter Minimal Use:**
- Simply flags for concerning information: "I tend not to put much stock in in the rec unless there's something that's been flagged that is concerning which is rare" (00:17:56)
- Does NOT read between the lines: "I try not to read between the lines because I don't know the recommenders very often" (00:19:11)

---

## 4. SCORING STRATEGIES

**Not "Starting High" - No Evidence of This:**
- Uses rubric as primary baseline
- Describes process as systematic application of rubric, not starting with high score and adjusting down

**No Explicit "Retrofitting":**
- However, does engage in what could be called retrospective calibration: "I will do things give an evaluation and sometimes I will go back to other applications when I see something that I think oh wait you should have done this too and I gave you that mark but now it's not fair" (00:29:00)

**Comparative Scoring Within Cycle:**
- Actively compares within evaluation cycle to maintain consistency
- Adjusts scores to "smooth that function" (00:29:00)

**Gut Instinct vs. Rubric:**
- Acknowledges difficulty in self-assessment: "Uh, it's it's hard to to give you a definitive answer about like my own internal psychological process. I'm not sure if I'm the best even judge of of that" (00:10:03)
- States clear intention: "I give it a little bit um more priority than my own initial gut instinct feelings very often" (00:10:03)

**Edge Case Approach:**
- "when you're on the fence...it can be case by case basis" (00:14:26)
- Looks to "level of preparation the application has undergone" (00:14:26)
- Considers whether applicant approached it holistically or "just filled this in as an online form" (00:15:38)

---

## 5. HOW THEY HANDLE EDGE CASES IN SCORING

**General Edge Case Philosophy:**
- "it's sometimes it can be case by case basis. They are overall impressions that that applications give you like and this is something we it's hard to quantify but maybe it wouldn't be a a bad thing to put into the rubric as well" (00:14:26)

**Key Edge Case Factor: Level of Preparation**
- "just the level of preparation the application has

---

## Frasia Oosthuizen

# Analysis of Rubric, Scoring System, and Evaluation Mechanics

## 1. How They Use the Rubric

**Process:**
- Reads application **first**, then applies rubric retrospectively: "I prefer reading all of it and then doing the rubric because the rubric is sort of"
- Reviews one candidate at a time, not in batch scanning mode
- Groups applications by category (Masters, PhD, Postdoc, Sabbatical) and completes each group in one sitting to maintain consistency
- Makes detailed handwritten notes during the process
- Uses a double-screen setup with rubric visible alongside application

**Calibration approach:**
- Flips back through previous scores within a batch to recalibrate: "It does happen that if I do that group I think okay I gave this one a four but really then that one shouldn't have been a four. It should have been a three."
- Compares candidates within groups rather than across groups
- Maintains internal consistency within each batch/category reviewed

## 2. Specific Complaints About the Rubric

**Missing data problem (Primary complaint):**
- "I think there was there's one that I always find a little bit of difficulty is and that's no fault of the applicants. Um I think it's the deadline for submission that they sometimes might not have marks as yet and the rubric works in such a way that you have to fill in every um question because it it adds to the final number that is generated."
- Problem: "you don't have marks" but the system forces completion, artificially affecting scores

**Referee reports criteria (Secondary complaint):**
- The rubric language is too explicit/stringent: "the rubric what they expect from the referee reports is stated it very explicitly...to score a four one of the referees or two of the referees must have said the candidate is exceptional"
- Mismatch with real-world referee language: "I don't really come across referees using that and it's very very clearly stated like that in the rubric"
- Inconsistency with normal academic practice: "not something we don't referee them as exceptional generally"
- Workaround admitted: "Sometimes I have um ignored it but as I said ignored it consistently in a group"

**Interpretation variability (Acknowledged concern, not rejected):**
- "I do believe that the way the rubric is structured that every evaluator might interpret it in their own way"
- However, she views this as acceptable if individual reviewers are internally consistent

**Applicability issues:**
- "In some instances, the rubric asks questions that are not possible to answer and I think in the past round I did find that there were things that might not have been applicable and I don't know if that were like marks for a mast's that going into a PhD or something."

## 3. Workarounds They've Developed

**For missing academic marks:**
- Workaround: Appears to apply subjective judgment despite missing data requirement, though not explicitly detailed

**For referee report discrepancies:**
- "Sometimes I have um ignored it but as I said ignored it consistently in a group that if I felt the referees mentioned this or referred to that that is similar to a four"
- Interprets referee language more leniently than rubric specifies, maintaining consistency within batch

**For subjective interpretation:**
- Maintains batch-level consistency as internal solution: "if I said someone should definitely get it and someone should ne not get it, I at least know in my candidates why I chose that one and not that one"
- Groups scoring to maintain relative ranking even if absolute scale interpretation varies

**For out-of-field submissions:**
- Conducts background research independently when needed (mentioned later)

## 4. Scoring Strategies

**Not explicitly "starting high" or "retrofitting"** but uses **comparative calibration within batches:**
- Reads all candidates in a category before finalizing scores
- Adjusts previous scores downward based on comparison: "I gave this one a four but really then that one shouldn't have been a four. It should have been a three"
- Maintains relative rankings rather than absolute rubric adherence
- No evidence of strategic starting points

**Alignment-based scoring:**
- Penalizes misalignment between motivation, proposal, and referee reports: "If there's a disalignment...they score lower"
- Bonus confidence when all elements align: "If everything if the big picture makes sense from the motivation through to the referee reports, I feel more confident that I have an understanding of who this candidate is"

## 5. How They Handle Edge Cases in Scoring

**Missing academic performance data:**
- Acknowledged problem but workaround unclear: "I don't think I need to complete that part of the rubric"
- Suggests option should exist: "maybe there should be an option there not to include it if it's missing"

**Disaligned applicants (motivation doesn't match proposal):**
- Automatically scores lower: "if the candidate wants to, you know, cure cancer, but the project is so basic, there's a total disalignment and you don't get a good sense and then inevitably they score lower as well. Um it's um but yeah it's a more difficult one to to then maybe not more difficult I just do score them lower"

**Two candidates with same score:**
- Asserts no massive gaps possible if same score: "I feel that I have two candidates on the same score. that even if it's very disparate um masters they're going to do or whatever the projects are might be in totally different fields I feel that they are on the same level."
- Suggests internal consistency prevents score inflation between candidates at same level

**Referee reports overwhelmingly positive despite weak application:**
- Applies caution and explicit checking: "I am cautious not to let it sway me too much...where I found the motivation the prior performance in terms of marks as well as the proposal not overwhelmingly positive and then getting overwhelmingly positive um referee reports. So, I'm cautious just to make sure it's all aligned."

## 6. Suggestions for Rubric Improvement

**Allow optional fields for missing data:**
- "maybe there should be an option there not to include it if it's missing...So that it doesn't pull the score down or up because it's absent."

**Revise referee report scoring criteria:**
- Make language more lenient/flexible: "maybe it was just needed a little bit more leniency there"
- Don't require explicit "exceptional" language from referees

**Add layman's abstract requirement:**
- "maybe, you know, like a layman's abstract that can accompany it"
- Rationale: "That also and that might also contribute to how we can see these applicants right their writing skills. Um, give us the version of both sides"
- "It's not dumped down. It's just written in a language that is more um easy to understand for everyone that's not necessarily a biologist or a bioineticist"
- Expected benefit: "once you have that understanding it's easier then to understand the proposal and then you can yeah easier judge it"

**Do NOT automate alignment checking:**
- Pre-identified red flags would introduce reviewer bias
- Explicitly rejected: "if that person tells me what the red flags are, I already have decided in my mind no this is not going to work. So I'm really not objective...you don't let the candidate speak"

**Consider applicability of criteria:**
- Some rubric questions may not apply to all categories; build in conditional logic

## 7. All Relevant Verbatim Quotes

### On Rubric Use and Interpretation

- "I prefer reading all of it and then doing the rubric because the rubric is sort of"

- "I try and do the masters as a group and at least to do it in a day. So I have a sort of a recall as I go through the process and I do sometimes flip back."

- "It does happen that if I do that group I think okay I gave this one a four but really then that one shouldn't have been a four. It should have been a three."

- "I do find the rubric as a very useful guide."

- "I do believe that the way the rubric is structured that every evaluator might interpret it in their own way."

- "I might interpret some of the things not exactly how someone else will do it but as I said in that little bit of subjectivity I try to do at least my group that in that group they are the same for me."

- "if I said someone should definitely get it and someone should ne not get it, I at least know in my candidates why I chose that one and not that one."

- "I hope so. And I I think personally that that is more important...if you look at the reviewers and at and what a single reviewer has done that there's consistency in their approach."

- "I don't think you you can expect that but as I said I don't believe that is a flaw in the system."

### On Specific Rubric Problems

**Missing marks:**
- "I think there was there's one that I always find a little bit of difficulty is and that's no

---

