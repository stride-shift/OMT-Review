# Rubric and Scoring Deep Dive

## Dina  Ligaga

Here's a comprehensive analysis of the rubric, scoring system, and evaluation mechanics from the transcript:

1. Rubric Usage:
- Uses a 4-point rubric (1-4 scale)
- Finds points 3 and 4 particularly "fuzzy" and challenging to distinguish
- Tries to align rubric scores with overall intuitive assessment of candidate

2. Rubric Complaints:
- Lacks sufficient gradation, especially between scores 3-4
- Feels confined/boxed in by the rigid scoring system
- Doesn't capture the nuanced complexity of candidate evaluations
- Particularly problematic for Masters-level applications with many similar candidates

3. Workarounds:
- Manually adjusts scores up/down to match intuitive assessment
- Takes extensive personal notes to supplement rubric
- Writes narrative explanations to justify scoring decisions

4. Scoring Strategies:
- Starts by reading entire application holistically
- Looks for narrative continuity and sincerity
- Prioritizes applicant's motivation and proposed study
- Sometimes retrofits scores to match overall impression

5. Edge Case Handling:
- Relies on personal judgment for borderline cases
- Considers factors like difficult personal background
- Trusts there's a synthesis process with other reviewers
- Comfortable making tough differentiation decisions

6. Rubric Improvement Suggestions:
- Add more granular scoring range (especially for Masters)
- Create more differentiation between 75-100% excellence
- Allow more flexibility in scoring

7. Relevant Verbatim Quotes:
- "The rubric can be a bit... fuzzy"
- "I sometimes mark up or mark down just so the final tally looks like what the person deserves"
- "Sometimes excellence can be 75% and sometimes 90%"
- "The rubric boxes me in"

The analysis reveals a complex, nuanced evaluation process that goes well beyond simple numerical scoring.

---

## Edzai Conilias Zvobwo

Based on the transcript, here's an analysis of the rubric, scoring system, and evaluation mechanics:

1. Rubric Usage:
- Covers multiple dimensions, described as "mutually exclusive, cumulatively exhaustive"
- Helps with initial elimination and categorization of candidates
- Requires meeting minimum requirements to proceed
- Covers academic performance, research potential, and impact

2. Specific Rubric Complaints:
- Scoring bin size (0-4) is too broad
- High and low-quality candidates can get lumped into the same score (e.g., all "threes")
- Difficulty with final summation of candidate evaluation

3. Workarounds:
- Flexibility to go back and revise initial scores
- Uses a "Bayesian" approach of adjusting beliefs with more evidence
- Sometimes gives high scores based on implicit understanding of an idea

4. Scoring Strategies:
- Start with initial scores, then recalibrate as more applications are reviewed
- Use personal story/resilience as a key differentiator
- Consider potential impact, especially for South African context
- Aim for 25% increments (0-4 scale mapped to 0-100%)

5. Edge Case Handling:
- Look for implicit potential even if articulation is poor
- Consider life circumstances that might explain academic inconsistencies
- Seek subject matter expert input for specialized research areas

6. Rubric Improvement Suggestions:
- More granular scoring system
- Thematic grouping of applications annually
- Institutional memory across review cycles
- Subject matter expert review for specialized fields

7. Verbatim Quotes:
- "I've had a situation where I literally held their hand to say, 'Okay, I get what you're saying and I'll give you four because I I I get what you're saying, but you're not saying it, but I get it.'"
- "The story matters... if it's a case of there's nothing amazing or bad, it's just neutral kind of thing. Then the story comes into the frame."
- "The bin is too big. So, you see at three, a high-quality three and a low-quality three, they're all bunched up in there."

---

## Freedom Gumedze

Based on my analysis, here are the key findings about the rubric and scoring process:

Rubric Usage:
1. The rubric is standardized across different application categories (masters, PhD, postdoc, sabbatical)
2. Different criteria are added for different applicant levels
3. Reviewer uses the rubric as a reference point after initially reading the full portfolio

Scoring Strategies:
1. Benchmarks against past submissions across multiple years
2. Uses comparative assessment ("Is this meeting four-star criteria?")
3. Does not necessarily compare within current applicant pool
4. Relies on personal expertise to judge innovation and quality

Specific Rubric Complaints:
1. Publication assessment is "loose" - no clear guidance on how to evaluate publications
2. Heavily modeled on NRF assessment approach
3. Does not capture nuanced research impact
4. Does not request comprehensive metrics from applicants

Suggestions for Improvement:
1. Separate evaluation criteria for master's by coursework vs dissertation
2. Implement a consensus/moderation process like NRF or research funding committees
3. Provide clearer guidance on publication and research impact assessment
4. Share review scores across reviewers
5. Provide feedback on funding outcomes

No specific verbatim quotes meet the extraction criteria for workarounds or edge case handling.

---

## Frelet De Villiers

Based on the transcript, here's a comprehensive analysis of the rubric, scoring system, and evaluation mechanics:

Rubric Usage:
1. She sees the rubric as a "framework" but finds it sometimes restrictive
2. Quote: "Sometimes the rubrics are very um restrictive and not really um incompulsing"
3. She will add value to things not explicitly in the rubric

Scoring Strategies:
1. Initial Approach: Starts with all candidates at a "4" and then downgrades
2. Quote: "I always start on a four... As I go further, I will then go, no, this is actually a three"
3. Scoring is impressionistic, not mathematically precise
4. Quote: "I don't really say a four is like 94.6% and higher"

Scoring Process:
1. Iterative process - scores can be changed after initial review
2. Compares candidates relative to current batch, not previous years
3. Looks for top candidates first, then refines scores
4. Quote: "Sometimes there's like just a margin and some of them are on the same then then I have to decide"

Rubric Limitations:
1. Finds the 0-4 scale somewhat limiting
2. Wants flexibility to capture nuanced assessments
3. Prefers detailed feedback over rigid scoring

Unique Approach:
1. Writes extensive feedback for applicants
2. Quote: "I'm really very detailed in my comments"
3. Aims to provide constructive guidance for future applications

Suggestions for Improvement:
1. She couldn't specify exact rubric improvements without reviewing it again
2. Suggested she could provide email feedback if sent the rubric

No direct verbatim quotes about specific workarounds or edge case handling were identified.

---

## Martin Clark

Based on the transcript, here are the key findings about the rubric, scoring system, and evaluation mechanics:

1. Rubric Usage:
- Parcels applications by level (Master's, PhD, sabbatical)
- Uses a 1-4 point rubric initially, then a 5-point final assessment
- Tries to calibrate "expectation of excellence" for each level
- Follows explicit OMT instructions for scoring

2. Rubric Complaints:
- Categories are "rigid"
- Five points isn't always enough to discriminate between candidates
- Difficult to benchmark grades across different institutions
- Metrics like Golden Key Society can disadvantage some candidates

3. Workarounds:
- Adds detailed comments explaining scoring rationale
- Includes caveats for edge cases
- Uses "tacet knowledge" and expert judgment
- Attempts to evaluate candidates from multiple angles

4. Scoring Strategies:
- Limits review to 5-10 applications per session to avoid "adjudication fatigue"
- Sometimes shifts between adjacent scores (3 vs 4)
- Considers candidate's circumstances and potential
- Looks at budget feasibility as an informal assessment

5. Edge Case Handling:
- Reviews applications multiple times
- Limits focus to specific categories
- Conducts additional research on niche topics
- Considers candidate's background and time constraints

6. Rubric Improvement Suggestions:
- Make categories more "nebulous"
- Allow more flexibility based on discipline
- Create a more sensitive definition of excellence
- Provide broader understanding of funding goals

7. Relevant Verbatim Quotes:
- "The rubric itself gets back to that measure of excellence... I find that the categories provided are rather rigid."
- "Five points isn't always enough to discriminate one against the other."
- "I do my best to adjudicate from multiple different angles because excellence... can be measured whether from a lay person's perspective or from a disciplined leader."

---

## Maureen De Jager

Here's the analysis focusing on rubric, scoring, and evaluation mechanics:

1. Rubric Usage:
- Uses 1-4 scoring system
- Skims all applications first to get an overall "lay of the land"
- Looks at portfolio critically as a key assessment element
- Performs comparative assessment across submissions
- "Retrofits" scores after initial detailed review if final rating doesn't match overall impression

2. Rubric Complaints:
- Some questions more helpful to candidates than reviewers
- Academic achievement questions require complex interpretation
- "Personal motivation" section poorly structured
- Vision/plan achievement question is "odd" and confusing
- Significance of study questions are difficult for applicants to answer convincingly

3. Workarounds:
- Uses external knowledge about institutional rigor when evaluating transcripts
- Relies on portfolio as critical assessment mechanism
- Looks for overall "coherence" between applicant's stated goals and actual work
- Applies different strictness for Masters vs PhD applications

4. Scoring Strategies:
- Starts with comparative overview
- Uses detailed rubric marking
- Retrofits scores based on holistic impression
- More strict for overseas study applications
- Considers technical/conceptual sophistication of portfolio

5. Edge Case Handling:
- Case-by-case assessment when "on the fence"
- Considers funding level/study type
- Checks progress evidence for continuing students
- Looks for misalignments between academic record and practical work

6. Rubric Improvement Suggestions:
- Reframe personal motivation section to better capture values
- Require research supervisor reference
- Add portfolio annotation/contextualization
- Clarify vision/plan achievement questions

7. Verbatim Quotes:
- "I do kind of retrofit things a little bit to try and get a sense of okay, this was the best one"
- "Sometimes there's a very clear misalignment between the academic record and the portfolio"
- "If there's a coherence then it's automatically persuasive"

---

## Mohamed Cassim

Based on the transcript, here's a comprehensive analysis of the rubric, scoring system, and evaluation mechanics:

1. Rubric Usage:
- Uses a standardized rubric with both factual and subjective components
- Tries to minimize bias while allowing for nuanced, experience-based assessments
- Focuses on sections like academic achievement, intended study, personal motivation
- Looks beyond simple scoring to provide differentiated perspectives

2. Rubric Complaints:
- "Extraordinary talent" category is too blunt and lacks scientific definition
- Specialization section biased toward topics "unavailable in SA"
- Some sections (like "intended study") rely heavily on adjudicator's broad experience
- Personal motivation assessment can be subjective

3. Workarounds:
- Uses reference letters as authenticity indicators
- Conducts deeper contextual analysis beyond rubric's surface-level questions
- Applies "relativity" by comparing similar candidates to find nuanced differences

4. Scoring Strategies:
- Two-step assessment: 
  1. Evaluate submission on its own merit
  2. Calibrate by comparing to similar submissions
- Seeks to understand "probability of delivery" rather than just intent
- Considers candidate's capacity to complete academic obligations

5. Edge Case Handling:
- Examines candidate's financial capacity
- Checks institutional performance of candidate's previous employers
- Looks for genuine motivation beyond written letters
- Considers cultural and language barriers in assessment

6. Rubric Improvement Suggestions:
- Create more scientific definition of "talent"
- Recognize areas of local expertise (e.g., South African mining knowledge)
- Develop more nuanced assessment of personal motivation
- Allow for cultural and language context

7. Verbatim Quotes:
- On talent: "I feel like honestly I feel like writing a definition against which they make it more scientific"
- On motivation: "I take all of that into account to assess personal motivation"
- On rubric subjectivity: "It relies on the adjudicator's experience"
- On delivery probability: "Ultimately it's not about the merit of intent... it's about the probability of that being delivered"

---

## Philippe Burger

Based on the interview transcript, here's an analysis of the rubric, scoring system, and evaluation mechanics:

Rubric and Scoring Mechanics:
1. Scoring System:
- Uses a 5-point scale
- 3 is considered the average baseline
- Looks for markers to justify scores of 4 or 5
- Scoring is iterative and adjusted as more applications are reviewed

2. Scoring Strategies:
- Starts with a provisional ranking
- Adjusts rankings as they review more candidates
- Revisits and refines scores throughout the process
- Reads about 10-12 applications before finalizing scoring

3. Evaluation Approach:
- Initial filter is academic record
- Motivation essay is highly subjective
- Different standards for different degree levels (Masters, PhD, Scholar)
- Looks for:
  * Focus
  * Clarity of goals
  * Realism
  * Ambition
  * Alignment with academic background

4. Edge Case Handling:
- Looks for "that little extra" to move from 3 to 4 or 4 to 5
- Considers value added by intended field of study
- Assesses depth of motivation beyond generic statements

5. Specific Challenges:
- Increasing difficulty with AI-generated essays
- Subjective translation of impressions into numerical scores
- Balancing relative and absolute benchmarking

6. Recommendations for Improvement:
- Implement interviews
- Use timed, proctored essays
- Stricter pre-screening
- Conscious bias mitigation

Verbatim Quote Highlights:
"You do a translation of what your impression into a into a score. Um so so there will always be something uh uh um in there that that's a bit subjective."

"I usually try to if if it's a score out of five to say, 'Okay, let's make three the average and and uh and and and see whether somebody pitches above that.'"

"As you start working through it, you you start getting an idea who is higher and who's lower. Um and and and then you revisit that and it's a bit of an iterative process."

---

## Pieter Pistorius

Based on the transcript, here's a comprehensive analysis of the rubric, scoring system, and evaluation mechanics:

Rubric Usage:
1. Finds the rubric "quite useful"
2. Four or five-point scoring system
3. Tries to avoid middle scores on first pass, intentionally being "extreme"
4. Uses rubric as a structured way to evaluate applications

Scoring Strategies:
1. Initial "sniff test" across entire batch
2. First pass intentionally uses extreme scores
3. Second pass calibrates scores considering the whole group
4. If initial intuition differs from rubric score, follows rubric
5. Uses discrepancies as opportunity for critical re-evaluation

Rubric Handling of Edge Cases:
1. Looks at top and bottom candidates
2. If two candidates are very close, tries to determine "most deserving"
3. Will force himself to pick a winner if necessary

Specific Quotes on Rubric:
- "I find the rubric quite useful"
- "Rubric is almost by definition limiting. It forces you to make a decision"
- "A rather coarse system is not a disadvantage"

Rubric Improvement Suggestions:
- No direct suggestions for rubric improvement
- Appreciates current structure as "reasonably well thought out"

Workarounds/Strategies:
1. Starts review process early to "sleep on it"
2. Avoids benchmarking against previous cohorts
3. Limits motivation/rationale to 2-3 sentences
4. Checks external details for consistency

Verbatim Rubric-Related Quotes:
- "I try not to give everybody three out of five"
- "If there's a discrepancy... I'll go back to individual parameters"
- "I wouldn't rework my scoring to get to my first answer"

---

## Ryan Nefdt

Based on the transcript, here's a detailed analysis of the rubric, scoring system, and evaluation mechanics:

1. Rubric Usage:
- Systematically reviews each essay against the rubric
- Uses rubric to extract specific information and assign numerical values
- Considers the rubric a tool for comparison and neutralizing personal bias
- Prioritizes rubric scoring over initial gut feelings

2. Rubric Complaints:
- Struggles with edge cases, especially theoretical projects
- Doesn't always capture nuanced work, particularly in humanities
- Lacks ability to score highly intricate "puzzle master" work
- Previously had technical scoring issues (e.g., NRF rating scoring)

3. Workarounds:
- Reads entire submission first before applying rubric
- Adjusts scores by reviewing other applications in the same cycle
- Looks for holistic application preparation beyond just filling out boxes
- Manually cross-references applications to ensure consistent standards

4. Scoring Strategies:
- Comparative scoring within a specific application cycle
- Smooths score "function" by revisiting and adjusting marks
- Prioritizes rubric-based scoring for standardization

5. Edge Case Handling:
- Evaluates level of application preparation
- Looks for holistic approach to application
- Considers overall impression beyond strict rubric categories
- Willing to be flexible for complex theoretical work

6. Rubric Improvement Suggestions:
- Add scoring for application preparation level
- Provide more nuanced scoring for theoretical/complex work
- Improve NRF rating scoring
- Create more flexible categories for humanities submissions

7. Verbatim Quotes:
- "I see the rubric as a measurement that allows comparison"
- "Sometimes there are issues that are just not going to be able to be translated that well, but they're very intricate"
- "I would on the side of using the rubric because I'm assuming that is a tool to to neutralize the sort of like personal bias"

---

## Frasia Oosthuizen

Based on the transcript, here's an analysis of the rubric, scoring system, and evaluation mechanics:

Rubric Usage:
1. Uses rubric as a "guide" after thoroughly reading the entire application
2. Applies rubric consistently within a single batch of applications (e.g., all masters applications)
3. Acknowledges rubric is open to interpretation between different reviewers

Rubric Complaints:
1. Some rubric questions are "not possible to answer"
2. Referee report section has overly strict language requirements
3. Mandatory fields can penalize students if information is missing (e.g., recent exam marks)

Scoring Strategies:
1. Reviews applications in grouped batches (masters, PhD, postdoc)
2. Retrofits/adjusts scores retroactively when comparing candidates
3. Ensures internal consistency within a single review batch
4. Aims to create a holistic "picture" of the candidate

Edge Case Handling:
1. Looks for alignment across motivation letter, proposal, and referee reports
2. Cautious about over-relying on referee reports
3. Manually compensates for rubric limitations by being consistent

Rubric Improvement Suggestions:
1. Add option to exclude missing information without score penalty
2. Make referee report language assessment more flexible
3. Potentially include a layman's abstract to aid understanding

Key Verbatim Quotes:
- "I do believe that the way the rubric is structured that every evaluator might interpret it in their own way."
- "I feel that I have two candidates on the same score... even if it's very disparate"
- "If everything if the big picture makes sense... I feel more confident"

---

